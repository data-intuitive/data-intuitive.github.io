[
  {
    "objectID": "careers/web_developer/index.html",
    "href": "careers/web_developer/index.html",
    "title": "Full Stack Web Developer",
    "section": "",
    "text": "What we need\nData Intuitive is currently intensively working on a new chapter and is seeking a motivated full stack web developer to join our team. We are looking for an individual who is passionate about learning, eager to contribute to our team’s success, and excited to take on new challenges on this exciting new journey together.\nWhat we offer\nWe offer a very competitive salary, benefits, possibility of company car and equity stake. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nA degree in software engineering or other degree or proven experience in line with the requirements for this job\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical requirements\n\nHTML, CSS, JavaScript, Node.js\nweb development frameworks and tools, such as Angular, React, Vue, Laravel, Ruby on Rails, Django, and Express.js.\nScripting languages (shell, python, R, etc.)\nUsing Git and GitHub (creating branches, pull requests, managing issues, github actions)\nContainerisation (Docker, Podman, or Singularity)\nLinux based operating systems, libraries and tools\nExperience working with databases, such as MySQL, PostgreSQL, and MongoDB.\n\nNice to haves knowledge\n\nHigh performance computing environment\nCloud computing\nModern workflow management systems (Nextflow, Snakemake, …)\nWriting in Markdown\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  },
  {
    "objectID": "careers/data_workflow_developer/index.html",
    "href": "careers/data_workflow_developer/index.html",
    "title": "Bioinformatics Data Workflow Developer",
    "section": "",
    "text": "What we need\nData Intuitive is embarking on a new chapter and is seeking a highly skilled, scientifically minded IT professional to join our team. We are looking for an individual who is passionate about learning, eager to contribute to our team’s success, and excited to take on new challenges on this exciting new journey together.\nWhat we offer\nWe offer a very competitive salary, benefits, possibility of company car and equity stake. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nUnderstanding of fundamental biological principles\nA degree in bioinformatics, software engineering, computational biology, or other degree or proven experience in line with the requirements for this job\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical requirements\n\nScripting languages (shell, python, R, etc.)\nUsing Git and GitHub (creating branches, pull requests, managing issues, github actions)\nModern workflow management systems (Nextflow, Snakemake, …)\nContainerisation (Docker, Podman, or Singularity)\nData wrangling tabular/structured data (tidyverse or pandas)\nLinux based operating systems, libraries and tools\nWriting documentation in Markdown\n\nNice to haves knowledge\n\nHigh performance computing environment\nCloud computing\nSingle-cell omics (scRNA-seq, multi-omics)\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  },
  {
    "objectID": "careers/ workflow_innovation_engineer_in_life_sciences/index.html",
    "href": "careers/ workflow_innovation_engineer_in_life_sciences/index.html",
    "title": "Workflow Innovation Engineer in Life Sciences",
    "section": "",
    "text": "Who we are\nData Intuitive creates innovative IT solutions for biotech through synergy of talents, knowledge and skills. We specialise in developing bioinformatics workflows in a highly effective and efficient manner.\nWhat we need\nWe are looking for a scientifically minded individual to become part of the core of Data Intuitive. We are currently in the exciting process of crafting a new chapter for our organisation and need a motivated, experienced key person to help us make this major step. A motivated individual eager to contribute to our strategic projects in data science through research, development and implementation of ML in workflow implementations.\nWhat we offer\nWe offer a competitive salary, benefits, possibility of company car and equity stake for the first employees. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nSolid understanding of fundamental AI and ML principles\nA master’s degree in bioinformatics, artificial intelligence, software engineering, computational biology, or equivalent demonstrated experience that meets the requirements for this job.\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical skills\n\nExperience with AI and ML techniques and frameworks (TensorFlow, PyTorch, etc.)\nExperience with scripting languages (shell, python, R, etc.)\nExperience with using version control tools (Git/GitHub) and Jira\nGood knowledge of Linux based operating systems, libraries and tools\nGeneral knowledge of software build automation (make, etc.)\n\nNice to haves knowledge\n\nKnowledge of modern workflow management systems (Nextflow, Snakemake, …)\nKnowledge of containerisation (Docker, Singularity, Kubernetes, …)\nKnowledge of high performance computing environment\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  },
  {
    "objectID": "insights/news/2022-12-15-fit_to_go_global/index.html",
    "href": "insights/news/2022-12-15-fit_to_go_global/index.html",
    "title": "Fit to Go Global",
    "section": "",
    "text": "We are thrilled to announce that this website is being launched with the funding of Flanders Investment and Trade (FIT). This funding will not only help us to create a professional and user-friendly website, but it will also support the international expansion of our company. We are grateful for the support and belief in our mission, and we look forward to working together as we continue to grow and develop on a global scale.\n\nBack"
  },
  {
    "objectID": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "href": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "title": "Fueled by VLAIO and Driven by innovation",
    "section": "",
    "text": "Data Intuitive is pleased to announce that we have received funding from VLAIO, which will bolster our research and development efforts as we continue to push the boundaries of data science and bioinformatics data pipelines. We are grateful for the support of VLAIO and are excited to see what the future holds as we continue to innovate and progress in our field.\n\nBack"
  },
  {
    "objectID": "insights/news/2023-02-05-sustain/index.html",
    "href": "insights/news/2023-02-05-sustain/index.html",
    "title": "DI’s Sustainability and Wellness Initiative",
    "section": "",
    "text": "As a telework-based company, Data Intuitive is committed to promoting both sustainability and the well-being of its employees. With the support of Ready2improve and Axudo, the company is developing a comprehensive personal health and sustainability plan in collaboration with its employees. Through organized group sessions during team-building days, the company and its partner aim to foster discussions and generate ideas for a better telework future.\nOne of the difficulties faced by a fully remote company is fostering a sense of unity and collaboration among employees. To address this, the company explores different work methods and leverages technology to create a virtual environment that mimics the experience of working together in one large shared space. Teleworking also helps reduce commute emissions, promote sustainability, and support personal well-being through flexible work arrangements. The plan places a strong emphasis on open-source technology and support for non-profit scientific organizations, further reinforcing Data Intuitive’s commitment to sustainability.\nThis effort showcases Data Intuitive’s dedication to sustainability and well-being, and its aim to create a better future for both its employees and the planet.\n\n\nBack"
  },
  {
    "objectID": "insights/news/2023-02-05-sustain/index.html#sustainable-telework-a-plan-for-a-better-future",
    "href": "insights/news/2023-02-05-sustain/index.html#sustainable-telework-a-plan-for-a-better-future",
    "title": "DI’s Sustainability and Wellness Initiative",
    "section": "",
    "text": "As a telework-based company, Data Intuitive is committed to promoting both sustainability and the well-being of its employees. With the support of Ready2improve and Axudo, the company is developing a comprehensive personal health and sustainability plan in collaboration with its employees. Through organized group sessions during team-building days, the company and its partner aim to foster discussions and generate ideas for a better telework future.\nOne of the difficulties faced by a fully remote company is fostering a sense of unity and collaboration among employees. To address this, the company explores different work methods and leverages technology to create a virtual environment that mimics the experience of working together in one large shared space. Teleworking also helps reduce commute emissions, promote sustainability, and support personal well-being through flexible work arrangements. The plan places a strong emphasis on open-source technology and support for non-profit scientific organizations, further reinforcing Data Intuitive’s commitment to sustainability.\nThis effort showcases Data Intuitive’s dedication to sustainability and well-being, and its aim to create a better future for both its employees and the planet.\n\n\nBack"
  },
  {
    "objectID": "insights/blog/2014-07-10-publishing-html-presentations-on-github/index.html",
    "href": "insights/blog/2014-07-10-publishing-html-presentations-on-github/index.html",
    "title": "Publishing html presentations on Github",
    "section": "",
    "text": "You’ve seen those fancy html presentations on the web? Reveal.js is a framework to create such things of beauty. And it goes along well with my Markdown based style of writing, even for presentation slides.\nI usually create the presentations on my laptop, using Pandoc to convert to html. In principle, this should be ready for the web by default. If only I had an easy hosting solution to move it to.\nGithub allows you to host html files and even complete web sites. I had never tried it myself, but now I did. It’s really simple. Move your repository into the branch gh-pages (you can do this on the Github website) and finished. The web site is accessible via http://&lt;username&gt;.github.io/&lt;projectname&gt;. This is what Github calls a Project Page.\nBehind the scenes, Github uses Jekyll for building static website from source files in, e.g., Markdown format. When you publish something to the gh-pages branch, it automatically kicks in… and gave very vague errors in my case.\nAfter some trial and error, I found out, the best approach is to install jekyll yourself and launch it locally. This immediately gives a readable error message. It turned out I had a stale symlink in my directory tree. Removing this removed the building issue for Github.\nThe result can be seen here: http://tverbeiren.github.io/BigDataBe-Spark.\nBTW, in order to install jekyll on my MacBook Air, I had to install a newer version of Ruby (tip: use rvm for this, link).\n\nBack"
  },
  {
    "objectID": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html",
    "href": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html",
    "title": "Viz and Math in Apache Zeppelin",
    "section": "",
    "text": "For some years, I have been going back an forth between [Spark-Notebook][spark-notebook] and [Apache Zeppelin][zeppelin] for different use-cases. Already 2 years ago, I made a little comparison of the two technologies.\nI used Spark-Notebook in order to do develop the code necessary to write the post about model error. It came in handy that Spark-Notebook supports math notation in Markdown out-of-the-box. Zeppelin does not do that.\nOn the other hand, I have never been fond of the graphics support in Spark-Notebook. Zeppelin has some interesting functionality by default, including the ability to pivot data tables. But it still is too limited for customized charts and visualizations.\nNow, luckily, both the visualization aspect and math support can easily be solved."
  },
  {
    "objectID": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#math-support",
    "href": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#math-support",
    "title": "Viz and Math in Apache Zeppelin",
    "section": "Math support",
    "text": "Math support\nSupport math notation is as easy as inserting a cell with the following content:\n%angular\n&lt;script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'&gt;\n&lt;/script&gt;\nA markdown formatted cell looks like this:\n# Introduction\n\nI don't want to discuss the biology of a virus or its spreading, even though virus spreading is the topic of the simulation we're about to do. I invite you to look [elsewhere](http://arxiv.org/abs/1411.1472) for that.\n\nWe mainly want to get a feel for the main message Taleb's argument in his analysis linked above: The dangers of basing decisions on simple models for the spreading of virusses and the lack discussing the risks involved in analysing these models.\n\nWe do this by modelling the spreading of a virus by means of a simple [geometric Brownian motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion) process. It's the same process that is used in the (in)famous [Black-Scholes model](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model).\n\nThe differential form of the process is as follows:\n\n$$\nd S(t) = S(t) \\mu dt + S(t) \\sigma d W(t)\n$$\nThis is an excerpt from the post on model error. Please note that the $$ symbols are on a line of their own. This is to avoid that Zeppelin renders math in the input field itself.\nThe following screenshot shows this in action in Zeppelin:\n\n\n\nMathjax in Zeppelin\n\n\nOh, yes, you might need to refresh the notebook in order for the rendering to take effect."
  },
  {
    "objectID": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#vis",
    "href": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#vis",
    "title": "Viz and Math in Apache Zeppelin",
    "section": "Vis",
    "text": "Vis\nZeppelin has an interesting approach to visualization already. It offers basic types of plots. Unfortunately, these plots are often not sufficient. Especially when one wants to provision dashboards based on notebook content.\nOf course, one can use R and Python in Zeppelin, and even interchange data between contexts so that plotting could be done using [ggplot2 or [matplotlib][matplotlib]. But it remains a hassle to use, and not all of these interpreter are always installed in every instance of Zeppelin.\nThere are a number of Scala plotting libraries, but none that convinced me up till now. Until [Vegas][vegas] came around. It is based on [vega-lite][vega-lite] and offers an API for Scala/Spark. I have done custom visualizations in Zeppelin and Spark-Notebook before, but those involved including additional javascript libraries. Vegas does not require a custom build. It can be used like this:\nFetch the binary package:\n%dep\nz.load(\"org.vegas-viz:vegas_2.11:0.3.6\")\nImport the necessary classes:\nimport vegas._\nimport vegas.render.HTMLRenderer._\nimplicit val displayer: String =&gt; Unit = { s =&gt; print(\"%html \" + s) }\nAnd plot the data:\nval plot = Vegas(\"Country Pop\")\n                .withData(\n                    Seq(\n                        Map(\"country\" -&gt; \"USA\", \"population\" -&gt; 314),\n                        Map(\"country\" -&gt; \"UK\", \"population\" -&gt; 64),\n                        Map(\"country\" -&gt; \"DK\", \"population\" -&gt; 80)\n                    )\n                 )\n                .encodeX(\"country\", Nom)\n                .encodeY(\"population\", Quant)\n                .mark(Bar)\n                .show\nPlease not that withData takes a Seq of Map, contrary to what is stated on the [github][vegas] page.\nThe following screenshot shows this running:\n\n\n\nVegas in Zeppelin\n\n\nInteresting links:\n\nvega-lite\n\nmatplotlib\n\nggplot2\n\nzeppelin\n\nspark-notebook\n\nmathjax\n\nvegas\n\n\nBack"
  },
  {
    "objectID": "insights/blog/2012-01-25-fight-against-tax-fraud-in-belgium/index.html",
    "href": "insights/blog/2012-01-25-fight-against-tax-fraud-in-belgium/index.html",
    "title": "Fight against Tax Fraud in Belgium",
    "section": "",
    "text": "The (brand new) Belgian government has decided to raise the stakes in the fight on tax fraud. That’s a good thing, especially in times of tight government budgets.\nIn the news last week, it is said that the group of people actively involved in detecting tax fraud will be enlarged by a quarter (a little less than 300 new hires). That means that in total, for Belgium alone, more than a thousand man and women are looking for ways to spot fraud. The involved government agencies will be looking for Masters in fields like ‘accountanting, law but also informatics’.\nThis sounds like the good old ‘brute force’ method: throw a bunch of people on a problem and when the problem gets bigger, throw more people at it.\nBasically, there’s one major field of interest missing from the list profiles in the press release: statistics. People trained in statistical analysis may have heard of Benford’s Law or related concepts. They may take a completely different approach to dealing with large data sets. Statisticians may propose to use relatively simple heuristics first, in order to select the subset of targets with the highest probability of fraud.\nAn example of what the process might look like:\n\nFrom a very large dataset, find a way to select a small subset that a) has the highest probability of fraud, b) has the highest probability of being able to prove fraud and c) actually benefits the government by selecting only the ‘big fish’.\nGet the informatics specialists to implement this heuristic algorithm as fast as possible, with ways to tune the individual parameters for deeper analysis.\nFrom the resulting small selection, invite accountants and lawyers to focus on those.\n\nHave I forgotten something? I’m tempted to try and contact the federal government agency responsible for tax fraud and propose to start with 1.\nUpdate: Thank you Paul for correcting some errors.\n\nBack"
  },
  {
    "objectID": "insights/blog/2012-09-25-rational-decision-making-in-business/index.html",
    "href": "insights/blog/2012-09-25-rational-decision-making-in-business/index.html",
    "title": "Rational Decision Making in Business?",
    "section": "",
    "text": "In business contexts, generally speaking, we believe we act as rational beings. The rise of Business Intelligence and the wonders of Number Crunching and Big Data make us believe we have entered the era of rational decision making.\nIt appears to me that this way of thinking is similar to what has been observed in the economic sciences. For long, men was thought of as a rational being, making rational choices. It has taken Kahneman, Tversky and others several decades to counter this supposition.\nCould it be that the belief in a solely rational business logic is equally false? We have reasons to believe so. The good thing is, one can be a fan of Big Data but nevertheless keep in mind that:\n\nRationality is just like perfectionism: It can be nice to aim for, but attaining it may take too much effort.\n\nPS: Please note that non-rational does not imply irrational.\n\nBack"
  },
  {
    "objectID": "insights/blog/2015-01-15-transposing-a-spark-rdd/index.html",
    "href": "insights/blog/2015-01-15-transposing-a-spark-rdd/index.html",
    "title": "Transposing a Spark RDD",
    "section": "",
    "text": "I have been using Spark quite a lot for the last year. At first using the Scala interface, but lately more using the Python one.\nIn one of my recent projects, I received a dataset that contains expression profiles of chemical compounds on genes. That is to say, I got a dataset which had this data transposed, i.e., genes versus compounds, but that is not a handy format to work with. I load the original data into an RDD, but then I have to transpose this RDD.\nI have been looking on the web but found no complete solution. Recently, a similar question came up on the Spark mailinglist. So I thought it is about time that I posted my approach.\nThis is the code for the function that transposes an RDD and returns a new RDD. There are other approaches, and there is room for optimisation as well. But this already gets the work done.\ndef rddTranspose(rdd):\n    rddT1 = rdd.zipWithIndex()\n            .flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e)))\n            .groupByKey().sortByKey()\n    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), \n                        cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n    return rddT4.map(lambda x: np.asarray(x))\nThis code converts the rows to numpy arrays in the return statement, so you need to import numpy as np. This step is strictly speaking not necessary, but it does make subsequent random access inside the rows faster. It must be noted as well that the procedure only works when one row (one element of the original RDD as well as the transposed RDD) fits into the JVM memory of the workers.\nI left out the comments in my code, to keep it a little exciting for you…\n\nBack"
  },
  {
    "objectID": "insights/blog/2014-09-24-writing-workflow-and-reproducible-data-analysis/index.html",
    "href": "insights/blog/2014-09-24-writing-workflow-and-reproducible-data-analysis/index.html",
    "title": "Writing Workflow and Reproducible Data Analysis",
    "section": "",
    "text": "I’ve been writing about my writing workflow before. Since some aspects of it are related to reproducible research and especially reproducible data analysis, I have collected some material and tips in a presentation I gave last week on my Github:\n\n\n\nRR\n\n\nOne aspect that I did not yet mention there, is how I approach this on my Mac. This depends a little bit on what type of text I’m writing. Data analysis is usually done within RStudio. It has very good functionality for generating PDFs and the like, but I still prefer my own Makefile and knitr/Pandoc combination.\nLess technical texts are usually writing using iAWriter, but sometimes also in Sublime Text. iAWriter by default has support for Markdown, Sublime Text can be configured with a very good Markdown plugin. I use Marked for previewing, proof-reading, etc.\n\nBack"
  },
  {
    "objectID": "insights/blog/2013-06-24-writing-workflow-markdown-pandoc-latex-and-the-likes/index.html",
    "href": "insights/blog/2013-06-24-writing-workflow-markdown-pandoc-latex-and-the-likes/index.html",
    "title": "Writing workflow: Markdown, Pandoc, LaTeX and the likes",
    "section": "",
    "text": "You wouldn’t tell from the updates on this website, but I’m actively writing again. Offline, that is, the online part is for later. For now, I want to share my experience improving my writing workflow.\nIn the past, I used LaTeX for scientific texts and MS Word for everything else. LaTeX gives me the professional and typographically correct texts that I want, but I spent too much time fiddling around with packages, remembering markup, etc. MS Word, on the other hand, quickly made me get things done, albeit without the professional look or scientific powers.\nI’m now in a situation that any writing (technical, scientific and even prose) can be done in the same way, delivering results in PDF, html or even MS Word:\n\nIt usually starts in iA Writer (on the Mac or the iPad), but any word processor able to handle ASCII text can be used. I choose iA Writer because of its distraction free writing.\nMarkdown is used as markup specification (including figures, footnotes, emphasis, etc.). Markdown is very basic, but it lets you focus on the content, rather than the form.\nProgramming code (R for instance), formulas, etc. can all be included in the Markdown format by means of the proper notation and possibly some extensions to the parser (see step 4).\nBy means of Pandoc, the text is converted into the appropriate format (html, pdf, LaTeX, ePub, DocBook, …)\nReady!\n\nOk, I hear you thinking, but you just lost all possible configuration of look and feel, layout, etc… That’s correct, there are some Markdown writing tools that allow you to create PDFs that look awful.\nThe nice thing about Pandoc though is that during the conversion step (4), you can specify the templates (CSS, LaTeX header code, MS Word template) that should be used.\nIt takes some fiddling in order to get the correct options to Pandoc and get proper templates in place. A Google should get you going.\nA similar process is used to create the reports for the different analysis steps for the dataMineR project.\n\nBack"
  },
  {
    "objectID": "insights/blog/2016-10-26-data-intuitive-at-spark-summit-2016/index.html",
    "href": "insights/blog/2016-10-26-data-intuitive-at-spark-summit-2016/index.html",
    "title": "Data Intuitive at Spark Summit 2016",
    "section": "",
    "text": "Short version\nNo, I’m not at the spark summit in Brussels. Time is not on my side, too much things to do.\nBut, being there is not necessary when other people make sure they do the publicity for you.\nThank you Miel Hostens!\n\n\nLonger version\nAbout two years ago, I gave a presentation at the first Spark Meetup in Belgium. It’s when I met Miel. I think it is fair to say that the architecture and application I was describing back then (and which is still under active development) made Miel realize that what he wanted to achieve was possible.\nThe talk used to be available as a video, but the link is no longer active. The slides, however, can be fetched here.\nMiel and I have stayed in contact. I’m happy he can get his message across. And I’m happy of course I have played a role in this.\n\nBack"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html",
    "title": "anndata: annotated data in R",
    "section": "",
    "text": "anndata is a commonly used Python package for keeping track of data and learned annotations, and can be used to read from and write to the h5ad file format. It is also the main data format used in the scanpy python package (Wolf, Angerer, and Theis 2018).\nHowever, using scanpy/anndata in R can be a major hassle. When trying to read an h5ad file, R users could approach this problem in one of two ways. A) You could read in the file manually (since it’s an H5 file), but this involves a lot of manual work and a lot of understanding on how the h5ad and H5 file formats work (also, expect major headaches from cryptic hdf5r bugs). Or B) interact with scanpy and anndata through reticulate, but run into issues converting some of the python objects into R.\nWe recently published anndata on CRAN, which is an R wrapper for the Python package – with some syntax sprinkled on top to make R users feel more at home.\nThe R anndata is still under active development at github.com/rcannood/anndata. If you encounter any issues, feel free to post an issue on GitHub!"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#installation",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#installation",
    "title": "anndata: annotated data in R",
    "section": "Installation",
    "text": "Installation\nInstalling anndata is not particularly hard. You do need R and Python to be installed. If you don’t have a working installation of Python, you can let reticulate install Miniconda.\n# install the R anndata package\ninstall.packages(\"anndata\")\n\n# skip this if you wish to use the local python installation\nreticulate::install_miniconda()\n\n# skip this if anndata is already installed\nanndata::install_anndata()"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#getting-started",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#getting-started",
    "title": "anndata: annotated data in R",
    "section": "Getting started",
    "text": "Getting started\nThe API of anndata is very similar to its Python counterpart. Check out ?anndata for a full list of the functions provided by this package.\nAnnData stores a data matrix X together with annotations of observations obs (obsm, obsp), variables var (varm, varp), and unstructured annotations uns.\nHere is an example of how to create an AnnData object with 2 observations and 3 variables.\n\nlibrary(anndata)\n\nad &lt;- AnnData(\n  X = matrix(1:6, nrow = 2),\n  obs = data.frame(group = c(\"a\", \"b\"), row.names = c(\"s1\", \"s2\")),\n  var = data.frame(type = c(1L, 2L, 3L), row.names = c(\"var1\", \"var2\", \"var3\")),\n  layers = list(\n    spliced = matrix(4:9, nrow = 2),\n    unspliced = matrix(8:13, nrow = 2)\n  ),\n  obsm = list(\n    ones = matrix(rep(1L, 10), nrow = 2),\n    rand = matrix(rnorm(6), nrow = 2),\n    zeros = matrix(rep(0L, 10), nrow = 2)\n  ),\n  varm = list(\n    ones = matrix(rep(1L, 12), nrow = 3),\n    rand = matrix(rnorm(6), nrow = 3),\n    zeros = matrix(rep(0L, 12), nrow = 3)\n  ),\n  uns = list(\n    a = 1, \n    b = data.frame(i = 1:3, j = 4:6, value = runif(3)),\n    c = list(c.a = 3, c.b = 4)\n  )\n)\n\nad\n\nAnnData object with n_obs × n_vars = 2 × 3\n    obs: 'group'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\nYou can read the information back out using the $ notation.\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nad$obs\n\n   group\ns1     a\ns2     b\n\nad$obsm[[\"ones\"]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    1    1    1\n[2,]    1    1    1    1    1\n\nad$layers[[\"spliced\"]]\n\n   var1 var2 var3\ns1    4    6    8\ns2    5    7    9\n\nad$uns[[\"b\"]]\n\n  i j      value\n1 1 4 0.03063135\n2 2 5 0.97626754\n3 3 6 0.70391182\n\n\n\nUsing scanpy\nA nice side-effect of loading this package is that it is now super easy to interact with scanpy through these AnnData objects.\n\nlibrary(reticulate)\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nsc &lt;- import(\"scanpy\")\nsc$pp$normalize_per_cell(ad)\n\nad$X\n\n       var1 var2     var3\ns1 1.166667  3.5 5.833333\ns2 1.750000  3.5 5.250000\n\n\n\n\nReading / writing AnnData objects\nRead from h5ad format:\nread_h5ad(\"pbmc_1k_protein_v3_processed.h5ad\")\n## AnnData object with n_obs × n_vars = 713 × 33538\n##     var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n##     uns: 'hvgParameters', 'normalizationParameters', 'pca', 'pcaParameters'\n##     obsm: 'X_pca'\n##     varm: 'PCs'\n\n\nCreating a view\nYou can use any of the regular R indexing methods to subset the AnnData object. This will result in a ‘View’ of the underlying data without needing to store the same data twice.\n\nview &lt;- ad[, 2]\nview\n\nView of AnnData object with n_obs × n_vars = 2 × 1\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nview$is_view\n\n[1] TRUE\n\nad[,c(\"var1\", \"var2\")]\n\nView of AnnData object with n_obs × n_vars = 2 × 2\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nad[-1, ]\n\nView of AnnData object with n_obs × n_vars = 1 × 3\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\n\n\nAnnData as a matrix\nThe X attribute can be used as an R matrix:\n\nad$X[,c(\"var1\", \"var2\")]\n\n       var1 var2\ns1 1.166667  3.5\ns2 1.750000  3.5\n\nad$X[-1, , drop = FALSE]\n\n   var1 var2 var3\ns2 1.75  3.5 5.25\n\nad$X[, 2] &lt;- 10\n\nYou can access a different layer matrix as follows:\n\nad$layers[\"unspliced\"]\n\n   var1 var2 var3\ns1    8   10   12\ns2    9   11   13\n\nad$layers[\"unspliced\"][,c(\"var2\", \"var3\")]\n\n   var2 var3\ns1   10   12\ns2   11   13\n\n\n\n\nNote on state\nIf you assign an AnnData object to another variable and modify either, both will be modified:\n\nad2 &lt;- ad\n\nad$X[,2] &lt;- 10\n\nlist(ad = ad$X, ad2 = ad2$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n$ad2\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n\nThis is standard Python behaviour but not R. In order to have two separate copies of an AnnData object, use the $copy() function:\n\nad3 &lt;- ad$copy()\n\nad$X[,2] &lt;- c(3, 4)\n\nlist(ad = ad$X, ad3 = ad3$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667    3 5.833333\ns2 1.750000    4 5.250000\n\n$ad3\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#future-work",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#future-work",
    "title": "anndata: annotated data in R",
    "section": "Future work",
    "text": "Future work\nIn some cases, this package may still act more like a Python package rather than an R package. Some more helper functions and helper classes need to be defined in order to fully encapsulate AnnData() objects. Examples are:\n\nad$chunked_X(1)\n\n&lt;generator object AnnData.chunked_X at 0x7f8d3424eac0&gt;\n\n\nFollowing functionality has not been tested:\nad$rename_categories(...)\nad$strings_to_categoricals(...)\nCurrently not implemented are the read_zarr() and ad$write_zarr() functions. I need some example data to test this functionality."
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#references",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#references",
    "title": "anndata: annotated data in R",
    "section": "References",
    "text": "References\nWolf, F Alexander, Philipp Angerer, and Fabian J Theis. 2018. “SCANPY: Large-Scale Single-Cell Gene Expression Data Analysis.” Genome Biology 19 (February): 15. https://doi.org/10.1186/s13059-017-1382-0.\n\nBack"
  },
  {
    "objectID": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html",
    "href": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html",
    "title": "A Practical Approach to Model Error",
    "section": "",
    "text": "In this post, I want to get a better sense of the effects of model error by simulating a very simple model for the spreading of a virus. It’s based on an analysis performed by Nassim Taleb. I used the simulation below in the scope of a workshop paper on the effect of cognitive biases. The published version of the paper can be found here."
  },
  {
    "objectID": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html#footnotes",
    "href": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html#footnotes",
    "title": "A Practical Approach to Model Error",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven variations that are normally distributed, i.e., thin tailed.↩︎"
  },
  {
    "objectID": "insights/blog/2015-05-29-code-snippet-repository/index.html",
    "href": "insights/blog/2015-05-29-code-snippet-repository/index.html",
    "title": "Code Snippet Repository",
    "section": "",
    "text": "I’m jumping between Scala/Spark coding, some Javascript in between, Python/PySpark and then some R every now and then. This in itself is already a challenge, but the worst thing is that I frequently encounter situations where I think: I’ve encountered this situation before. In many cases, it’s a situation that required quite some work to resolve. You end up with two possibilities: 1) retrieve the solution from some code somewhere on your harddisk or 2) start finding the solution again from scratchGoogle.\nSo I’m now wondering if this could not be organized better… Of course it can, but how? Github has the ability to store snippets of code, I could just keep a file handy for every programming environment/language. What are you using?\nHere’s an example of a snippet of Scala code that I need a lot: configuring a Spark context to use my credentials (stored in environment variable) to connect to Amazon S3:\nval fs_s3_awsAccessKeyId = sys.env.get(\"AWS_ACCESS_KEY_ID\").getOrElse(\"&lt;key\")\nval fs_s3_awsSecretAccessKey = sys.env.get(\"AWS_SECRET_ACCESS_KEY\").getOrElse(\"&lt;key&gt;\")\nsc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", fs_s3_awsAccessKeyId)\nsc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", fs_s3_awsSecretAccessKey)\nHow do you cope with this challenge?\n\nBack"
  },
  {
    "objectID": "insights/blog/2009-06-19-how-to-cope-with-change-an-alternative-approach/index.html",
    "href": "insights/blog/2009-06-19-how-to-cope-with-change-an-alternative-approach/index.html",
    "title": "How to cope with change: An alternative approach",
    "section": "",
    "text": "CHANGE…\nA lot of people are involved in change: Every project is a change, and every change should be a project. In project management, one is used to document risks of the project and think about how to mitigate those risks. Sometimes, mitigating the risks can be costly. But on the other hand, having the risks and the possibility that something goes wrong may cost more.\nA risk that is often forgotten in projects (and thus also in processes of change) is the one of ‘changing people’s mind’. Letting a human being start working (or even thinking) differently is a great challenge.\nIn other words, there is a technological barrier and a mental barrier to any project or change process. Both generate their own set of risks to be taken into account. In many cases the mental barrier is forgotten or at least underestimated.\nA technique that is widely adopted in situations like this is ‘chunking’ the technology: go step-by-step. Start small, but gradually extend the scope of the change. Doing this on a technical level usually also impacts the mental barrier.\nRecently, I was talking to a project manager for a technology company involved in the ‘people change’ process in relation to nanotechnology and its applications in biomedicine. He told me that the mental aspect of the whole project, the fact that little things enter our body, or even remain there for years, is to a large extend not yet common ground in our society. This is a considerable risk for the development of new applications of nanotechnology.\nIn order to mitigate this risk, the project manager told me they sometimes developed a completely different product first (and freezed the other project) because this new product was easier ‘to sell’. So basically, you develop something in parallel in order for people to feel comfortable with this change because it will make future changes easier.\n“But this costs a lot of money!” Yes, obviously, but so does developing a product that has a high risk of not being adopted because the mental leap is too big.\n\nBack"
  },
  {
    "objectID": "insights/blog/2012-11-16-slm-introduction-part-1/index.html",
    "href": "insights/blog/2012-11-16-slm-introduction-part-1/index.html",
    "title": "SLM: Introduction",
    "section": "",
    "text": "We start this introduction with some examples of traditional metrics for measuring performance.\n\nExamples\nIn what follows, we give some examples of metrics that are traditionally used in SLM in a number of contexts. First, some callcenter metrics:\n\nAverage time elapsed for calls on hold\nAverage number of interactions with customer before problem solved\nNumber of escalations in a given period\n\nThe second set of examples are related to IT helpdesk services:\n\nAverage time taken to acknowledge a problem ticket\nAverage resolution time for an incident (with priority 1)\nPercentage of incidents with resolution time above threshold\n\nThe third set of examples has to do with IT operations:\n\nPercentage systems uptime\nUnexpected systems downtime\nMean time between system failures\n\nWe will refer to these examples in later posts. We now turn to some characteristics of the way most service levels are implemented.\nConsider the following (fake) recommendation for a hotel room:\n\nThe guaranteed average room temperature is 20 degrees Celcius.\n\nEven with a money back guarantee, I would not rent the room. Suppose its 40 degrees during the day and 0 degrees during the night? The average temperature may still be 20 degrees…\n\n\nAverages\nWhen dealing with customer interactions (calls), IT incidents and problems, etc., one usually deals with a large number of instances that are observed. Hundreds or even thousands of calls may be logged in a call center per day. In order to aggregate the information on these calls, one usually takes averages over a given period of time. This is reflected in the examples given before.\nAverages, however, are only a first order representation of a set of instances. One bad instance can easily be compensated by a good instance resulting in a good average. In other words, if the average on-hold time for a callcenter is 2 minutes, it may well be that I have to wait 10 minutes. If only at another time 5 calls are answered within a second, the average is back to 2 minutes.\n\n\nAverages and Sums\nOne often deals with lots of data points, for instance when considering incidents or service calls to a helpdesk.\nUsually, a (small) number of important metrics are selected and calculated on a regular basis (monthly, quarterly). The metrics are defined as averages or sums, such that one number is characteristic for the set of incidents, calls, problems, … that it covers.\nAn important metric is often called a Key Performance Indicator.\n\n\nService Level\nTraditionally, a service level is either defined as the result of one KPI or as the (weighted) average of a small set of KPIs, most often expressed as a percentage. This way, it is at best an average of a limited number of KPIs.\nIn practice, one often measures the service level and takes a baseline which is then used as benchmark.\n\n\nService Level Agreement\nA contract with a service provider typically includes several services. An agreement about each of these services and their respective targets and quality parameters is called a Service Level Agreement (SLA). Often, though, the term SLA is used not to denote the contract as such, but rather the Service Level calculated for all the services is in the contract.\n\nBack"
  },
  {
    "objectID": "insights/blog/2007-12-17-performance-monitoring-more-about-peaks/index.html",
    "href": "insights/blog/2007-12-17-performance-monitoring-more-about-peaks/index.html",
    "title": "Performance Monitoring: More about Peaks",
    "section": "",
    "text": "In a previous post, we talked about averages (types of averages) and peaks and how peaks can tell you something about the spreading (variance, standard deviation) of the data.\nInformation about peaks is required (especially in capacity planning situations) to understand the sizing of the platform you’re running on. On the other hand, having a peak utilization of (say) 80% and an average of 20% still does not tell you that much: how long was the system running at high CPU levels? Maybe only for 10 seconds during the day (a scheduled database operation, backup procedures, etc.)? Is it crucial for our service that this high level of CPU can be guaranteed at that moment, or is it affordable to let the application/server wait a little longer for CPU requests? Think of a mail server, for instance, where it wouldn’t be a big deal if the server would forward your mail a few milliseconds later or earlier (would it?).\nBasically, what we need is a load profile for a server. A load profile contains information like:\n\nLoad during hour, day, week, month (or any other relevant period for this server)\nExpected response times instead of observed response times (basically, a cutoff on the resources)\nCurrent hardware inventory\nCurrent ‘scaled’ hardware inventory (20% CPU usage is different for a quad core than a single core, a scaled inventory takes that into account and enables easy comparison of systems)\nEtc.\n\n\nBack"
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html",
    "title": "Bullet Proof Data Science in Scala",
    "section": "",
    "text": "In this post, we go over some typical aspects and challenges that occur in typical data science projects in order to extract some requirements for data analysis in the broad sense of the word. We then illustrate how we tackle these requirements in typical data science projects using Scala."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#joining-and-annotations",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#joining-and-annotations",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Joining and Annotations",
    "text": "Joining and Annotations\nThe reality is that often times several data sources need to be aggregated. Aggregation can be in two ways/directions:\n\nVertical: New data, similar to what we already have, and\nHorizontal: Additional information about data we already obtained."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#parsing-libraries",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#parsing-libraries",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Parsing Libraries",
    "text": "Parsing Libraries\nLast week I was parsing a tab-separated file with information about genomic variants (aka a VCF file). The parsing was done using a custom library created by a colleague of mine. I had uses the library before, but now suddenly it did not work anymore. All I got was a confusing error about some exception\n\nAnd then, the fun starts. How to find out where things go wrong and how to make sure you don’t have to rewrite (part of) the parser? It turned out additional fields can be added to a VCF format file, which the parser does not take into account."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Missing Values",
    "text": "Missing Values\nA similar issue occurs when dealing with missing values, or missing columns in the data. It’s very easy to end up with exceptions in the Scala/Java world or equivalent in other languages.\nThe challenge of missing data becomes even more concrete when additional data is aggregated. Suppose we have additional annotations about a subset of the data. There needs to be a way to cover situations like this."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#solving-the-challenge-in-scala",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#solving-the-challenge-in-scala",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Solving the Challenge in Scala",
    "text": "Solving the Challenge in Scala\nIn other words, you don’t have control over the input in most cases. In what follows, we describe an approach to data analysis in Scala that takes into account the above challenges. The approach is heavily based on principles of Functional Programming while capturing the data model in an object model."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values-1",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values-1",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Missing Values",
    "text": "Missing Values\nOn important aspect of the above challenges is missing values. Say you’re a service organization that keeps records of potential customers. Furthermore, say you want to analyze people’s hobbies. You would like to allow for a distinction between 3 situations:\n\nThere is no information about the customer’s hobbies\nThe customer does not have any hobbies\nThe customer has 1 or more hobbies\n\nSay you encode the hobbies as a List of String (free form), then (2) corresponds to an empty list and (3) corresponds to a list of n hobbies. But what does (1) correspond to? In R, one usually gets NA. In Java, one would often sees the occurrence of null, but using null is not a good habit.\nInstead, we use the Option type. It encapsulates whatever other data structure you want. The above 3 situations then correspond to:\n\nNone for no hobbies known about this person\nSome(List()) for this person does not have hobbies\nSome(List(hobby1, hobby2, ...)) for the hobbies for this person\n\nFor the FP (Functional Programming) people among us, the option is a Monad. But let’s not go there yet in order not to scare off the others…"
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#start-with-the-end-in-mind",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#start-with-the-end-in-mind",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Start with the End in Mind",
    "text": "Start with the End in Mind\nIn our experience, it makes sense to define a good model for the data after aggregation and processing and capture this in an object model. This model need to be the one that can be used as input for a machine learning library as such, but it should capture the logic of the application domain.\nAnd Scala’s case classes come in very handy. We will not cover the specifics or benefits of case classes here, but remember this: always put case in front of your class definition. Oh, and while you’re at it, add val before every class parameter as well.\nFor instance, and to be in line with the discussion above, we could define a Name class definition as follows:\ncase class Name(val firstName:Option[String], val lastName:Option[String])\nOne can add safe getters and setters if necessary:\ncase class Name(val firstName:Option[String], \n                val lastName:Option[String]) {\n    def getSafeFirstName = firstName.getOrElse(\"First name not known\")\n    def getSafeLastName  = lastName.getOrElse(\"Last name not known\")\n}\nPlease note that the safe getters return a String, even if the value is not available."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#safe-transformation",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#safe-transformation",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Safe transformation",
    "text": "Safe transformation\nOnce we encapsulated the data in an Option, we can safely process this data as well. There are multiple way to do this, but all come down to the same principle:\n\nOnly process values that should be processed, so don’t process missing entries, and\nMake sure the processing itself is safe by catching exceptions where necessary.\n\nComing back to our example: In practice one seldom gets a dataset with first name and last name clearly split. We could store input as well, and define a companion object in order for people to easily use the API:\ncase class Name(val unparsed:Option[String], \n                val firstName:Option[String]=None,\n                val lastName:Option[String]=None)\n\nobject Name {\n    def apply(unparsedStr: String) = new Name(Some(unparsedStr))\n    def apply(unparsedStr: String, fnStr:String, lnStr:String) = {\n        new Name(Some(unparsedStr), Some(fnStr), Some(lnStr))\n    }\n}\nWe used default values for the first and last name. Imagine now what you can do with an object model like this:\nval name1BeforeParsing = Name(\"John Doo\")\nval name2BeforeParsing = Name(\"Bar Foo\")\nval name3BeforeParsing = Name(\"Franz Octupus\")\n\n// A very crude parsing function\ndef parseName(in:Name):Name = {\n    val names = in.unparsed.map(_.split(\" \"))\n    in.copy(firstName = names.map(_(0)), lastName = names.map(_(1)))\n}\n\nval name1 = parseName(name1BeforeParsing)\nval name2 = parseName(name2BeforeParsing)\nval listOfPersons = List(name1, name2, name3BeforeParsing)\nNow, obviously there are lots of problems with this approach to parsing the name. One could improve this in many ways. For instance, we did not take into account a middle name. One approach to improve the parsing itself could be to use a database of common first names and last names. But that’s not the aim of this post. So let us continue with the important stuff here. Let us convert all first names to initials in a safe way:\ndef getInitials(s: String):String = s.toCharArray.head.toString.toUpperCase + \".\"\n\ndef firstNameToInitials(name: Name):Name = name.firstName match {\n    case Some(nameString) =&gt; name.copy(firstName = Some(getInitials(nameString))\n        )\n    case None =&gt; name\n}\nThe result is as follows:\nlistOfPersons.map(firstNameToInitials).foreach{println}\nName(Some(John Doo),Some(J.),Some(Doo))\nName(Some(Bar Foo),Some(B.),Some(Foo))\nName(Some(Franz Octupus),None,None)\nSee how the function gracefully managed to pass over nonexistent values! The initials of nothing is still nothing. It does not end here, though, because we managed to cope with missing values, but we did not yet make the transformation fully bullet proof! See what happens in the following case:\nfirstNameToInitials(parseName(Name(\"Jacobus\")))\njava.util.NoSuchElementException: next on empty iterator\n  at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n  at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n  at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n  at scala.collection.IterableLike$class.head(IterableLike.scala:107)\n  at scala.collection.mutable.ArrayOps$ofChar.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:222)\n  at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n  at scala.collection.mutable.ArrayOps$ofChar.head(ArrayOps.scala:222)\n  at .firstNameToInitials(&lt;console&gt;:10)\n  ... 33 elided\nAn easy way to solve this, and one that is compatible with our approach thus far is by using Try in Scala. You have two choices now, which depend on how you want the API to work:\n\nWhen an exception occurs during the transformation, let the result be None. So in other words, let it correspond to a missing value.\nWhen an exception occurs, insert a default value\n\nBoth scenarios are shown below:\nimport scala.util.Try\n\nval DEFAULT:String = \"\"\n\ndef getInitialsDefault(s: String):Option[String] = {\n    Some(Try(s.toCharArray.head.toString.toUpperCase + \".\").toOption.getOrElse(DEFAULT))\n}\n\ndef getInitialsOption(s: String):Option[String] = {\n    Try(s.toCharArray.head.toString.toUpperCase + \".\").toOption\n}\n\n\ndef firstNameToInitials(name: Name, \n                        getInitialF:String=&gt;Option[String]):Name = {\n    name.firstName match {\n        case Some(nameString) =&gt; name.copy(firstName = getInitialF(nameString))\n        case None =&gt; name\n    }\n}\nAnd use it as follows:\nfirstNameToInitials(Name(\"def\", \"\", \"def\"), getInitialsDefault _)\nres: Name = Name(Some(def),Some(),Some(def))\n\nfirstNameToInitials(Name(\"def\", \"\", \"def\"), getInitialsOption _)\nres: Name = Name(Some(def),None,Some(def))\nBy now, the FP adepts give us 1 point for keeping our data structures immutable: the above functions do not mutate the name object, but rather instantiate a new one.\nBut we receive a negative score on omitting to acknowledge that Option is a Monad and that a much better way of writing the above exists:\ndef getInitialsDefault2(name: Option[String]):Option[String] = \n    name.map( s =&gt; Try(s.toCharArray.head.toString.toUpperCase + \".\").getOrElse(DEFAULT))\n\ndef getInitialsOption2(name: Option[String]):Option[String] = \n    name.flatMap( s =&gt; Try(s.toCharArray.head.toString.toUpperCase + \".\").toOption)\n\ndef firstNameToInitials2(name: Name, \n                         getInitialF:Option[String]=&gt;Option[String]):Name = {\n    name.copy(firstName = getInitialF(name.firstName))\n}\nThese functions do not exactly do the same as the earlier functions, but they illustrate a very important concept: map or flatMap on None results in None. So, there is no need to explicitly use pattern matching here. In order to parse the list of names, we simply map over it (indentation added for ease of reading):\nlistOfPersons.map(firstNameToInitials2(_, getInitialsDefault2))\nres: List[Name] = List(\n                    Name(Some(John Doo),Some(J.),Some(Doo)), \n                    Name(Some(Bar Foo),Some(B.),Some(Foo)), \n                    Name(Some(Franz Octupus),None,None))\nFor a nice and graphical introduction to Monads and related concepts, I recommend the following blog post. Transformations like the above can be composed in a bullet-proof way, once a missing value or exception occurs, we either continue with a default value or with None.\nWe can go a bit further still for the FP fanatics among us. The function firstNameToInitials in fact is the setter part of what is called a Lens. We will come back to this later in this post."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#immutability-and-lenses",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#immutability-and-lenses",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Immutability and Lenses",
    "text": "Immutability and Lenses\nPlease note the examples above do not mutate any objects. We use the copy method to create an updated version of an object. We will not discuss the benefits of this kind of programming, but just mention the use of a Lens in order to update an immutable data structure.\nSeveral libraries exist for working with Lenses, some based on Scala macros others more high-level. For the sake of the argument, we lift the Lens definition out the Scalaz project:\ncase class Lens[A,B](get: A =&gt; B, set: (A,B) =&gt; A) extends Function1[A,B] with Immutable {\n  def apply(whole: A): B   = get(whole)\n  def updated(whole: A, part: B): A = set(whole, part) // like on immutable maps\n  def mod(a: A, f: B =&gt; B) = set(a, f(this(a)))\n  def compose[C](that: Lens[C,A]) = Lens[C,B](\n    c =&gt; this(that(c)),\n    (c, b) =&gt; that.mod(c, set(_, b))\n  )\n  def andThen[C](that: Lens[B,C]) = that compose this\n}\nA lens for first name can then be defined as such:\nval aLens = Lens[Name,Option[String]](\n    _.firstName, \n    (o, value) =&gt; o.copy(firstName = value))\nSo, you provide two functions: a getter and a setter.\naLens.get(listOfPersons(1))\nres: Option[String] = Some(Bar)\n\naLens.set(listOfPersons(1), Some(\"Barby\"))\nres: Name = Name(Some(Bar Foo),Some(Barby),Some(Foo))\nIn this case the usage of a lens is almost trivial because using the copy method on a class is an easy thing to do. But when you start to nest classes to create a more complicated model, multiple copy calls are required. Lenses are a good alternative in that case."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-reading-of-data-sources",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-reading-of-data-sources",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Bullet Proof Reading of Data Sources",
    "text": "Bullet Proof Reading of Data Sources\nThe foundation of the bullet proof approach thus far clearly is the Option type, aka the Maybe Monad in Haskell. We already used Try to catch exceptions in a user-friendly and safe way.\nUsing Try while reading a file allows us to cope with missing values, columns or when transforming numbers encoded in strings to integers or floating point numbers. In every case, we have the option to cast the possible exceptions to None (missing value) or a default value."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-aggregation-of-data-sources",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-aggregation-of-data-sources",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Bullet Proof Aggregation of Data Sources",
    "text": "Bullet Proof Aggregation of Data Sources\nAs mentioned already it is seldom the case that all data is available from one input file/database. Sometimes additional annotations need to be added (horizontal aggregation), coming from a different source. Sometimes, more data should be added which lacks certain features that the already parsed data contains (vertical aggregation).\nThere are several approaches to this. One is to go from one class-representation to another. But in this case, classes should be closely matched to the source of the data. We use a different approach, also making use of the (… tada! …) Option type.\nSince a) the data ready for analytics should be in denormalized form, and b) we already have a model for that data that is able to cope with missing values, it is not hard to start from the data source that contains the most information about the denormalized form of the data. All fields/features that are not available in this first data source remain None (aka the default).\nAdding additional features later can easily be done by updating the features from None to Some(...)1. Adding additional data vertically can be done in the same way. It’s perfectly fine to end up with a data structure where most of the rows have None for a certain feature but some contain more information. And since our transformations are bullet proof, all runs safe."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#footnotes",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#footnotes",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis even reads nice!↩︎"
  },
  {
    "objectID": "insights/blog/2012-09-18-choosing-the-right-cover-photo/index.html",
    "href": "insights/blog/2012-09-18-choosing-the-right-cover-photo/index.html",
    "title": "Choosing the right Cover Photo",
    "section": "",
    "text": "Say you are the editor in chief of a magazine and you have to decide among two possible cover photos. The magazine draws a significant part of its sales from customers that are not subscribers. In other words, you know the cover is important. Which one do you choose?\nOne editor in chief of a magazine I met recently takes the following approach in this situation: they send both photos to a large number of readers asking them which one they like most. The majority vote wins.\nThis is a typical crowdsourcing approach and probably the best option available. James Surowiecki discusses crowdsourcing in detail in his book ‘The Wisdom of Crowds: Why the Many Are Smarter Than the Few’.\nAdditionally, I argue that the crowdsourcing approach yields a better customer experience because readers are involved in the selection process.\n\nBack"
  },
  {
    "objectID": "insights/blog/2012-01-17-infografiek/index.html",
    "href": "insights/blog/2012-01-17-infografiek/index.html",
    "title": "Infografiek",
    "section": "",
    "text": "There’s a new word in the Dutch language: infografiek. It’s clearly derived from the English infographic. If this is a trend, it’s only the start. FlowingData has a nice overview of different names that are all related to this new type of journalism and reporting which blends data with visuals.\nThe power of this approach to reporting is that it can yield better insight and understanding. The dangers are twofold:\nFirstly there’s the risk of overshooting, of spending more time and effort in using a visual approach but ending up with something that doesn’t improve understanding nor insight.\nSecondly (and more profoundly), there’s the risk of oversimplication or drawing false conclusions. That is part of what EagerEyes refers to when citing Jason Moore’s version of the Hippocratic oath for infographics.\nLet’s hope the Dutch speaking community of artists and journalists who deal with ‘infografieken’ do it in a scientifically correct way."
  },
  {
    "objectID": "insights/blog/2014-09-23-impact-in-risk-assessment/index.html",
    "href": "insights/blog/2014-09-23-impact-in-risk-assessment/index.html",
    "title": "Impact in Risk Assessment",
    "section": "",
    "text": "How many times have you seen project charters or business cases where a form of risk assessment has been provided? In many cases, people try to make the risks tangible and actionable by means of a risk matrix. In this approach, every risk gets 2 parameters associated to it: probability of the risk occurring and impact when the risk occurs. In most cases, 3 possibilities are given for both parameters: low, medium, high.\nThere are two very big advantages to this approach:\n\nIt gives people a framework, a way to quantify risks that is easy to understand\nThe method not only looks at the probability of an event, but also at its impact. The latter has practical consequences.\n\nThere are also disadvantages to this approach. Some are mentioned already on the Wikipedia page, but three are missing, in my opinion:\n\nMany events with high impact a) have often not been taught of as a risk or b) are the consequence of a combination of factors.\nUsually, events that are rare have a bigger impact. But the probability of rare events is very hard to assess.\nImpact appears to be a linear function, but it is not.\n\nLet us give an example of the latter point. One computer failing in the office has small impact because the user may get a replacement lying around in 10 minutes or less. 10 computers failing in the office will require more time, because no 10 people are standby to service them in parallel and spare computers may be lacking. Imagine a fire, a hack, or something we can not think of now (see above)? When hundreds of computers need to be replaced?\nThis is a situation where the impact of an event increases faster than linear (polynomial? exponential?). It may get worse, when an event causes an impact that is not recoverable anymore: too much electricity can kill a person, too many losses can ruin even a bank or a state, etc.\nThe first two arguments can be thought of as applications of the Black Swan concept to projects and risk. The last argument is an application of the concept that is elaborated on in a book by the same author: Antifragile. Books could be written just applying concepts from these books!\nWhat does it tell us? When considering mitigation of risks, think about what are possible consequences on a large scale, not on the scale of individual events. Think about consequences like: nobody is able to work anymore, the whole building is destroyed, our competitor has leapfrogged us, etc. Thinking about mitigation of these consequences may tell you more about the underlying risks and events than the other way around.\nOn a slightly cynical note: While doing so, you may find that more (advanced) technology is not always a solution because it usually makes things more complex and thus prone to more complex risks rather than avoiding them.\n\nBack"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "",
    "text": "It’s been a while since I first wrote about tackling model error using a simple model. It’s about time to come back to it.\nWhat triggered the current post is the opportunity I was given to give a masterclass in the Evidence and policy summer school. Since my masterclass is about uncertainty in decision making, it seemed like a nice opportunity to look back at the simple model."
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-1-no-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-1-no-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 1: No Randomness",
    "text": "Step 1: No Randomness\nBy clicking on the parameters of the simulation, or below in the footer on the words can be changed, the simulation settings can be adjusted.\nSet the variables such that there is no randomness applied to the scenario. Only one scenario is sufficient in this case:\n\nAnd see what the result looks like:"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-2-a-bit-of-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-2-a-bit-of-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 2: A bit of Randomness",
    "text": "Step 2: A bit of Randomness\nIn step 2, we add a bit of randomness. For this we use the default settings, although you could increase the number of scenarios if you wanted to:\n\nThe result should look similar to the following. Please note that randomness has been added, so your result should not look exactly like the one presented here!"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-3-a-bit-more-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-3-a-bit-more-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 3: A bit more Randomness",
    "text": "Step 3: A bit more Randomness\nIn step 3, we increase the amount of randomness added to the scenarios. For instance:\n\nNow, given more randomness in the scenarios, yours might be completely different from the one below:"
  },
  {
    "objectID": "insights/blog/2008-01-08-capacity-monitoring-for-desktops/index.html",
    "href": "insights/blog/2008-01-08-capacity-monitoring-for-desktops/index.html",
    "title": "Capacity Monitoring for Desktops?",
    "section": "",
    "text": "With the rise of VDI (Virtual Desktop Infrastructure, see here for a comprehensive overview) and the momentum it has, one starts to ask similar questions as with conventional server consolidation: what type of virtualization platform is required, how many users/desktops can I host, will there still be room for scaling, etc.\nThe way to approach this in server virtualization projects is by means of capacity monitoring and planning using virtualization scenarios. We refer to earlier posts for more information about this topic.\nThe question we are asking: can we use the same concepts and ideas for desktop virtualization? My answer is ‘NO’, because:\n\nA user acts completely different than a process or service: less predictable, depending on our mood, depending on the time of the day, etc.\nSome end-user applications ask 1OO% of the CPU even while they are not doing anything. The classic example used to be the game ‘Pinball’. Even a screensaver can take a large amount of CPU power.\nIn a VDI context, this becomes even more important. For instance: why would you scale your virtual desktop CPU and memory to include desktop search if really nothing personal can be found on the hosted desktop?\nWhen starting and running 10 applications at the same time, we will probably only use 5 of them later, but still… they require CPU power (and memory) while seeming idle.\nIf a user has meetings half the time, does that mean his session is closed? Does it still require processing power? How can this be analyzed?\nEtc.\n\nIn other words, a desktop environment is inherently different from a server environment. This is why in my opinion it is harder to maintain a Citrix farm than a VMware farm: applications and users tend to be less predictable and ‘stable’ than servers.\nDoes that mean that we can not do any sizing or planning in a VDI context? On the contrary, we should only keep in mind that applying exactly the same reasoning as with server consolidation planning is not a good idea.\nA few more tips:\n\nMake sure you have an overview of running processes and their CPU/memory utilization. This helps in deciding what is really important.\nBe careful with averages and peaks: a PC that is running all the time will have a low average, but may be used heavily during the day and a PC may be 100% used during the day because of some applications.\nTake into account inactive time during the day.\nDo not try to analyze the ‘average’ user, instead create classes (say 5 or so) that each have typical characteristics. Use these classes to create the virtualization scenarios.\n\n\nBack"
  },
  {
    "objectID": "insights/blog/2007-10-25-vmware-capacity-planner-taking-the-data-offline-part-1-introduction/index.html",
    "href": "insights/blog/2007-10-25-vmware-capacity-planner-taking-the-data-offline-part-1-introduction/index.html",
    "title": "VMware Capacity Planner: taking the data offline - introduction",
    "section": "",
    "text": "Lately, I have been involved in some VMware Capacity Planner (VMCP) tracks. VMCP deals with monitoring a set of (physical) servers in order to assess possible virtualization scenarios.\nVMCP works in the following way: a monitoring server is set-up with a tool that does (but is not limited to) regular performance measurements. This tools sends the data to a VMware database through the web (over a secure channel). Via a web interface, one can then query the information, get reports, view trends, show graphs, configure and create consolidation scenarios, etc. Usually, we let the system run for around 4 weeks to get a realistic idea of the performance characteristics.\nWhat I like about VMCP is that the data is not located at the customer site, and available at all times (once it has been uploaded). This gives me the opportunity to regularly check on the status of the performance measurements.\nThe biggest disadvantage of VMCP is that the web interface is not the most flexible and fast interface around. Some things I would like to do are not available (lack of flexibility) but could easily be done in, e.g., Excel and at times when everyone in the world is awake it takes ages to refresh a page and get the result of a query. Moreover, it is not easy to get good-looking information to paste in a document.\nWhen it comes to writing a report, the customer is obviously not only interested in a statement like: you will need 5 ESX server of type X to cope with the load. Therefore, I like to add tables with the most useful metrics (CPU, network, Disk I/O, …) for later reference. I add this information as an appendix.\nThis is where I would spend at least half a day exporting CSV files from the VMCP website, loading them in Excel, laying it out as a nice table and paste it in the document. I started thinking about automating some of the steps required, and I covered the most time-consuming already: exporting the information from the website as a CSV file.\n\nBack"
  },
  {
    "objectID": "insights/blog/2015-10-29-thoughts-on-the-spark-summit-europe-2015/index.html",
    "href": "insights/blog/2015-10-29-thoughts-on-the-spark-summit-europe-2015/index.html",
    "title": "Thoughts on the Spark Summit Europe 2015",
    "section": "",
    "text": "In this post, I summarize some of the things I picked up at the Spark Summit. Some of these require more thought or research, but at least I’ll have a to do list of things to look back at.\nIn the meanwhile, a lot of the talks are posted already on Youtube.\nSo, let’s get started.\n\nDataframes\nI’ve been putting off using dataframes for a year now. There are two main reasons for doing this:\n\nI like the (functional) API of the RDD’s more than the dataframes API.\nI can do more with the RDD API which I need for most of the applications we’re working on.\n\nUnfortunately, a lot of the cool work on Spark and especially the machine learning (ML) and optimization (Tungsten) work is currently focussed on dataframes. I understand that, multiple people have explained why that is, but still I keep on thinking about the two reasons above.\nIn the next release (1.6), a new API will be available: DataSets. And it is exactly what I need to be happy using dataframes: The dataframe API, extended such that you can use the traditional functional API as well.\n\n\nsparkR\nFor the same reason I hadn’t used dataframes yet, I have not yet tried out using sparkR: it uses dataframes. The limitations of the dataframe means that when it comes to transforming the data in a dataframe, you are limited to the functions that are provided.\nIn Scala, it’s possible to define a UDF (User Defined Function), but until now that was not possible in sparkR. The next version of Spark, however, should support just that.\n\n\nDatabricks Cloud\nI’ve been really impressed by the Databricks notebooks. Having worked with different Open Source notebooks already (iPython, Zeppelin, Spark-Notebook), this one is definitely better.\nSome highlights:\n\nVery intuitive interface and extremely easy to attach a cluster to a notebook\nMultiple users on the same notebook works seamlessly, and much like editing a Google Document\nGood revision history\nImport code from files (%run ...) or from Github\nIncluded REST Server\nInteroperability between different notebooks\n\n\n\nSpark Streaming\nSpark streaming has long been the part of the Spark package that was least covered and used. Everyone found it cool and interesting.\nNow, during the talks, it became clear that the streaming aspect of Spark is very important. Lot’s of use cases described how they use Spark Streaming in their application, even in production.\n\n\nML pipelines\nI read about it. It looked cool on paper. Turns out it is even better in practice.\nMany times, different steps occur when going from a dataset to a model and predictions. Often-times these steps are poorly documented and as a consequence not reproducible.\nNow, with pipelines, you get a high-level API for configuring and running the different stages of a process. Downside is that it is yet another API to learn. But in this business, we don’t mind learning a few APIs…\nIt becomes even more interesting when the set of underlying algorithms and methods gets extended: hyper-parameter tuning, ML models, …\n\nBack"
  },
  {
    "objectID": "insights/blog/2012-12-04-slm-issues-with-traditional-slm-part-1/index.html",
    "href": "insights/blog/2012-12-04-slm-issues-with-traditional-slm-part-1/index.html",
    "title": "SLM: Issues with traditional SLM",
    "section": "",
    "text": "We introduced some basic aspects of SLM in the previous posts. We now turn to a list of issues with traditional Service Level Management.\n\nQuantity instead of Quality\nIn general, it is easier to define a metric or KPI that is based on a quantitative parameter: the number of incidents, number of escalations, number of errors, minutes downtime, etc. That is the reason that this type of parameters are often found in SLA’s. In a lot of cases, quantity is not related to quality.\n‘The number of incidents resolved’, for instance, is not a good quality parameter. It may well be that some of the incidents took years to acknowledge. ‘The number of incidents resolved in an agreed time frame’ is better, but the resolution may be only a temporary workaround? Or, in order to attain the required time frame, staffing is doubled and costs skyrocket.\nWe can go on giving examples like this. Sometimes, quantitative information is good, or can be rephrased to be meaningful. Sometimes quantitative parameters can be used as informational. But be avoid drawing up an SLA on the basis of purely quantitative parameters only.\nThere is another way to handle quantitative KPIs, but we leave this for a later post.\nIn our series of posts concerning SLM, SLAs, etc. we have started considering aspects of traditional SLM that lead us astray. We continue with the quantitative versus qualitative discussion.\n\n\nLack of representativeness\nThe reason for defining metrics and KPIs was to measure the quality of a service. Quality is an abstract and broad notion that may have a subjective connotation that is hard to put in words. In most cases, KPIs are a representation of an aspect of the service, not the quality of the service as a whole. Performance related metrics that deal with speed or turnover, for instance, are often used as KPIs (see examples 1, 2, 4, 5).\nPerformance (typically measured by the quantitative types of metrics, see here) is only part of the quality that is expected. Consider the following two examples:\n\nA ticket can be solved in the requested amount of time but using a temporary solution. Overall quality is suffering, having an impact on customer satisfaction.\nThe time on hold at a service desk is too high according to company standards, but most cases are resolved during this one call meaning that the effort pays and customers are positive about the service.\n\nIn other words, the lack of representation can go both ways: the quality may be higher or lower than expected, based on  the performance KPI.\nAnother example: It is true that, on average, a customer that has to wait long on the phone before getting someone on the phone will likely be less happy than someone who does not have to wait at all. It is true that an IT issue that is resolved in 1 hour will be more appreciated than one that takes 2 days. But living in the illusion that fixing all IT incidents within an hour is the way to have a perfect IT service desk, is doomed to fail.\n\n\nThe wrong way around\nThe practice of defining (mainly performance-based) KPIs is wide-spread, so wide-spread in fact that often the reasoning is reversed: Instead of measuring a set of KPIs that may have a positive influence quality, one often (implicitly) states that the quality of a service is defined as the outcome of these KPIs.\nTo give an extreme example: Suppose the weather is proven to have an impact on customer satisfaction of a call center. Is it rational to state that the wheather defines quality? Should the Service Level be calculated based on the weather statistics? I don’t think many managers would buy this.\nOn the other hand, when Service Levels are defined, one often finds quality defined as just one parameter like, e.g., ‘waiting time on-hold’ or ‘system uptime’.\n\n\nFocus on the wrong instances\nA consequence of dealing with averages and aggregate metrics is that in some cases, when a threshold value is reached there is no longer an incentive to act on the individual instance that went wrong. As an example, consider example 6 given before. If the resolution time of an incident is above threshold, it is counted in the percentage and it will not get any better over time, but also not worse! In other words, there is no longer a reason to fix the incident as soon as possible. Rather, it may be better to focus on other incidents that can still be fixed within threshold.\nThe result may be a large number of open incidents and thus unhappy customers, and rightly so.\n\n\nGaming the system\nWe start by considering metric 1 in the examples given before, which is related to the on-hold time of a call. There are several ways to avoid the penalty of crossing the threshold for this metric:\n\nOne such way is to drop the call (a technical error is always possible after all).\nAnother way is to have an automated answering machine ask the user some (possibly irrelevant) questions.\nA third option is to answer the call but forward it to a different team as soon as the user starts explaining the question or problem. In none of these cases, the service can be called qualitative, but the appropriate metrics are perfect. Other metrics can be gamed in similar ways.\n\nTo make things worse: Since usually only a small amount of KPIs are selected for monitoring service quality, they are easy to keep an eye on and follow-up. It is thus possible to make sure most of the metrics are managed in such a way that success is guaranteed.\nIf you combine this effect with the earlier issues one gets a nice collection of, say 5 KPIs and concludes that the service quality is optimal.\n\n\nAvoid Accountability\nWhen all goes wrong, and the KPIs are showing bad performance, there is one last option: avoid accountability. For instance, because other teams have not done their job which caused the resolution time to be above threshold. This can quickly lead to long discussions and eventually mistrust between teams and companies.\n\nBack"
  },
  {
    "objectID": "insights/blog/2007-12-14-performance-monitoring-averages-and-peaks/index.html",
    "href": "insights/blog/2007-12-14-performance-monitoring-averages-and-peaks/index.html",
    "title": "Performance Monitoring: Averages and Peaks",
    "section": "",
    "text": "Now that we are into the topic of performance (of capacity) monitoring and planning, let us continue with something that has kept me busy the last couple of days: averages of performance data (and other statistical information) versus peaks.\nThis goes back to a classic textbook example in statistics, where the mean value of a series of data points is completely irrelevant as a representation of the data points itself. Let us consider the following example. Given a series of data like in the table below:\n\nX Y\nA 1\nB 2\nC 4\nD 7\nE 100\nF 4\nG 9\nH 7\nI 3\nJ 5\n\nThese may, for instance, represent scores (0-100) given to students (by a very strange teacher). In a graph, this is presented below:\n\n\n\n\n\nThe red line represents the average value. It is clear that everyone (except the teacher’s little friend with 100 points) is below the average. As a consequence, the average is not a good representation of the data as a whole. Some say it is too much influenced by the extreme values. In fact, this average (sum of the data values divided by the total amount of data points) is called the arithmetic mean. There is another notion of ‘an average’ which is called ‘geometric mean’. In the example above, the value of it would be 5.4 which is much more relevant. The median would even better define the data set, but that would lead us too far.\nBasically, the fact that the arithmetic mean does not give a good indication of the data set is caused by the large spread of the data. In statistics, there is another indicator for this: the variance, or standard deviation. It is a measure for how close or far apart the values are. In the example above, the standard deviation. In our example above, it would read 30.2. Suppose the value of E would be 10 instead of 100, the standard deviation becomes 3. In others words, the lower is the standard deviation, the more the data is ‘close’.\nThe above brings me back to performance monitoring. Somehow, I want to summarize the performance data of a server by a small set of indicators (averages of time etc.) that give a reasonable picture of the actual performance or in other words: that are representable for the system’s actual performance. Typically, when looking at the percentage a CPU is used over time, we see fluctuations that are similar to the figure above (don’t believe me, below is an actual example of a system with some high peaks and further sitting idle for most of the time - this server has 4 cores by the way). We conclude that, therefore, it does not make much sense to look at simple averages of the performance counters in order to get an idea of the behavior of the system..\n\n\n\n\n\nComing back to VMware Capacity Planner: the tool keeps track of the average value (yes, the simple average that does not say much) but also internally uses the geometric mean but not the variance (as far as I can tell). From this perspective, the whole performance gathering using this tool would be worthless. Luckily, it also keeps track of peak values (and calculates averages over these peaks but that is another story). Comparing the peak values with the average tells us a lot about the spread of the data points. The system behind the graph above, has an average CPU value of less than 20% while the peak CPU utilization is higher than 90%! This tells us that the variance/spreading in data points is large.\nIn a virtualization/consolidation assessment, these cases have to be taken into account, as we do not want our systems to become unresponsive because they have their peaks at the same time.\n\nBack"
  },
  {
    "objectID": "insights/blog/2020-12-15-diflow/index.html",
    "href": "insights/blog/2020-12-15-diflow/index.html",
    "title": "DiFlow",
    "section": "",
    "text": "Developing and maintaining pipelines/workflows can be a genuine challenge. Doing this in a collaborative context adds even more to this complexity. We all dream of a flexible platform that allows us to easily express the computational requirements and is able to then run those optimally.\nData Intuitive is working in a project where such a pipeline is being developed. The choice of the platform is NextFlow and very early on we decided that we wanted to use DSL2 even if it was very early stages.\nIt turned out, though, that DSL2 in itself did not yet grant us enough flexibility. We ended up creating a set of conventions for creating modules that enable us to achieve our collaborative development goal.\nA DiFlow pipeline is a combination of modules:\n\nA module contains one step in a larger process\nEach module is independent\nA module can be tested\nA module runs in a dedicated and versioned container\nA module takes a triplet as argument:\n\n[ ID, data, config ]\nPlease refer to the GitHub repository for more information and documentation.\n\nBack"
  },
  {
    "objectID": "insights/blog/2007-12-14-capacity-planning-what-to-monitor-and-how-to-interpret/index.html",
    "href": "insights/blog/2007-12-14-capacity-planning-what-to-monitor-and-how-to-interpret/index.html",
    "title": "Capacity Planning: What to monitor and how to interpret",
    "section": "",
    "text": "Capacity planning starts with capacity (of performance) monitoring.\nEverybody who is involved in the monitoring of systems will acknowledge that the most difficult aspects in monitoring a server (or set of servers) are:\n\nFinding the proper indicators for the performance of the system (CPU usage, CPU cycles, memory usage, paging, etc.)\nMaking sure they are queried regularly, but not too much in order to avoid impacting the performance of the system by monitoring it.\nStoring the resulting data\nSummarize, create views, average, etc. (this also depends on what you want to know about the system)\nAnalyze, interpret, etc.\n\nDid I say the most difficult aspects? Are there any other aspects? Well, not really… capacity monitoring (and planning as a further step) is not an easy task:\n\nAre you aware of the utilization of your systems? Even of your workstations?\nWould you have any idea how many of your servers could be placed on a virtualization platform with a specific set of hardware characteristics?\nWould you know when your mail server had the hardest time managing mail boxes the last couple of weeks?\n\nProbably the answer is ‘no’. Maybe the answer is ‘I don’t care’?\nMost companies do care, because of several reasons: cost, manageability, flexibility, scalability, environment, space, etc.\nThere are already some players on the market: VMware Capacity Planner (see earlier posts), PlateSpin PowerRecon, Veeam Monitor, etc. I’m mostly used to VMware Capacity Planner (VMCP) but recently, I have also evaluated both PowerRecon and Veeam Monitor.\n\nBack"
  },
  {
    "objectID": "insights/blog/2007-12-17-performance-monitoring-correlations/index.html",
    "href": "insights/blog/2007-12-17-performance-monitoring-correlations/index.html",
    "title": "Performance Monitoring: Correlations",
    "section": "",
    "text": "Although I have been arguing that performance monitoring and capacity planning require a decent server montoring environment, it also requires more. This extra part comes from the fact that often services depend on each other. A web service connects to a database (hosted on a different server) and fetches data from the file server (local, or SAN/NAS). Often, one part in the chain is a bottleneck for the whole of the process. This is a shame and can be avoided by careful analysis of the correlations between performance data.\nAgain, this is an argument in favor of what I called a ‘load profile’ earlier. By modeling a server by means of a load profile, we get a representation of that server in terms of measurable quantities. Statistics and mathematics in general can then help us analyze the correlations between those load profiles.\n\nBack"
  },
  {
    "objectID": "insights/use_cases/openpipelines/index.html",
    "href": "insights/use_cases/openpipelines/index.html",
    "title": "OpenPipelines",
    "section": "",
    "text": "Breakthroughs in single-cell genomics have been made possible by the simultaneous advancement of experimental and computational technologies. While experimental techniques standardize quickly, computational analysis pipelines are more challenging to compare and thus vary. Analysis platforms such as Seurat and Scanpy have facilitated pipeline building and introduced basic quality standards. Furthermore, we have made first attempts at standardizing workflows for scRNA-seq to bridge the language-gap between these platforms and their users.\nHowever, as new analysis methods are being proposed at an increasing rate and novel multi-omic sequencing technologies are becoming commonplace, there is an urgent need for new analysis standards for single-cell (multi-)omics data.\nOpenPipelines are best-practice workflows for single-cell single- and multi-omics data. To ensure these workflows are accessible to non-experts and can be deployed in a fast and reproducible way, we will build these into reproducible, modular and updatable best-practice analysis pipelines using industry-standard workflow tools, high-performance versions of popular methods and an interoperable, language-independent framework.\n\nBack"
  },
  {
    "objectID": "insights/use_cases/openproblems/index.html",
    "href": "insights/use_cases/openproblems/index.html",
    "title": "Open Problems for Single-Cell Analysis",
    "section": "",
    "text": "OpenProblems is an open-source platform for benchmarking and formalizing computational tasks in single-cell analysis. The platform aims to facilitate the development of new methods for single-cell omics by defining mathematical interpretations of tasks, providing publicly available gold-standard datasets in standardized formats, defining quantitative metrics to evaluate success, and ranking state-of-the-art methods in continuously updated leaderboards. OpenProblems is hosted on GitHub and evaluated using AWS with support from the Chan Zuckerberg Initiative.\nViash is being used in OpenProblems to streamline the development, execution, and sharing of methods, metrics and dataset loaders. It allows users to define their components as reusable modules that can be combined to form pipelines, and it manages dependencies, versioning, and execution on different computing environments. Viash also provides a user-friendly interface for configuring pipelines and visualizing results. By using Viash, OpenProblems can provide a flexible and scalable platform for benchmarking single-cell analysis methods.\n\nBack"
  }
]