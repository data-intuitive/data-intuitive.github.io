[
  {
    "objectID": "contact/thank_calendar_booking.html",
    "href": "contact/thank_calendar_booking.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Thank you for booking a meeting with us through our website!\nWe are thrilled to meet you and discuss your project further. If you’d like to share any interesting information or materials related to your project ahead of our meeting, please feel free to send them to info@data-intuitive.com. Don’t forget to mention the date and time of the meeting you’ve booked in your email.\nWe look forward to our conversation and exploring how we can collaborate.\nWarm regards,\nThe Data Intuitive Team"
  },
  {
    "objectID": "commontext/intro.html",
    "href": "commontext/intro.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Data Intuitive is a pioneering company specialising in state-of-the-art biotech data workflows. We excel in developing multi-omics bioinformatics data pipelines for biotech using a highly effective and efficient method."
  },
  {
    "objectID": "commontext/outro.html",
    "href": "commontext/outro.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "We value candidates with the ‘nice to have’ skills and are committed to providing training and development opportunities to expand your skill set.\nIf you are interested in growing professionally and aspiring to become an expert, coach, or future leader within our expanding organisation, we encourage you to apply.\nTo apply for this opportunity, please either fill out the form here or alternatively email your application to andy@data-intuitive.com"
  },
  {
    "objectID": "services/course/introduction_running/index.html",
    "href": "services/course/introduction_running/index.html",
    "title": "Introduction and Running Viash Pipelines",
    "section": "",
    "text": "The course begins with an overview of Viash and its benefits for data science workflows and your organizationa. The second part involves practical exercises teaching you to operate existing Viash-built data pipelines with Nextflow."
  },
  {
    "objectID": "services/course/introduction_running/index.html#subject",
    "href": "services/course/introduction_running/index.html#subject",
    "title": "Introduction and Running Viash Pipelines",
    "section": "",
    "text": "The course begins with an overview of Viash and its benefits for data science workflows and your organizationa. The second part involves practical exercises teaching you to operate existing Viash-built data pipelines with Nextflow."
  },
  {
    "objectID": "services/course/introduction_running/index.html#target-audience",
    "href": "services/course/introduction_running/index.html#target-audience",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Target Audience",
    "text": "Target Audience\n\nManagers\nBiologists\nComputational Researchers\nBioinformaticians\nIT Developers"
  },
  {
    "objectID": "services/course/introduction_running/index.html#location",
    "href": "services/course/introduction_running/index.html#location",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Location",
    "text": "Location\nOnline"
  },
  {
    "objectID": "services/course/introduction_running/index.html#course-overview",
    "href": "services/course/introduction_running/index.html#course-overview",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Course Overview",
    "text": "Course Overview\n\nModule 1: Understanding the NoPipeline Concept\n\nIntroduction to Data Workflows in Biotechnology\nThe Emergence of NoPipeline\nBenefits of NoPipeline in Research\n\n\n\nModule 2: Introduction to Viash\n\nViash overview\nSetting up Viash\n\n\n\nModule 3: Setup for Executing Viash Pipelines\n\nAcquiring Pipelines for Local Use\nUnderstanding and Preparing Datasets\nConfiguring Pipeline Inputs and Outputs\n\n\n\nModule 4: Operating Viash Pipelines\n\nExecuting Pipelines Locally with Test Data\nDataset Integration in Pipeline Execution\nRunning Pipelines with Seqera Cloud (NF-Tower)\nRunning Pipelines from Viash Hub"
  },
  {
    "objectID": "services/course/introduction_running/index.html#objectives",
    "href": "services/course/introduction_running/index.html#objectives",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Objectives",
    "text": "Objectives\n\nProvide an in-depth understanding of the NoPipeline concept and its significance in modern biotechnological research.\nEquip participants with the skills to effectively utilize and managing existing data pipelines to process datasets.\nDemonstrate the practical application of Vias] and the NoPipeline concept through hands-on experience and case studies."
  },
  {
    "objectID": "services/course/introduction_running/index.html#required-skills",
    "href": "services/course/introduction_running/index.html#required-skills",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Required skills",
    "text": "Required skills\n\nUnderstanding of Bioinformatics Pipelines: Participants should have a foundational knowledge of the role and context of bioinformatics pipelines in research and development (R&D) environments. This includes an awareness of how these pipelines are integral to processing and analyzing biological data, and their significance in advancing scientific discoveries and innovations in biotechnology.\nBasic IT Proficiency: For the segment focusing on running pipelines, participants should possess basic information technology skills, particularly in using Command Line Interfaces (CLI). This skill is crucial for navigating and executing commands in environments where pipelines are developed and deployed, enabling participants to effectively interact with Viash and other pipeline tools."
  },
  {
    "objectID": "services/course/introduction_running/index.html#conclusion",
    "href": "services/course/introduction_running/index.html#conclusion",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Conclusion",
    "text": "Conclusion\nUpon completion, participants will have a comprehensive understanding of the NoPipeline concept, proficiency in running and managing existing Viash pipelines, and insights into leveraging the Viash Hub platform for collaborative and efficient research endeavors. This course aims to empower researchers with the tools and knowledge to navigate the evolving landscape of biotechnological data processing with confidence."
  },
  {
    "objectID": "services/course/introduction_running/index.html#trainers",
    "href": "services/course/introduction_running/index.html#trainers",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Trainers",
    "text": "Trainers\n\nRobrecht Cannoodt\nToni Verbeiren"
  },
  {
    "objectID": "services/course/introduction_running/index.html#program",
    "href": "services/course/introduction_running/index.html#program",
    "title": "Introduction and Running Viash Pipelines",
    "section": "Program",
    "text": "Program\n              \n              \n                \n                    - \n                \n                \n                    -  -  Online\n                \n                \n                   150,00 € taxes excluded\n                \n              \n              \n              Enroll This Course\n            \n            \n             \n              Enroll Multiple Courses\n            \n            \n            \n             Our  general terms and conditions apply to all contracts and courses.  \n            \n\n Back"
  },
  {
    "objectID": "services/subscribe.html",
    "href": "services/subscribe.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "I’m interested in the introduction course \n\n\n  I’m interested in learning how to run pipelines \n\n\n  I’m interested in the pipeline development fundamentals course \n\n\n  I’m interested in the advandced pipeline development course \n\n\n\n\n  I’m interested in a custom course for my company \n\n\n  I’d like to know more about Data Intuitive \n\n\n  I’d like to subscribe to the Data Intuitive e-mail list \n\n\n\n\n \n\n\n\n \n\n\nBy submitting this form, you agree to the collection and use of your personal data in accordance with our privacy policy. Your data will be used solely for the purpose of responding to your inquiry and will not be shared with third parties. You have the right to access, modify, or erase your personal data at any time."
  },
  {
    "objectID": "static/disclaimer.html",
    "href": "static/disclaimer.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Disclaimer\nThe information contained on this website is for general information purposes only. While we strive to keep the information up to date and correct, we make no representations or warranties of any kind, express or implied, about the completeness, accuracy, reliability, suitability or availability with respect to the website or the information, products, services, or related graphics contained on the website for any purpose. Any reliance you place on such information is therefore strictly at your own risk.\nIn no event will we be liable for any loss or damage including without limitation, indirect or consequential loss or damage, or any loss or damage whatsoever arising from loss of data or profits arising out of, or in connection with, the use of this website.\nThrough this website, you are able to link to other websites which are not under the control of Data Intuitive BV. We have no control over the nature, content and availability of those sites.\nAll information and services provided by Data Intuitive BV are subject to change without notice. This website, including its contents and information, is provided on an “as is” basis, without warranty of any kind, either expressed or implied, including, but not limited to, the implied warranties of merchantability, fitness for a particular purpose, or non-infringement.\nData Intuitive BV\nKorte Breestraat 4a\n9280 Lebbeke\nBelgium\nBE 0833.160.219"
  },
  {
    "objectID": "static/thank_subscribe.html",
    "href": "static/thank_subscribe.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Thank you for subscribing\nWe have received your message and will respond to your inquiry as soon as possible."
  },
  {
    "objectID": "static/sustainability_statement.html",
    "href": "static/sustainability_statement.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Last updated: Dec 17, 2023\nSustainability statement 2024\nVision on sustainabiltiy: At Data Intuitive, we harness the transformative power of open-source collaboration to drive sustainable practices and address global challenges while prioritizing well-being. We’ve built a business model that not only focuses on sustainability and supporting the open-source community, but also places a strong emphasis on the well-being of our employees. By fostering a work environment that promotes physical and mental well-being, we strive to shape a future where technology, sustainability, and individual well-being coexist to build a better, fairer, and more inclusive world..\nSustainability Practices: We recognize that our success depends on various stakeholders, including employees, customers, suppliers, partners, non-profit organizations, and the planet. Each year, Data Intuitive selects specific focus points and Sustainable Development Goals (SDGs) to work on throughout the year and during team meetings, ultimately contributing to building a sustainable future.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn 2024, we focus on three key Sustainable Development Goals (SDGs) in our operations.\nAt Data Intuitive, we prioritize employee well-being, aligning with SDG 3. Our actions include promoting open communication, supporting mental and physical well-being, offering growth opportunities and fostering teamwork.\nFor SDG 12, which encourages responsible consumption and production, we embed sustainability into our procurement policies and services, favoring durable materials and eco-friendly alternatives.\nLastly, we align with SDG 13’s call for climate action by managing our computational resources effectively, striving to achieve the perfect blend of technological innovation and environmental responsibility. We also use our influence to guide our clients toward energy-efficient coding practices, promoting sustainability beyond our immediate surroundings."
  },
  {
    "objectID": "company/careers/bioinformatics-workflow-engineer/index.html",
    "href": "company/careers/bioinformatics-workflow-engineer/index.html",
    "title": "Senior Bioinformatics Workflow Engineer",
    "section": "",
    "text": "Data Intuitive is a pioneering company specialising in state-of-the-art biotech data workflows. We excel in developing multi-omics bioinformatics data pipelines for biotech using a highly effective and efficient method.\nWe are currently seeking a scientifically-minded individual with a strong passion for IT to join our core team. This position offers a unique opportunity for professional development, allowing you to grow as an expert, coach, or future leader within our company.\nOur competitive compensation package includes an attractive salary, numerous benefits, a potential company car, and an equity stake.\nThis position primarily involves remote work, offering flexibility tailored to your personal needs. Our team gathers approximately eight times a year at an enjoyable working location for essential meetings and in-person collaborations.\nYou will embrace a modern approach to remote work, working within our virtual office and effortlessly connecting and communicating with your colleagues for discussing any challenges or issues you may encounter.\nWe take pride in our distinctive company culture that emphasises open source, well-being, sustainability, and technology-driven innovation. We value our collaboration with non-profit clients, as it allows us to contribute to their mission and create a positive impact.\nKey Responsibilities\n\nCollaborate with academics, pharma industry R&D, and research centres to develop and maintain (multi) omics data workflows for biotech R&D projects, focusing on fostering collaborative partnerships with clients rather than acting as temporary staff.\nDeliver high-quality IT coding and sustainable R&D solutions with a strong focus on problem-solving.\nParticipate in both internal R&D and client projects, utilising your unique talents, knowledge, skills, and preferences to contribute to the company’s success while maintaining a strong customer-centric focus.\nProvide support and mentorship to colleagues to contribute to everyone’s ongoing growth in performance and efficiency.\nEmbrace constant improvement by receiving continuous support and mentorship from your colleagues.\nBuild long-term relationships with our clients based on trust, mutual success, and pleasant collaboration.\n\nRequired Skills and Qualifications\n\nA master’s degree in Bioinformatics, Software Engineering, Computational Biology, Artificial Intelligence, or a related field\nA strong understanding of fundamental biological principles\nProficiency in scripting languages (e.g., Shell, Python, R)\nExperience using version control tools (e.g., Git/GitHub)\nGood knowledge of Linux-based operating systems, libraries, and tools\nUnderstanding of software build automation (e.g., Make)\nExcellent analytical and problem-solving skills\nAbility to work independently and as part of a team\nFluency in English, Dutch knowledge preferred\n\nNice-to-Have Skills\n\nRust Programming Language\nModern workflow management systems (e.g., Nextflow, Snakemake)\nContainerization (e.g., Docker, Singularity, Kubernetes)\nHigh-performance computing environments\nArtificial Intelligence (AI) and Machine Learning (ML)\n\nBenefits\n\nCompetitive compensation package including an attractive salary\nPotential company car and equity stake\nRemote work with flexible arrangements\nRegular in-person collaborations at enjoyable locations\nPart of a culture emphasizing open source, well-being, sustainability, and technological innovation\nCollaborate with non-profit clients to create a positive impact\n\nWe value candidates with the ‘nice to have’ skills and are committed to providing training and development opportunities to expand your skill set.\nIf you are interested in growing professionally and aspiring to become an expert, coach, or future leader within our expanding organisation, we encourage you to apply.\nTo apply for this opportunity, please either fill out the form here or alternatively email your application to andy@data-intuitive.com\n\n Back"
  },
  {
    "objectID": "insights/projects/virusbank/index.html",
    "href": "insights/projects/virusbank/index.html",
    "title": "Phylotrees for Virusbank",
    "section": "",
    "text": "The VirusBank Initiative, an ambitious project led by KU Leuven, aims to transform the global response to viral epidemics and pandemics. At the heart of this initiative is the VirusBank Platform, a strategically compiled collection of viruses. These viruses are meticulously chosen to accurately represent each virus family known for their high risk of causing epidemics or pandemics, making it an invaluable asset for worldwide health research."
  },
  {
    "objectID": "insights/projects/virusbank/index.html#introduction",
    "href": "insights/projects/virusbank/index.html#introduction",
    "title": "Phylotrees for Virusbank",
    "section": "",
    "text": "The VirusBank Initiative, an ambitious project led by KU Leuven, aims to transform the global response to viral epidemics and pandemics. At the heart of this initiative is the VirusBank Platform, a strategically compiled collection of viruses. These viruses are meticulously chosen to accurately represent each virus family known for their high risk of causing epidemics or pandemics, making it an invaluable asset for worldwide health research."
  },
  {
    "objectID": "insights/projects/virusbank/index.html#data-intuitives-contribution-developing-the-toolbox-for-virusbank",
    "href": "insights/projects/virusbank/index.html#data-intuitives-contribution-developing-the-toolbox-for-virusbank",
    "title": "Phylotrees for Virusbank",
    "section": "Data Intuitive’s Contribution: Developing the Toolbox for VirusBank",
    "text": "Data Intuitive’s Contribution: Developing the Toolbox for VirusBank\nData Intuitive has been instrumental in developing the Toolbox for the VirusBank. This toolbox is a critical component for delving into the intricate details of viral replication cycles. It provides a comprehensive framework for assessing new antiviral strategies, which is crucial for enhancing our understanding of viral behavior and mechanisms. The toolbox’s capabilities are key to driving forward the discovery and development of novel therapeutic approaches, thereby bolstering the initiative’s role in advancing our preparedness and response to viral threats.\n\n            \n              \n                Back\n               Website\n               GitHub"
  },
  {
    "objectID": "insights/projects/openpipelines/index.html",
    "href": "insights/projects/openpipelines/index.html",
    "title": "OpenPipelines",
    "section": "",
    "text": "In collaboration with a leading pharmaceutical company’s R&D department, we have embarked on an ambitious journey to revolutionize the field of single-cell genomics. This partnership combines our cutting-edge computational expertise with their extensive knowledge in pharmaceutical research, enabling us to push the boundaries of data pipelines."
  },
  {
    "objectID": "insights/projects/openpipelines/index.html#introduction",
    "href": "insights/projects/openpipelines/index.html#introduction",
    "title": "OpenPipelines",
    "section": "",
    "text": "In collaboration with a leading pharmaceutical company’s R&D department, we have embarked on an ambitious journey to revolutionize the field of single-cell genomics. This partnership combines our cutting-edge computational expertise with their extensive knowledge in pharmaceutical research, enabling us to push the boundaries of data pipelines."
  },
  {
    "objectID": "insights/projects/openpipelines/index.html#advancements-and-challenges-in-single-cell-genomics-analysis",
    "href": "insights/projects/openpipelines/index.html#advancements-and-challenges-in-single-cell-genomics-analysis",
    "title": "OpenPipelines",
    "section": "Advancements and Challenges in Single-Cell Genomics Analysis",
    "text": "Advancements and Challenges in Single-Cell Genomics Analysis\nBreakthroughs in single-cell genomics have been made possible by the simultaneous advancement of experimental and computational technologies. While experimental techniques standardize quickly, computational analysis pipelines are more challenging to compare and thus vary. Analysis platforms such as Seurat and Scanpy have facilitated pipeline building and introduced basic quality standards. Furthermore, we have made first attempts at standardizing workflows for scRNA-seq to bridge the language-gap between these platforms and their users.\nBeing able to easily adopt new and updated methods in a systematic and predictable way is what Viash is made for: every computational step is defined as a Viash component that is built into a Nextflow module by Viash. The Nextflow module is generated such that it can easily plugged into an existing pipeline flow."
  },
  {
    "objectID": "insights/projects/openpipelines/index.html#the-need-for-new-analysis-standards-in-evolving-single-cell-omics",
    "href": "insights/projects/openpipelines/index.html#the-need-for-new-analysis-standards-in-evolving-single-cell-omics",
    "title": "OpenPipelines",
    "section": "The Need for New Analysis Standards in Evolving Single-Cell Omics",
    "text": "The Need for New Analysis Standards in Evolving Single-Cell Omics\nAs new analysis methods are being proposed at an increasing rate and novel multi-omic sequencing technologies are becoming commonplace, there is an urgent need for new analysis standards for single-cell (multi-) omics data.\nOpenPipelines is an active project and collaboration aimed precisely at delivering best-practice workflows for single-cell single- and multi-omics data.\n            \n              \n                Back\n               Website\n               GitHub"
  },
  {
    "objectID": "insights/projects/anndata-for-r/index.html",
    "href": "insights/projects/anndata-for-r/index.html",
    "title": "anndata: annotated data in R",
    "section": "",
    "text": "anndata is a commonly used Python package for keeping track of data and learned annotations, and can be used to read from and write to the h5ad file format. It is also the main data format used in the scanpy python package (Wolf, Angerer, and Theis 2018).\nHowever, using scanpy/anndata in R can be a major hassle. When trying to read an h5ad file, R users could approach this problem in one of two ways. A) You could read in the file manually (since it’s an H5 file), but this involves a lot of manual work and a lot of understanding on how the h5ad and H5 file formats work (also, expect major headaches from cryptic hdf5r bugs). Or B) interact with scanpy and anndata through reticulate, but run into issues converting some of the python objects into R.\nWe recently published anndata on CRAN, which is an R wrapper for the Python package – with some syntax sprinkled on top to make R users feel more at home.\nThe R anndata is still under active development at github.com/rcannood/anndata. If you encounter any issues, feel free to post an issue on GitHub!"
  },
  {
    "objectID": "insights/projects/anndata-for-r/index.html#installation",
    "href": "insights/projects/anndata-for-r/index.html#installation",
    "title": "anndata: annotated data in R",
    "section": "Installation",
    "text": "Installation\nInstalling anndata is not particularly hard. You do need R and Python to be installed. If you don’t have a working installation of Python, you can let reticulate install Miniconda.\n# install the R anndata package\ninstall.packages(\"anndata\")\n\n# skip this if you wish to use the local python installation\nreticulate::install_miniconda()\n\n# skip this if anndata is already installed\nanndata::install_anndata()"
  },
  {
    "objectID": "insights/projects/anndata-for-r/index.html#getting-started",
    "href": "insights/projects/anndata-for-r/index.html#getting-started",
    "title": "anndata: annotated data in R",
    "section": "Getting started",
    "text": "Getting started\nThe API of anndata is very similar to its Python counterpart. Check out ?anndata for a full list of the functions provided by this package.\nAnnData stores a data matrix X together with annotations of observations obs (obsm, obsp), variables var (varm, varp), and unstructured annotations uns.\nHere is an example of how to create an AnnData object with 2 observations and 3 variables.\n\nlibrary(anndata)\n\nad &lt;- AnnData(\n  X = matrix(1:6, nrow = 2),\n  obs = data.frame(group = c(\"a\", \"b\"), row.names = c(\"s1\", \"s2\")),\n  var = data.frame(type = c(1L, 2L, 3L), row.names = c(\"var1\", \"var2\", \"var3\")),\n  layers = list(\n    spliced = matrix(4:9, nrow = 2),\n    unspliced = matrix(8:13, nrow = 2)\n  ),\n  obsm = list(\n    ones = matrix(rep(1L, 10), nrow = 2),\n    rand = matrix(rnorm(6), nrow = 2),\n    zeros = matrix(rep(0L, 10), nrow = 2)\n  ),\n  varm = list(\n    ones = matrix(rep(1L, 12), nrow = 3),\n    rand = matrix(rnorm(6), nrow = 3),\n    zeros = matrix(rep(0L, 12), nrow = 3)\n  ),\n  uns = list(\n    a = 1, \n    b = data.frame(i = 1:3, j = 4:6, value = runif(3)),\n    c = list(c.a = 3, c.b = 4)\n  )\n)\n\nad\n\nAnnData object with n_obs × n_vars = 2 × 3\n    obs: 'group'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\nYou can read the information back out using the $ notation.\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nad$obs\n\n   group\ns1     a\ns2     b\n\nad$obsm[[\"ones\"]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    1    1    1\n[2,]    1    1    1    1    1\n\nad$layers[[\"spliced\"]]\n\n   var1 var2 var3\ns1    4    6    8\ns2    5    7    9\n\nad$uns[[\"b\"]]\n\n  i j      value\n1 1 4 0.03063135\n2 2 5 0.97626754\n3 3 6 0.70391182\n\n\n\nUsing scanpy\nA nice side-effect of loading this package is that it is now super easy to interact with scanpy through these AnnData objects.\n\nlibrary(reticulate)\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nsc &lt;- import(\"scanpy\")\nsc$pp$normalize_per_cell(ad)\n\nad$X\n\n       var1 var2     var3\ns1 1.166667  3.5 5.833333\ns2 1.750000  3.5 5.250000\n\n\n\n\nReading / writing AnnData objects\nRead from h5ad format:\nread_h5ad(\"pbmc_1k_protein_v3_processed.h5ad\")\n## AnnData object with n_obs × n_vars = 713 × 33538\n##     var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n##     uns: 'hvgParameters', 'normalizationParameters', 'pca', 'pcaParameters'\n##     obsm: 'X_pca'\n##     varm: 'PCs'\n\n\nCreating a view\nYou can use any of the regular R indexing methods to subset the AnnData object. This will result in a ‘View’ of the underlying data without needing to store the same data twice.\n\nview &lt;- ad[, 2]\nview\n\nView of AnnData object with n_obs × n_vars = 2 × 1\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nview$is_view\n\n[1] TRUE\n\nad[,c(\"var1\", \"var2\")]\n\nView of AnnData object with n_obs × n_vars = 2 × 2\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nad[-1, ]\n\nView of AnnData object with n_obs × n_vars = 1 × 3\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\n\n\nAnnData as a matrix\nThe X attribute can be used as an R matrix:\n\nad$X[,c(\"var1\", \"var2\")]\n\n       var1 var2\ns1 1.166667  3.5\ns2 1.750000  3.5\n\nad$X[-1, , drop = FALSE]\n\n   var1 var2 var3\ns2 1.75  3.5 5.25\n\nad$X[, 2] &lt;- 10\n\nYou can access a different layer matrix as follows:\n\nad$layers[\"unspliced\"]\n\n   var1 var2 var3\ns1    8   10   12\ns2    9   11   13\n\nad$layers[\"unspliced\"][,c(\"var2\", \"var3\")]\n\n   var2 var3\ns1   10   12\ns2   11   13\n\n\n\n\nNote on state\nIf you assign an AnnData object to another variable and modify either, both will be modified:\n\nad2 &lt;- ad\n\nad$X[,2] &lt;- 10\n\nlist(ad = ad$X, ad2 = ad2$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n$ad2\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n\nThis is standard Python behaviour but not R. In order to have two separate copies of an AnnData object, use the $copy() function:\n\nad3 &lt;- ad$copy()\n\nad$X[,2] &lt;- c(3, 4)\n\nlist(ad = ad$X, ad3 = ad3$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667    3 5.833333\ns2 1.750000    4 5.250000\n$ad3\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000"
  },
  {
    "objectID": "insights/projects/anndata-for-r/index.html#future-work",
    "href": "insights/projects/anndata-for-r/index.html#future-work",
    "title": "anndata: annotated data in R",
    "section": "Future work",
    "text": "Future work\nIn some cases, this package may still act more like a Python package rather than an R package. Some more helper functions and helper classes need to be defined in order to fully encapsulate AnnData() objects. Examples are:\n\nad$chunked_X(1)\n\n&lt;generator object AnnData.chunked_X at 0x7f8d3424eac0&gt;\n\n\nFollowing functionality has not been tested:\nad$rename_categories(...)\nad$strings_to_categoricals(...)\nCurrently not implemented are the read_zarr() and ad$write_zarr() functions. I need some example data to test this functionality."
  },
  {
    "objectID": "insights/projects/anndata-for-r/index.html#references",
    "href": "insights/projects/anndata-for-r/index.html#references",
    "title": "anndata: annotated data in R",
    "section": "References",
    "text": "References\nWolf, F Alexander, Philipp Angerer, and Fabian J Theis. 2018. “SCANPY: Large-Scale Single-Cell Gene Expression Data Analysis.” Genome Biology 19 (February): 15. https://doi.org/10.1186/s13059-017-1382-0.\n            \n              \n                Back\n              Website\n               GitHub"
  },
  {
    "objectID": "insights/showcase/viashhub_cli/index.html",
    "href": "insights/showcase/viashhub_cli/index.html",
    "title": "Viash Hub CLI demo",
    "section": "",
    "text": "Introduction\nIn what follows, we will demonstrate how Viash and later Viash Hub allow anyone with a minimal set of technical skills to develop and perform a simple task: Run QC on a (potentially large) set of fastq files and combine all those QC reports into one (multiqc) report.\nA bioinformatician could use fastqc in combination with a (bash) shell for loop. This, however, would not be run in parallel. Command-line tools exists to parallelize these tasks, but the ones we know of can hardly be called easy to use.\nWhat if one could just reuse existing functionality (aka Viash components or Nextflow modules) and combine those in a simple Nextflow pipeline in order to achieve the mentioned goal. That’s where this demo project comes in: https://viash-hub.com/data-intuitive/viash_hub_demo/-/tree/v0.1?ref_type=heads.\nBelow, we run this pipeline on a test dataset in two ways. Screencasts are provided to demonstrate the use.\n\n\nTest data\nWe will fetch test data from this repository: https://github.com/hartwigmedical/testdata:\ngit clone https://github.com/hartwigmedical/testdata testData \n\n\nRun directly from ViashHub\nIn order to fetch the workflow from Viash Hub, the following should be added to ~/.nextflow/scm:\nproviders {\n  vsh {\n    platform = 'gitlab'\n    server = \"viash-hub.com\"\n  }\n}\nThen, with the data fetched above present under testData, we can run fastqc in parallel on all 32 fastq files:\nnextflow run data-intuitive/viash_hub_demo \\\n    -hub vsh \\\n    -main-script target/nextflow/workflows/parallel_qc/main.nf \\\n    -r main \\\n    --input \"testData/**/*.fastq.gz\" \\\n    --publish_dir output \\\n    -with-docker\nThe output will be stored under output as indicated by the --publish_dir argument.\nScreencast of fetching test data and running the pipeline from Viash Hub directly:\n\n  \n  \n\n\nRun from a local copy\nFirst of all, build the workflow component and fetch the dependencies:\n❯ viash ns build\ntemporaryFolder: /tmp/viash_hub_repo5484030342718552259 uri: https://github.com/openpipelines-bio/openpipeline.git\nCloning into '.'...\ncheckout out: List(git, checkout, tags/0.12.1, --, .) 0\nCreating temporary 'target/.build.yaml' file for op as this file seems to be missing.\nExporting parallel_qc (workflows) =nextflow=&gt; &lt;...&gt;/demo/target/nextflow/workflows/parallel_qc\nExporting transpose (utils) =nextflow=&gt; &lt;...&gt;/demo/target/nextflow/utils/transpose\nAll 2 configs built successfully\nNow, run fastqc on all fastq files that can be found under in the testData directory:\n❯ nextflow run target/nextflow/workflows/parallel_qc/main.nf \\\n    --input \"testData/**/*.fastq.gz\"  \\\n    --publish_dir output \\\n    -with-docker\nScreencast of fetching test data and running the pipeline from a local copy:\n\n  \n  \nDone!\n            \n              \n                Back"
  },
  {
    "objectID": "insights/showcase/viash_basic/index.html",
    "href": "insights/showcase/viash_basic/index.html",
    "title": "Viash Basics Demo",
    "section": "",
    "text": "Watch our screencast demonstration to see the getting started script of Viash.io in action."
  },
  {
    "objectID": "insights/showcase/viash_basic/index.html#quickstart-example-project",
    "href": "insights/showcase/viash_basic/index.html#quickstart-example-project",
    "title": "Viash Basics Demo",
    "section": "Quickstart example project",
    "text": "Quickstart example project\nTo get up and running fast, we provide a template project for you to use. It contains three components, which are combined into a Nextflow pipeline as follows:\n\n\n\n\n\ngraph TD\n   START(input: file?.tsv) --&gt; X[vsh_flatten] \n   X --file1.tsv--&gt; B1[/remove_comments/] --&gt; C1[/take_column/] --&gt; Y\n   X --file2.tsv--&gt; B2[/remove_comments/] --&gt; C2[/take_column/] --&gt; Y\n   Y[vsh_toList] --&gt; D[/combine_columns/]\n   D --&gt; STOP(output)\n\n\n\n\n\n\nThis pipeline takes one or more TSV files as input and stores its output in an output folder.\nHere is a CLI screencast demo:\n\n  \n  \n  \nMore details on Quickstart on viash.io documentation site\n            \n              \n                Back\n               Website\n               GitHub\n                Preprint"
  },
  {
    "objectID": "insights/news/2023-12-15-fit_to_go_global/index.html",
    "href": "insights/news/2023-12-15-fit_to_go_global/index.html",
    "title": "Fit to Go Global",
    "section": "",
    "text": "We are thrilled to announce that this website is being launched with the funding of Flanders Investment and Trade (FIT). This funding will not only help us to create a professional and user-friendly website, but it will also support the international expansion of our company. We are grateful for the support and belief in our mission, and we look forward to working together as we continue to grow and develop on a global scale.\n\n Back"
  },
  {
    "objectID": "insights/blog/2024-03-19-framework-laptops/index.html",
    "href": "insights/blog/2024-03-19-framework-laptops/index.html",
    "title": "Data Intuitive Embraces Sustainable Tech with Framework Laptops",
    "section": "",
    "text": "Data Intuitive is thrilled to announce our move towards sustainability by adopting Framework. Framework laptops, a decision that mirrors our commitment to innovative and environmentally responsible technology. Framework’s mission aligns with ours: to challenge the norm of planned obsolescence with easily repairable and upgradable laptops.\n\nWhy Framework?\nChoosing Framework laptops is a statement of our dedication to a sustainable future. These laptops are designed with longevity in mind, allowing for easy repairs and upgrades. This aligns with our goal to minimize electronic waste and champion sustainable tech use. \n\n\nCustomization and Sustainability\nFramework stands out for its customization capabilities, offering users the choice of essential ports like USB-A, USB-C, HDMI, or Ethernet. This not only tailors the laptop to individual needs but also prolongs its lifespan by focusing on upgrades only where necessary.\n\n\n\nThe Path Ahead\nOur choice to work with Framework underscores our broader commitment to blending technological advancement with environmental stewardship. It’s a step towards a future where technology and sustainability go hand in hand, exemplifying our role in shaping a greener, tech-enabled world.\n\nJoin Us as we lead the charge towards a sustainable tech future, proving that environmental responsibility and cutting-edge technology can coexist.\n\n Back"
  },
  {
    "objectID": "insights/blog/2023-05-22-scverse-hackathon/index.html",
    "href": "insights/blog/2023-05-22-scverse-hackathon/index.html",
    "title": "anndataR at the scverse 2023-04 hackathon",
    "section": "",
    "text": "Last month I had the privilege of participating in the scverse Hackathon in Heidelberg hosted by the Stegle and Saez labs. It was an exhilarating event packed with exciting developments, intense coding, inspiring collaborations, and of course, pizza. The hackathon’s central themes were enhancing AnnData operability across programming languages, and improving the integration of biological information into the AnnData file format.\nOur team, which consisted of Martin Morgan (@mtmorgan), Luke Zappia (@lazappi), Louise Deconinck (@LouiseDck), Danila Bredikhin (@gtca), and myself (@rcannood), dedicated our efforts towards bridging the gap between .h5ad files and the R / Bioconductor ecosystem."
  },
  {
    "objectID": "insights/blog/2023-05-22-scverse-hackathon/index.html#anndata-in-r",
    "href": "insights/blog/2023-05-22-scverse-hackathon/index.html#anndata-in-r",
    "title": "anndataR at the scverse 2023-04 hackathon",
    "section": "AnnData in R",
    "text": "AnnData in R\nOur ultimate goal was an implementation of AnnData in R that would allow us to read, edit, and write .h5ad files, and convert them to SingleCellExperiment or SeuratObject objects. To achieve this, we outlined a set of minimal requirements based on our combined use cases:\nRequirement 1: Use a native HDF5 library. We aimed for a solution that utilized an HDF5 library like hdf5r or rhdf5 to read or write data from .h5ad files. This native approach would prevent challenges related to installing Python dependencies from R and converting Python objects to R, which could arise when using an inter-language interface such as reticulate. Packages with native h5ad readers: h5ad, MuData, SeuratDisk, MuDataSeurat.\nRequirement 2: Mimic existingAnnData interface. Despite several R packages that directly convert an .h5ad file into a SingleCellExperiment or SeuratObject, the data structures of AnnData objects do not map perfectly to those of SingleCellExperiment and SeuratObject. Consequently, a direct conversion inadvertently results in data loss. As such, we prefer data structures and interfaces that emulate a Python AnnData object as closely as possible. Packages with AnnData-like interfaces: h5ad, anndata for R.\nRequirement 3: Allow converting to common R data structures. Both SingleCellExperiment and SeuratObject are associated with a wide range of existing tools and packages for data analysis and visualization. By offering converters to these structures, users can easily leverage these tools for their .h5ad data."
  },
  {
    "objectID": "insights/blog/2023-05-22-scverse-hackathon/index.html#existing-approaches",
    "href": "insights/blog/2023-05-22-scverse-hackathon/index.html#existing-approaches",
    "title": "anndataR at the scverse 2023-04 hackathon",
    "section": "Existing approaches",
    "text": "Existing approaches\nWe evaluated the strengths and limitations of existing solutions for handling .h5ad files in R. Unfortunately, none of them fully satisfied our minimal requirements. Moreover, each package had a significant list of known issues or was too limited in scope.\n\n\n\nName\nBackend\nData structures\nConverters\n\n\n\n\nzellkonverter\nreticulate\nSingleCellExperiment\nImplicit\n\n\nh5ad\nrhdf5\nAnnData\nNone\n\n\nanndata for R\nreticulate\nAnnData\nNone\n\n\nMuData\nrhdf5\nSingleCellExperiment\nImplicit\n\n\nsceasy\nreticulate\nNone\nSeurat & SingleCellExperiment\n\n\nSeuratDisk\nhdf5r\nSeurat\nImplicit\n\n\nMuDataSeurat\nhdf5r\nSeurat\nImplicit\n\n\n\nTo better address our needs, we set about developing yet another AnnData for R package that fulfills each of our requirements. We identified the following strategies to avoid common pitfalls:\n\nUnderstand the AnnData HDF5 on-disk format: Start by gaining an in-depth understanding of the AnnData HDF5 on-disk format. Knowing the specifications and how the data is organized in .h5ad files will help in designing effective internal data structures and interfaces.\nImplement rigorous testing: Implement a comprehensive suite of unit tests for individual data structures and functions. This can help catch and fix bugs early in the development process, and ensures that the package continues to work correctly as changes and updates are made.\nLearn from existing implementations: Use the combined experience gained from developing and using zellkonverter, h5ad, anndata for R and MuData.\nPlan for flexibility: Consider how the package might be used in various workflows and aim for flexibility."
  },
  {
    "objectID": "insights/blog/2023-05-22-scverse-hackathon/index.html#introducing-anndatar",
    "href": "insights/blog/2023-05-22-scverse-hackathon/index.html#introducing-anndatar",
    "title": "anndataR at the scverse 2023-04 hackathon",
    "section": "Introducing anndataR",
    "text": "Introducing anndataR\nanndataR is an R package engineered to bridge the gap between the Python-based AnnData and the R ecosystem. It provides an interface for handling backed h5ad files, directly accessing various slots, and converting the data into SingleCellExperiment and Seurat objects. As part of the hackathon, we had set ourselves the goal to only include the X, obs, var and layers slots for now.\n\n\n\n\n\nclassDiagram\n  class AbstractAnnData {\n    abstract Matrix X\n    abstract List~Matrix~ layers\n    abstract DataFrame obs\n    abstract DataFrame var\n    abstract Array~String~ obs_names\n    abstract Array~String~ var_names\n    int n_obs()\n    int n_vars()\n    to_SingleCellExperiment() SingleCellExperiment\n    to_Seurat() SeuratObject\n    to_HDF5AnnData() HDF5AnnData\n    to_InMemoryAnnData() InMemoryAnnData\n  }\n  AbstractAnnData &lt;|-- HDF5AnnData\n  class HDF5AnnData {\n    -H5File .h5file\n    init(path, obs_names, var_names, X, layers, obs, var) HDF5AnnData\n  }\n  AbstractAnnData &lt;|-- InMemoryAnnData\n  class InMemoryAnnData {\n    -Matrix .X\n    -List~Matrix~ .layers\n    -DataFrame .obs\n    -DataFrame .var\n    -Array~String~ .obs_names\n    -Array~String~ .var_names\n    init(X, obs_names, var_names, X, layers, obs, var) InMemoryAnnData\n  }\n  class anndataR {\n    from_SingleCellExperiment(sce, backend) AbstractAnnData\n    from_Seurat(obj, backend) AbstractAnnData\n  }\n  anndataR --&gt; AbstractAnnData\n\n\n\n\n\n\nSince the functions to_SingleCellExperiment() and to_Seurat() are defined in the AbstractAnnData class, we can seamlessly implement other backends such as a ZarrAnnData and ReticulateAnnData and conveniently convert them into different file formats. Note that the X, layers, obs, var, obs_names and var_names are in reality active bindings; while they appear as variables to the end-user, the HDF5AnnData and InMemoryAnnData should have their own implementation for reading data from / to these slots."
  },
  {
    "objectID": "insights/blog/2023-05-22-scverse-hackathon/index.html#demo",
    "href": "insights/blog/2023-05-22-scverse-hackathon/index.html#demo",
    "title": "anndataR at the scverse 2023-04 hackathon",
    "section": "Demo",
    "text": "Demo\nBelow is a simple demonstration of how to use anndataR:\nlibrary(anndataR)\n\n# Download a sample h5ad file\ndownload.file(\"https://github.com/chanzuckerberg/cellxgene/raw/main/example-dataset/pbmc3k.h5ad\", \"pbmc3k.h5ad\")\n\n# Read an h5ad file\nadata &lt;- HDF5AnnData$new(\"pbmc3k.h5ad\")\n\n# Access AnnData slots\nadata$X\nadata$obs\nadata$var\n\n# Convert the AnnData object to a SingleCellExperiment object\nsce &lt;- adata$to_SingleCellExperiment()\n\n# Convert the AnnData object to a Seurat object\nobj &lt;- adata$to_Seurat()"
  },
  {
    "objectID": "insights/blog/2023-05-22-scverse-hackathon/index.html#looking-ahead",
    "href": "insights/blog/2023-05-22-scverse-hackathon/index.html#looking-ahead",
    "title": "anndataR at the scverse 2023-04 hackathon",
    "section": "Looking ahead",
    "text": "Looking ahead\nAt the scverse hackathon, we developed the initial version of anndataR, and we’re still actively enhancing it. Our aim is for it to fully replace zellkonverter, h5ad and anndata for R, and serve as a basis for interacting with SpatialData and MuData objects in R.\nThe scverse Hackathon 2023 was a remarkable event, and I’m grateful for the opportunity to have been a part of it. The development of anndataR is an ongoing process, and we’re excited about the potential this package has to offer. If you’re interested in contributing, feel free to check out the project on GitHub.\nStay tuned for more updates on our ongoing projects and initiatives, and happy coding!\n\n Back"
  },
  {
    "objectID": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html",
    "href": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html",
    "title": "A Practical Approach to Model Error",
    "section": "",
    "text": "In this post, I want to get a better sense of the effects of model error by simulating a very simple model for the spreading of a virus. It’s based on an analysis performed by Nassim Taleb. I used the simulation below in the scope of a workshop paper on the effect of cognitive biases. The published version of the paper can be found here."
  },
  {
    "objectID": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html#footnotes",
    "href": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html#footnotes",
    "title": "A Practical Approach to Model Error",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEven variations that are normally distributed, i.e., thin tailed.↩︎"
  },
  {
    "objectID": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html",
    "href": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html",
    "title": "GitHub Action: Reclaim The Bytes",
    "section": "",
    "text": "In the dynamic world of data science and software development, making efficient use of available resources is not just an option but an absolute necessity. In a lot of our projects (e.g. OpenPipelines, viash.io, OpenProblems), we need to build or use so many Docker image as part of our CI workflow, that we run out of disk space quite quickly.\nAs part of our commitment to empowering developers, we at Data Intuitive are excited to announce the Version 1 release of our innovative GitHub Action, “Reclaim The Bytes”. This GitHub action is designed to streamline your workflow by freeing up disk space on GitHub runners during the build process, helping you make the most of the available space."
  },
  {
    "objectID": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html#background-and-inspiration",
    "href": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html#background-and-inspiration",
    "title": "GitHub Action: Reclaim The Bytes",
    "section": "Background and Inspiration",
    "text": "Background and Inspiration\nThe development of “Reclaim The Bytes” was motivated by a desire to enhance the efficiency of workspace environments in GitHub. It was inspired by similar initiatives such as easimon’s maximize-build-space, ThewApp’s free-actions. The discussion thread here provided additional insights that shaped the development of this action.\nThe action works by executing rm -rf on specific folders, thereby removing unnecessary software and liberating valuable disk space. While the process is simple and quick, it is critical to understand that it might inadvertently delete dependencies required by your job, potentially disrupting your build. For instance, if your build job uses a .NET based tool and the required runtime is deleted, it may affect the smooth execution of your task. Therefore, it is crucial to ascertain which software may or may not be removed for your specific use case."
  },
  {
    "objectID": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html#usage",
    "href": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html#usage",
    "title": "GitHub Action: Reclaim The Bytes",
    "section": "Usage",
    "text": "Usage\nBelow is an example usage in a GitHub workflow:\nname: My build action requiring more space\non: push\n\njobs:\n  build:\n    name: Build my artifact\n    runs-on: ubuntu-latest\n    steps:\n      - name: Reclaim the bytes\n        uses: data-intuitive/reclaim-the-bytes@v1\n        with:\n          remove-hosted-tool-cache: true\n          remove-go: false\n          remove-codeql: false\n          remove-powershell: false\n          remove-android-sdk: true\n          remove-haskell-ghc: true\n          remove-swift: true\n          remove-dotnet: true\n          remove-docker-images: true\n          remove-swap: true\n\n      - name: Checkout\n        uses: actions/checkout@v3\n\n      - name: Report free space\n        run: |\n          echo \"Free space:\"\n          df -h"
  },
  {
    "objectID": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html#measurements",
    "href": "insights/blog/2023-06-16-github-actions-reclaim-the-bytes/index.html#measurements",
    "title": "GitHub Action: Reclaim The Bytes",
    "section": "Measurements",
    "text": "Measurements\nUnderstanding the balance between the time taken to remove a piece of software and the amount of space freed as a result is essential for optimizing your usage of “Reclaim The Bytes”. We have provided a visualization to help you comprehend this balance better. Below is a summary table that provides data on the software removed, the operating system, the duration in seconds for the removal process, and the amount of space freed in GB.\n\n\n\nSoftware\nOS\nDuration (s)\nSpace freed (GB)\n\n\n\n\nandroid-sdk\nubuntu-20.04\n26.8\n12\n\n\nandroid-sdk\nubuntu-22.04\n37.0\n13\n\n\ncodeql\nubuntu-20.04\n1.0\n6\n\n\ncodeql\nubuntu-22.04\n1.0\n6\n\n\ndocker-images\nubuntu-20.04\n18.4\n5\n\n\ndocker-images\nubuntu-22.04\n17.0\n4\n\n\ndotnet\nubuntu-20.04\n5.6\n3\n\n\ndotnet\nubuntu-22.04\n2.2\n2\n\n\ngo\nubuntu-20.04\n2.2\n1\n\n\ngo\nubuntu-22.04\n1.2\n2\n\n\nhaskell-ghc\nubuntu-20.04\n2.8\n5\n\n\nhaskell-ghc\nubuntu-22.04\n3.4\n5\n\n\nhosted-tool-cache\nubuntu-20.04\n5.0\n10\n\n\nhosted-tool-cache\nubuntu-22.04\n3.4\n9\n\n\npowershell\nubuntu-20.04\n0.8\n1\n\n\npowershell\nubuntu-22.04\n0.6\n2\n\n\npython\nubuntu-20.04\n0.0\n0\n\n\npython\nubuntu-22.04\n0.4\n0\n\n\nswap\nubuntu-20.04\n0.0\n0\n\n\nswap\nubuntu-22.04\n0.0\n0\n\n\nswift\nubuntu-20.04\n0.6\n2\n\n\nswift\nubuntu-22.04\n0.4\n2\n\n\n\nAdditionally, a scatterplot visualizes the same information, providing an intuitive understanding of the trade-off between removal duration and space freed. This way, you can make informed decisions about which software to keep or remove based on your unique requirements and constraints.\n\nIn summary, “Reclaim The Bytes” provides an efficient method for software developers to optimize their GitHub actions and maximize disk space usage. However, it is essential to understand its operation to ensure the successful execution of your build jobs. With this understanding, we hope you will leverage “Reclaim The Bytes” to improve your workflow and make your software development journey smoother and more efficient. Enjoy coding!\n\n Back"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "",
    "text": "It’s been a while since I first wrote about tackling model error using a simple model. It’s about time to come back to it.\nWhat triggered the current post is the opportunity I was given to give a masterclass in the Evidence and policy summer school. Since my masterclass is about uncertainty in decision making, it seemed like a nice opportunity to look back at the simple model."
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-1-no-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-1-no-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 1: No Randomness",
    "text": "Step 1: No Randomness\nBy clicking on the parameters of the simulation, or below in the footer on the words can be changed, the simulation settings can be adjusted.\nSet the variables such that there is no randomness applied to the scenario. Only one scenario is sufficient in this case:\n\nAnd see what the result looks like:"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-2-a-bit-of-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-2-a-bit-of-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 2: A bit of Randomness",
    "text": "Step 2: A bit of Randomness\nIn step 2, we add a bit of randomness. For this we use the default settings, although you could increase the number of scenarios if you wanted to:\n\nThe result should look similar to the following. Please note that randomness has been added, so your result should not look exactly like the one presented here!"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-3-a-bit-more-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-3-a-bit-more-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 3: A bit more Randomness",
    "text": "Step 3: A bit more Randomness\nIn step 3, we increase the amount of randomness added to the scenarios. For instance:\n\nNow, given more randomness in the scenarios, yours might be completely different from the one below:"
  },
  {
    "objectID": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "href": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "title": "Fueled by VLAIO and Driven by innovation",
    "section": "",
    "text": "Data Intuitive is pleased to announce that we have received funding from VLAIO, which will bolster our research and development efforts as we continue to push the boundaries of data science and bioinformatics data pipelines. We are grateful for the support of VLAIO and are excited to see what the future holds as we continue to innovate and progress in our field.\n\n Back"
  },
  {
    "objectID": "insights/news/2023-02-05-sustain/index.html",
    "href": "insights/news/2023-02-05-sustain/index.html",
    "title": "DI’s Sustainability and Wellness Initiative",
    "section": "",
    "text": "As a telework-based company, Data Intuitive is committed to promoting both sustainability and the well-being of its employees. With the support of Ready2improve and Axudo, the company is developing a comprehensive personal health and sustainability plan in collaboration with its employees. Through organized group sessions during team-building days, the company and its partner aim to foster discussions and generate ideas for a better telework future.\nOne of the difficulties faced by a fully remote company is fostering a sense of unity and collaboration among employees. To address this, the company explores different work methods and leverages technology to create a virtual environment that mimics the experience of working together in one large shared space. Teleworking also helps reduce commute emissions, promote sustainability, and support personal well-being through flexible work arrangements. The plan places a strong emphasis on open-source technology and support for non-profit scientific organizations, further reinforcing Data Intuitive’s commitment to sustainability.\nThis effort showcases Data Intuitive’s dedication to sustainability and well-being, and its aim to create a better future for both its employees and the planet.\n\n\n Back"
  },
  {
    "objectID": "insights/news/2023-02-05-sustain/index.html#sustainable-telework-a-plan-for-a-better-future",
    "href": "insights/news/2023-02-05-sustain/index.html#sustainable-telework-a-plan-for-a-better-future",
    "title": "DI’s Sustainability and Wellness Initiative",
    "section": "",
    "text": "As a telework-based company, Data Intuitive is committed to promoting both sustainability and the well-being of its employees. With the support of Ready2improve and Axudo, the company is developing a comprehensive personal health and sustainability plan in collaboration with its employees. Through organized group sessions during team-building days, the company and its partner aim to foster discussions and generate ideas for a better telework future.\nOne of the difficulties faced by a fully remote company is fostering a sense of unity and collaboration among employees. To address this, the company explores different work methods and leverages technology to create a virtual environment that mimics the experience of working together in one large shared space. Teleworking also helps reduce commute emissions, promote sustainability, and support personal well-being through flexible work arrangements. The plan places a strong emphasis on open-source technology and support for non-profit scientific organizations, further reinforcing Data Intuitive’s commitment to sustainability.\nThis effort showcases Data Intuitive’s dedication to sustainability and well-being, and its aim to create a better future for both its employees and the planet.\n\n\n Back"
  },
  {
    "objectID": "insights/showcase/openpipeline/index.html",
    "href": "insights/showcase/openpipeline/index.html",
    "title": "Running OpenPipeline on public data",
    "section": "",
    "text": "You can easily run the pipelines/workflows that are present in the opensource repo of Openpipeline. This can be done by using the Seqera Cloud (formerly known as nextflow tower or nf-tower) instance of Seqera Labs. This is a cloud based service that allows you to run nextflow pipelines in the cloud.\nBelow a picture of a succesfull run on the DI Seqera Cloud workspace:"
  },
  {
    "objectID": "insights/showcase/openpipeline/index.html#getting-started",
    "href": "insights/showcase/openpipeline/index.html#getting-started",
    "title": "Running OpenPipeline on public data",
    "section": "Getting started",
    "text": "Getting started\nThere are several ways to get started with Openpipeline and nf-tower:\n\nYou can either start a pipeline through the CLI wether it is the nextflow CLI or the nf-tower CLI.\nYou can also start a pipeline through the Seqera Cloud interface.\n\nFor this demo we will be using the nf-tower CLI. This is a command line interface that allows you to start a pipeline from the command line. This is a very powerful tool as it allows you to automate the running of pipelines. For example you can run a pipeline on a daily basis to analyse new data."
  },
  {
    "objectID": "insights/showcase/openpipeline/index.html#requirements",
    "href": "insights/showcase/openpipeline/index.html#requirements",
    "title": "Running OpenPipeline on public data",
    "section": "Requirements",
    "text": "Requirements\n\nSeqera Cloud\nYou need access to Seqera Cloud with a workspace and compute environment configured. For this demo we will be using the Data Intuitive workspace “Demo” and Google Batch as the compute environment.\n\n\nTower CLI\nIf this is the first installation, the following steps are required:\n\nInstall the CLI Tool according to nextflow Tower CLI\nAfter Installing the cli tool, you will need to provide your nf-tower token.\n\nClick on the user icon on the top right corner of the page and select the option Your tokens:\n\n\n\nClick on the “Add Token” button and create a new Personal Access Token. Copy the token and save it in a safe place.\nAdd the token as a shell variable: directly to your terminal:\nexport TOWER_ACCESS_TOKEN=&lt;your access token&gt;\n\nTo run the analysis through seqera cloud you will also need the workspace and compute environment ID.\n\nworkspace ID:\n\nClick on the dropdown menu on the top right and select the organisation: \nIn the workspace tab you can find the ID below the workspace name in the list: \n\nCompute ID:\n\nIn the selected wokspace (same as above) go to the “Compute Environments” tab.\nIn the provided list you can find the ID below the status of the required environment: \n\n\nStore the public data in a bucket for example on google cloud. For this demo we will be using the dataset 5k Human PBMCs, 3’ v3.1, Chromium Controller. The fastQ files are stored at gs://&lt;...&gt;/openpipeline/resource_public/PBMC."
  },
  {
    "objectID": "insights/showcase/openpipeline/index.html#run-cellranger",
    "href": "insights/showcase/openpipeline/index.html#run-cellranger",
    "title": "Running OpenPipeline on public data",
    "section": "Run cellranger",
    "text": "Run cellranger\nThe fastQ files are mapped using the cellranger pipeline from Openpipeline. To run the cellranger pipeline a reference is needed. The reference can be downloaded from the cellranger download page. For this demo we will be using the de reference stored at gs://&lt;...&gt;/openpipeline/reference/human/refdata-gex-GRCh38-2020-A.tar.gz. This is the same reference used by the 10x genomics analysis https://cf.10xgenomics.com/samples/cell-exp/7.0.1/SC3pv3_GEX_Human_PBMC/SC3pv3_GEX_Human_PBMC_web_summary.html.\nFollowing code should be copied to a bash script:\n#!/bin/bash\n\nset -eo pipefail\n\ncat &gt; /tmp/params.yaml &lt;&lt; HERE\nparam_list:\n- id: \"Chromium_3p_GEX_Human_PBMC\"\n  input: \"gs://&lt;...&gt;/openpipeline/resource_public/PBMC\"\n  library_id:\n    - \"Chromium_3p_GEX_Human_PBMC\"\n  library_type:\n    - \"Gene Expression\"\n\ngex_reference: \"gs://&lt;...&gt;/openpipeline/reference/human/refdata-gex-GRCh38-2020-A.tar.gz\"\npublish_dir: \"gs://&lt;...&gt;/openpipeline/resource_public/PBMC/processed\"\nHERE\n\ntw \\\n  launch https://viash-hub.com/openpipelines-bio/openpipeline \\\n  --revision 0.12.1 --pull-latest \\\n  --main-script target/nextflow/mapping/cellranger_multi/main.nf \\\n  --workspace ... \\\n  --compute-env ... \\\n  --profile docker,mount_temp \\\n  --params-file /tmp/params.yaml \\\n  --config src/workflows/utils/labels.config"
  },
  {
    "objectID": "insights/showcase/openpipeline/index.html#convert-cellranger-output",
    "href": "insights/showcase/openpipeline/index.html#convert-cellranger-output",
    "title": "Running OpenPipeline on public data",
    "section": "Convert cellranger output",
    "text": "Convert cellranger output\nFor the next step of the analysis we will need a “h5mu” data file. As this is not available from the cellranger output we will need to convert it to the correct format. We will be using the convert/from_cellranger_multi_to_h5mu pipeline.\nIn a bash script you will need to add the follow code:\n#!/bin/bash\n\nset -eo pipefail\n\nOUT=gs://&lt;...&gt;/openpipeline/resource_public/PBMC\n\ncat &gt; /tmp/params.yaml &lt;&lt; HERE\nid: \"Chromium_3p_GEX_Human_PBMC\"\ninput: \"$OUT/processed/Chromium_3p_GEX_Human_PBMC.cellranger_multi.output.output\"\npublish_dir: \"$OUT/\"\noutput: \"Chromium_3p_GEX_Human_PBMC.h5mu\"\nHERE\n\n\ntw launch https://viash-hub.com/openpipelines-bio/openpipeline \\\n  --revision 0.12.1 --pull-latest \\\n  --main-script target/nextflow/convert/from_cellranger_multi_to_h5mu/main.nf \\\n  --workspace ...  \\\n  --compute-env ... \\\n  --profile docker,mount_temp \\\n  --params-file /tmp/params.yaml \\\n  --config src/workflows/utils/labels.config"
  },
  {
    "objectID": "insights/showcase/openpipeline/index.html#run-the-analysis-pipeline",
    "href": "insights/showcase/openpipeline/index.html#run-the-analysis-pipeline",
    "title": "Running OpenPipeline on public data",
    "section": "Run the analysis pipeline",
    "text": "Run the analysis pipeline\nNow that we have the “h5mu” data file we can analyse the pipeline with the multiomics/full_pipeline.\nAdd the following code to a bash file:\n#!/bin/bash\n\nset -eo pipefail\n\nOUT=gs://&lt;...&gt;/openpipeline/resource_public/PBMC\n\ncat &gt; /tmp/params.yaml &lt;&lt; HERE\nid: \"Chromium_3p_GEX_Human_PBMC\"\ninput: \"$OUT/Chromium_3p_GEX_Human_PBMC.h5mu\"\npublish_dir: \"$OUT/\"\noutput: \"Chromium_3p_GEX_Human_PBMC_mms.h5mu\"\nHERE\n\ntw launch https://viash-hub.com/openpipelines-bio/openpipeline \\\n  --revision 0.12.1 --pull-latest \\\n  --main-script workflows/multiomics/full_pipeline/main.nf \\\n  --workspace ... \\\n  --compute-env ... \\\n  --profile docker,mount_temp \\\n  --params-file /tmp/params.yaml \\\n  --config src/workflows/utils/labels.config"
  },
  {
    "objectID": "insights/showcase/openpipeline/index.html#caveats",
    "href": "insights/showcase/openpipeline/index.html#caveats",
    "title": "Running OpenPipeline on public data",
    "section": "Caveats",
    "text": "Caveats\nIt turns out that not all Google datacenter regions have all possible instance types. As a consequence, we ran into quota limits which could not be resolved.\nIn order to workaround this issue, we configured Nextflow to disable the use of those instance types. The following lines need to be adjusted in the config file:\n\n\nsrc/workflows/utils/labels.config\n\n  // Resource labels\n  withLabel: singlecpu { cpus = 1 }\n  withLabel: lowcpu { cpus = 4 }\n  withLabel: midcpu { cpus = 10 }\n  withLabel: highcpu { cpus = 20 }\n\n\n\nsrc/workflows/utils/labels.config\n\n  // Resource labels\n  withLabel: singlecpu { cpus = 1 }\n  withLabel: lowcpu { cpus = 4 }\n  withLabel: midcpu { \n    cpus = 10\n    machineType = 'c2-*' \n  }\n  withLabel: highcpu {\n    cpus = 20\n    machineType = 'c2-*'\n  }\n\n            \n              \n                Back\n               Website\n               GitHub"
  },
  {
    "objectID": "insights/projects/benchmarking_openproblems/index.html",
    "href": "insights/projects/benchmarking_openproblems/index.html",
    "title": "Open Problems for Single-Cell Analysis",
    "section": "",
    "text": "OpenProblems is an open-source platform for benchmarking and formalizing computational tasks in single-cell analysis. The platform aims to facilitate the development of new methods for single-cell omics by defining mathematical interpretations of tasks, providing publicly available gold-standard datasets in standardized formats, defining quantitative metrics to evaluate success, and ranking state-of-the-art methods in continuously updated leaderboards. OpenProblems is hosted on GitHub and evaluated using AWS with support from the Chan Zuckerberg Initiative."
  },
  {
    "objectID": "insights/projects/benchmarking_openproblems/index.html#introduction",
    "href": "insights/projects/benchmarking_openproblems/index.html#introduction",
    "title": "Open Problems for Single-Cell Analysis",
    "section": "",
    "text": "OpenProblems is an open-source platform for benchmarking and formalizing computational tasks in single-cell analysis. The platform aims to facilitate the development of new methods for single-cell omics by defining mathematical interpretations of tasks, providing publicly available gold-standard datasets in standardized formats, defining quantitative metrics to evaluate success, and ranking state-of-the-art methods in continuously updated leaderboards. OpenProblems is hosted on GitHub and evaluated using AWS with support from the Chan Zuckerberg Initiative."
  },
  {
    "objectID": "insights/projects/benchmarking_openproblems/index.html#viash-a-catalyst-for-methodological-advancement",
    "href": "insights/projects/benchmarking_openproblems/index.html#viash-a-catalyst-for-methodological-advancement",
    "title": "Open Problems for Single-Cell Analysis",
    "section": "Viash: A Catalyst for Methodological Advancement",
    "text": "Viash: A Catalyst for Methodological Advancement\nViash is being used in OpenProblems to streamline the development, execution, and sharing of methods, metrics and dataset loaders. It allows users to define their components as reusable modules that can be combined to form pipelines, and it manages dependencies, versioning, and execution on different computing environments. Viash also provides a user-friendly interface for configuring pipelines and visualizing results. By using Viash, OpenProblems can provide a flexible and scalable platform for benchmarking single-cell analysis methods.\n            \n              \n                Back\n              Website\n               GitHub"
  },
  {
    "objectID": "insights/projects/dynverse/index.html",
    "href": "insights/projects/dynverse/index.html",
    "title": "Dynverse: Inferring trajectories using dyno",
    "section": "",
    "text": "Dynverse, utilizing the ‘dyno’ package, is a state-of-the-art framework designed for inferring cellular trajectories from single-cell RNA sequencing data. It offers a comprehensive suite of tools for robust and efficient trajectory analysis, ensuring high reproducibility and comparability across methods. Dynverse facilitates the exploration of cellular dynamics, aiding in the understanding of developmental processes and disease progression. The platform’s user-friendly interface and integration with other bioinformatics tools make it accessible to both biologists and data scientists. Its modular design allows for the seamless incorporation of new trajectory inference methods, fostering continuous innovation in the field.\nThe dyno package offers end-users a complete TI pipeline. It features: - a uniform interface to 60 TI methods, - an interactive guideline tool to help the user select the most appropriate method, - streamlined interpretation and visualisation of trajectories, including colouring by gene expression or clusters, and - downstream analyses such as the identification of potential marker genes.\nFor information on how to use dyno, check out the installation instructions, tutorials and documentation at dynverse.org"
  },
  {
    "objectID": "insights/projects/dynverse/index.html#dynverse-dependencies",
    "href": "insights/projects/dynverse/index.html#dynverse-dependencies",
    "title": "Dynverse: Inferring trajectories using dyno",
    "section": "Dynverse dependencies",
    "text": "Dynverse dependencies\n\n\n            \n              \n                Back\n               Website\n               GitHub"
  },
  {
    "objectID": "insights/projects/ComPass/index.html",
    "href": "insights/projects/ComPass/index.html",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "",
    "text": "ComPass is an application for querying and analyzing Connectivity Map data1, also known as L1000 data but could be extended for other types of data as well. It captures the essential tasks that researchers ordinarily execute as batch processes in a realtime web interface. ComPass provides four interactive workflows and offers a variety of technical features, including compound and target annotations, the ability to export data to CSV and JSON formats, and more.\nComPass is developed by Data Intuitive, is Open Source and freely available."
  },
  {
    "objectID": "insights/projects/ComPass/index.html#introduction",
    "href": "insights/projects/ComPass/index.html#introduction",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "",
    "text": "ComPass is an application for querying and analyzing Connectivity Map data1, also known as L1000 data but could be extended for other types of data as well. It captures the essential tasks that researchers ordinarily execute as batch processes in a realtime web interface. ComPass provides four interactive workflows and offers a variety of technical features, including compound and target annotations, the ability to export data to CSV and JSON formats, and more.\nComPass is developed by Data Intuitive, is Open Source and freely available."
  },
  {
    "objectID": "insights/projects/ComPass/index.html#customization-for-pharma-client-enhancing-data-integration-and-analysis",
    "href": "insights/projects/ComPass/index.html#customization-for-pharma-client-enhancing-data-integration-and-analysis",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "Customization for Pharma Client: Enhancing Data Integration and Analysis",
    "text": "Customization for Pharma Client: Enhancing Data Integration and Analysis\nData Intuitive recently engaged in a project with a second undisclosed top 10 pharma client. As part of the project, we meticulously prepared and optimized the client’s datasets for seamless integration with ComPass. Additionally, we tailored the ComPass to meet the unique requirements of the client, ensuring that it perfectly aligned with their specific research needs.\nWe take immense pride in our commitment to open-source development, and any customization performed during the project is thoughtfully integrated into the latest open source version of the ComPass Tool. However, it’s essential to note that personalization of ComPass in industry remains exclusively accessible to the client for whom these were developed.\nBy providing our client with this tailor-made solution, we have significantly enhanced their data analysis capabilities, allowing them to derive valuable insights and drive meaningful advancements in their research. As a trusted partner in biotechnology, we uphold the highest standards of confidentiality, and while we cannot disclose the details of this specific project, we are delighted to have contributed to the success of our client’s scientific endeavors.\nThe Compass tool consists 4 of workflows:"
  },
  {
    "objectID": "insights/projects/ComPass/index.html#the-disease-workflow",
    "href": "insights/projects/ComPass/index.html#the-disease-workflow",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "The Disease Workflow",
    "text": "The Disease Workflow\nThe disease workflow allows users to search for samples or compounds that are highly correlated or anti-correlated to a gene signature related to a specific disease or other biological effect."
  },
  {
    "objectID": "insights/projects/ComPass/index.html#the-correlation-workflow",
    "href": "insights/projects/ComPass/index.html#the-correlation-workflow",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "The Correlation Workflow",
    "text": "The Correlation Workflow\nThe Correlation workflow enables users to compare two gene signatures (or two compounds) by calculating and plotting Zhang scores. The data points in this plot are binned to ensure scalability."
  },
  {
    "objectID": "insights/projects/ComPass/index.html#the-compound-workflow",
    "href": "insights/projects/ComPass/index.html#the-compound-workflow",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "The Compound Workflow",
    "text": "The Compound Workflow\nThe Compound workflow allows users to generate a gene signature of genes that are highly correlated with samples available for a specific compound. It also performs a similarity analysis on the generated signature, similar to the Disease workflow."
  },
  {
    "objectID": "insights/projects/ComPass/index.html#the-target-workflow",
    "href": "insights/projects/ComPass/index.html#the-target-workflow",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "The Target Workflow",
    "text": "The Target Workflow\nThe Target workflow enables users to search for compounds that are known to affect well-known and experimentally validated target genes in the L1000 subset of genes. Users can view the correlation of these targets with all available samples.\n            \n              \n                Back\n              \n               GitHub\n                Paper"
  },
  {
    "objectID": "insights/projects/ComPass/index.html#footnotes",
    "href": "insights/projects/ComPass/index.html#footnotes",
    "title": "ComPass: Connectivity Map Data Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Connectivity Map: Using Gene-Expression Signatures to Connect Small Molecules, Genes, and Disease (https://www.science.org/doi/10.1126/science.1132939)↩︎"
  },
  {
    "objectID": "insights/projects/HT-RNA-SEQ/index.html",
    "href": "insights/projects/HT-RNA-SEQ/index.html",
    "title": "High-throughput RNA sequencing",
    "section": "",
    "text": "Data Intuitive recently undertook a successful project for an undisclosed client in the top 10 pharma R&D. In this project, we co-developed a high-throughput RNA sequencing pipeline using our powerful and versatile tool, Viash. The goal was to streamline the processing and analysis of large-scale RNA sequencing data, allowing the client to extract valuable insights from their experiments efficiently and accurately.\nAn early implementation was developed using an approach that was hard to maintain and did not scale well."
  },
  {
    "objectID": "insights/projects/HT-RNA-SEQ/index.html#introduction",
    "href": "insights/projects/HT-RNA-SEQ/index.html#introduction",
    "title": "High-throughput RNA sequencing",
    "section": "",
    "text": "Data Intuitive recently undertook a successful project for an undisclosed client in the top 10 pharma R&D. In this project, we co-developed a high-throughput RNA sequencing pipeline using our powerful and versatile tool, Viash. The goal was to streamline the processing and analysis of large-scale RNA sequencing data, allowing the client to extract valuable insights from their experiments efficiently and accurately.\nAn early implementation was developed using an approach that was hard to maintain and did not scale well."
  },
  {
    "objectID": "insights/projects/HT-RNA-SEQ/index.html#implementation-and-customization-with-viash",
    "href": "insights/projects/HT-RNA-SEQ/index.html#implementation-and-customization-with-viash",
    "title": "High-throughput RNA sequencing",
    "section": "Implementation and Customization with Viash",
    "text": "Implementation and Customization with Viash\nOur team of experts leveraged Viash’s modular approach to create a customized and optimized pipeline tailored to the client’s specific requirements. Due to the fact that we create Nextflow-compatible modules and pipelines using Viash, we could leverage Nextflow’s ability to run on a cluster and thus scale the processing.\nThroughout the development process, we prioritized reproducibility and scalability, ensuring that the pipeline could handle increasing volumes of data without compromising data quality or performance. As a result, the client now benefits from an advanced pipeline, empowering them to conduct cutting-edge research and accelerate their discoveries in a number of applications while still actively extending it to new application domains."
  },
  {
    "objectID": "insights/projects/HT-RNA-SEQ/index.html#confidentiality-and-impact",
    "href": "insights/projects/HT-RNA-SEQ/index.html#confidentiality-and-impact",
    "title": "High-throughput RNA sequencing",
    "section": "Confidentiality and Impact",
    "text": "Confidentiality and Impact\nDue to confidentiality agreements, we are unable to disclose the specific details of the client or the project, but we are proud to have played a crucial role in advancing their scientific endeavors.\n\n Back"
  },
  {
    "objectID": "static/privacy_policy.html",
    "href": "static/privacy_policy.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Last updated: March 2, 2023\nPRIVACY POLICY\nIntroduction\nThis Privacy Policy explains how your personal data is collected, used, and protected when you use our website. This policy is intended to help you understand what information we collect, why we collect it, and what we do with it.\nData Controller\nThe data controller responsible for the collection and processing of your personal data is Data Intuitive BV, with a registered office at Korte Breestraat 4a, 9280 Lebbeke, Belgium, registered BE 0833.160.219.\nPersonal Data We Collect\nThe personal data we collect may include:\n\nContact information, such as name, e-mail address, and telephone number\nDemographic information, such as age, gender, and location\nInformation about your interests and preferences\nInformation you provide through our website’s contact forms, surveys, or other features\nInformation about your use of our website, such as the pages you visit and the links you click on\nIP address, browser type and version, time zone setting, and operating system\nInformation collected through cookies and other tracking technologies\n\nHow We Use Your Personal Data\nWe use your personal data to:\n\nProvide and improve our website and services\nRespond to your inquiries and support requests\nSend you promotional materials or other communications about our products and services\nMonitor the usage of our website and improve its performance\nPrevent, detect, and investigate fraudulent or illegal activities\n\nData Retention\nWe will retain your personal data for as long as it is necessary to fulfill the purposes for which it was collected. This may include retaining your data for a period of time after you stop using our website or services to comply with legal requirements and to resolve any disputes.\nDisclosure of Your Personal Data\nWe may disclose your personal data to third-party service providers who assist us in providing our website and services. These third parties are subject to confidentiality agreements and are only permitted to use your personal data for the purpose of providing their services to us.\nWe may also disclose your personal data if required to do so by law or in response to a valid request from a law enforcement or governmental authority.\nYour Rights\nUnder the GDPR, you have the following rights:\n\nThe right to request access to your personal data\nThe right to request correction or deletion of your personal data\nThe right to request that we limit the processing of your personal data\nThe right to object to the processing of your personal data\nThe right to request the transfer of your personal data\nThe right to withdraw your consent to the processing of your personal data at any time.\n\nTo exercise these rights, please contact us at DPO at Data-intuitive.com.\nData Security\nWe take reasonable and appropriate measures to protect your personal data from unauthorized access, alteration, disclosure, or destruction.\nChanges to This Privacy Policy\nWe may update this Privacy Policy from time to time to reflect changes to our data collection and processing practices. When we update this Privacy Policy, we will revise the “Last Updated” date at the top of this page.\nContact Us\nIf you have any questions or concerns about this Privacy Policy or our data collection and processing practices, please contact us at DPO at Data-intuitive.com."
  },
  {
    "objectID": "static/thank_apply.html",
    "href": "static/thank_apply.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Thank you for your interest in joining our team!\nWe have received your message and will respond to your inquiry as soon as possible."
  },
  {
    "objectID": "static/thank_contact.html",
    "href": "static/thank_contact.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Thank you for your interest in our company.\nWe have received your message and will respond to your inquiry as soon as possible."
  },
  {
    "objectID": "static/generalconditions.html",
    "href": "static/generalconditions.html",
    "title": "General Conditions Data Intuitive BV",
    "section": "",
    "text": "General Conditions Data Intuitive BV\nArticle 1. General Information\nData Intuitive BV (hereinafter briefly referred to as Data Intuitive) is a limited liability company located at 9280 Lebbeke, Korte Breestraat 4a, and is registered in Dendermonde at the Crossroads Bank for Enterprises under number BE 0833.160.219.\nArticle 2. Applicability\nThese general conditions apply to all services, agreements, deliveries, and sales of goods by Data Intuitive to its customers. The customer is deemed to be aware of these general conditions and to accept them without reservation. In case of contradiction, these general conditions will prevail over any differing general conditions of the customer, unless otherwise agreed in writing by one of the directors of Data Intuitive. Agreements that deviate from one or more provisions of these general conditions will only replace the provision or provisions from which they deviate. The other provisions remain fully applicable.\nArticle 3. Object of the Service\nThe parties will agree on the precise object of Data Intuitive’s service provision at the start of the activities and, if necessary, adjust and/or expand it during further execution.\nArticle 4. Price Quotes\n§1. The specifications and price offers provided to the customer are valid for fourteen calendar days, unless otherwise stated on the specifications. §2. In addition to the current general conditions, the rights and obligations of the parties are determined by the agreement that follows from the specifications. (determination of the price)\nArticle 5. Obligations of Data Intuitive BV\n§1. An assignment is only binding if it is expressly and in writing accepted by a director. In the event of cancellation of the assignment, the total amount remains due, unless after prior written agreement. §2. The commitments of Data Intuitive are not obligations of result but obligations of effort unless agreed otherwise. All assignments are executed to the best of Data Intuitive’s ability. Data Intuitive has the option of outsourcing the assignments to employees within and outside the company, whereby the present general conditions remain fully applicable. Data Intuitive commits to delivering its services within a reasonable period. Data Intuitive is in no way liable for exceeding deadlines due to the customer, third parties, or force majeure. §3. Data Intuitive takes appropriate organizational and administrative measures to prevent conflicts of interest in advisory services between its customers from harming the interests of the latter. The customer must inform Data Intuitive of any information in its possession that would indicate that Data Intuitive may be in a conflict situation. Consequently, Data Intuitive reserves the right, for ethical reasons, to refuse an assignment if conflicts of interest may arise that could harm the interests of its customers.\nArticle 6. Cooperation of the Customer\nData Intuitive does everything to provide optimal service to its customers. Smooth interaction and collaboration between Data Intuitive and the customer are very important in this regard. The service of Data Intuitive is tailor-made. The customer provides Data Intuitive, both at the start of the agreement and during its duration, if requested by Data Intuitive, promptly all the information required to make the optimal execution of its service possible. The customer is responsible for the accuracy, completeness, and reliability of the data, information, and documents provided by him. If the customer does not, does not timely, or does not in accordance with the agreements provide the necessary cooperation, Data Intuitive is free to no longer perform work for the customer and to completely withdraw from the case. Data Intuitive is not liable for any damage that may result from this withdrawal.\nArticle 7. Remuneration – Complaints – Payment\n§1. Data Intuitive charges for its services according to hourly rates or other methods, which have been determined in agreement with the customer. The rate is also determined based on the nature, complexity, commitment, and urgent character of the case. §2. Data Intuitive may ask the customer for one or more advances at the beginning and during its activities. After completing an assignment, the customer will receive a final invoice where the paid advances will be deducted from the total amount. §3. If the customer does not agree with an invoice, he must protest in writing and motivated within fourteen calendar days from the date of the invoice, under penalty of forfeiting the right. §4. Unless otherwise agreed, all invoices are payable in cash at the seat of Data Intuitive, without discount. If an invoice is not paid by the due date of the invoice, Data Intuitive - without having to give prior notice of default to the customer – has the right by law (A) to charge late payment interests at an interest rate of 1.5% per month from the due date of the invoice until the date of full payment as well as 10% of the late paid amount with a minimum of 200 EUR, notwithstanding her right to the court costs (including the applicable legal fees), should a judicial collection follow. Also, in this case, Data Intuitive has the right either to suspend the execution of its activities in all files with the concerned customer until all invoices are fully paid, or to terminate the overall cooperation with the customer with immediate effect. Data Intuitive is not liable for any damage resulting from the suspension of its activities or the termination of its agreement with the customer.\nArticle 8. Delivery\n§1. It is the customer’s responsibility to verify the accuracy of the results from services/products delivered by Data Intuitive. The customer must inspect the delivered goods as soon as possible after delivery. Any defects must be reported to Data Intuitive by registered letter within fourteen calendar days after delivery. After this period, Data Intuitive is only responsible for hidden defects that make the item unsuitable for the use for which it is intended. The customer must notify Data Intuitive of the existence of the hidden defect by registered letter with a detailed description of the defect within fourteen calendar days after discovering the hidden defect. Complaints about hidden defects do not suspend the customer’s payment obligation. §2. Products and services are delivered within the period specified in the agreement. Goods remain the property of Data Intuitive until full payment of the principal amount, costs, and interests. The goods are shipped at the risk of the customer. The transportation costs, unless otherwise determined, are at the expense of the customer. §3. If the customer does not pick up the goods on the communicated date, Data Intuitive reserves the right to consider the agreement as dissolved after a period of fourteen days, without prior notice of default. §4. The storage of the goods pending delivery or collection is at the risk of the customer. §5. As long as the delivered goods have not been paid for, they remain the property of Data Intuitive. However, the risk is transferred to the customer at the moment of contract conclusion.\nArticle 8b. Training Courses\n§1 In cases where a training course organized by Data Intuitive has fewer than three registrants, the following conditions will apply:\n\nRescheduling Right: Data Intuitive reserves the right to reschedule the training course to a later date due to low enrollment. This rescheduling is limited to one occurrence.\n\nNotification of Rescheduling: Data Intuitive will inform all registered participants about the rescheduled date in a timely manner.\n\nNo Refund Policy: In the event of rescheduling due to low enrollment, participants are not entitled to a refund of any fees paid for the training course.\n\nParticipant Commitment: Participants who are registered for a course that has been rescheduled due to low enrollment are expected to attend the course on the new date.\n\nFinal Rescheduling: If, after being rescheduled once, the training course still does not achieve the minimum required number of three participants, Data Intuitive will proceed with the course on the rescheduled date with the registered participants. No further rescheduling or cancellation will be possible.\n\nArticle 9. Liability\n§1. Data Intuitive is not liable for damage resulting from incorrect or incomplete information provided by the customer. §2. Data Intuitive is not liable for any compensation for damage, directly or indirectly resulting from services provided by us, except in case of gross negligence or intent. Any liability of Data Intuitive and/or its subcontractors and appointees, whether contractual or non-contractual, is always limited in principal, costs, and interests to the amount excl. VAT charged in the file in which the liability is retained, and in the absence of such a file to a maximum of € 7,500 per incident. §3. Data Intuitive cannot be held liable for any indirect damage, consequential damage, loss of use, or loss of profit suffered by the customer or by third parties. §4. Although Data Intuitive makes reasonable efforts to protect its digital assets from viruses or other defects that could affect computers or an IT system, it is the responsibility of the customer to ensure appropriate measures exist to protect the customer’s computers and IT system against such viruses or defects. Data Intuitive accepts no liability for any loss or damage resulting from receiving or using electronic communications from Data Intuitive. §5. Data Intuitive is not liable for any damage caused by the installation of software. §6. Any unauthorized modification of the software or goods by the customer voids the right to warranty.\nArticle 10. Intellectual Property Rights\nThe customer is not allowed to reproduce, make public, or use in any way the advices, notes, opinions, contracts, documents, slides, and all other intellectual works created by Data Intuitive, regardless of their form, without its prior written consent, other than in the context of the assignment awarded to Data Intuitive.\nArticle 11. Modification\nData Intuitive reserves the right to modify these general conditions at any time.\nArticle 12. Applicable Law and Competent Court\n§1. All agreements between Data Intuitive and the customer are exclusively subject to Belgian law. §2. Parties prefer to settle their disputes amicably. §3. Only the courts of the judicial district of Dendermonde are competent to take cognizance of any dispute between Data Intuitive and the customer."
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html",
    "href": "services/course/pipeline_fundamentals/index.html",
    "title": "Fundamentals of Pipeline Development",
    "section": "",
    "text": "This course covers the basics of Viash & Nextflow (VDSL3), teaching you to streamline data science workflows, create, run, and share workflows, and integrate Viash with various tools and platforms."
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#subject",
    "href": "services/course/pipeline_fundamentals/index.html#subject",
    "title": "Fundamentals of Pipeline Development",
    "section": "",
    "text": "This course covers the basics of Viash & Nextflow (VDSL3), teaching you to streamline data science workflows, create, run, and share workflows, and integrate Viash with various tools and platforms."
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#target-audience",
    "href": "services/course/pipeline_fundamentals/index.html#target-audience",
    "title": "Fundamentals of Pipeline Development",
    "section": "Target Audience:",
    "text": "Target Audience:\n\nComputational Researchers\nBioinformaticians\nIT Developers"
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#location",
    "href": "services/course/pipeline_fundamentals/index.html#location",
    "title": "Fundamentals of Pipeline Development",
    "section": "Location:",
    "text": "Location:\nOnline"
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#course-overview",
    "href": "services/course/pipeline_fundamentals/index.html#course-overview",
    "title": "Fundamentals of Pipeline Development",
    "section": "Course Overview",
    "text": "Course Overview\n\nModule 1: Introduction to Data Pipelines in Biotechnology\n\nBasics of Data Pipelines\nChallenges in Pipeline Development\n\n\n\nModule 2: Getting Started with Viash\n\nOverview of Viash\nInstalling and Configuring Viash\n\n\n\nModule 3: Building Pipelines with Viash\n\nCreating Basic Pipelines\nAdvanced Pipeline Features\nDebugging and Optimization\n\n\n\nModule 4: Integration and Deployment\n\nIntegrating with Workflow Managers\nPipeline Version Control and Management\nDeployment Strategies\n\n\n\nModule 5: Advanced Topics and Case Studies\n\nAutomating and Scaling Pipelines\nReal-world Applications"
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#objectives",
    "href": "services/course/pipeline_fundamentals/index.html#objectives",
    "title": "Fundamentals of Pipeline Development",
    "section": "Objectives",
    "text": "Objectives\n\nImpart comprehensive knowledge about data pipeline development in the context of bioinformatics.\nProvide hands-on experience in creating, managing, and optimizing pipelines using Viash.\nShowcase the integration of Viash pipelines with other tools and platforms, enhancing functionality and usability."
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#required-skills",
    "href": "services/course/pipeline_fundamentals/index.html#required-skills",
    "title": "Fundamentals of Pipeline Development",
    "section": "Required skills",
    "text": "Required skills\n\nParticipants should have a foundational understanding of bioinformatics and data analysis, along with basic knowledge of scripting languages. Familiarity with the concepts of data pipelines is advantageous but not mandatory."
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#conclusion",
    "href": "services/course/pipeline_fundamentals/index.html#conclusion",
    "title": "Fundamentals of Pipeline Development",
    "section": "Conclusion",
    "text": "Conclusion\nUpon completing this course, participants will be adept at developing and managing efficient and scalable data pipelines using Viash. They will gain practical skills and insights into advanced pipeline development strategies, preparing them to tackle complex data processing challenges in their respective fields of research. This course is a step towards empowering researchers and professionals with the tools and knowledge needed to innovate and excel in the dynamic landscape of bioinformatic data processing."
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#trainers",
    "href": "services/course/pipeline_fundamentals/index.html#trainers",
    "title": "Fundamentals of Pipeline Development",
    "section": "Trainers",
    "text": "Trainers\n\nRobrecht Cannoodt\nToni Verbeiren"
  },
  {
    "objectID": "services/course/pipeline_fundamentals/index.html#program",
    "href": "services/course/pipeline_fundamentals/index.html#program",
    "title": "Fundamentals of Pipeline Development",
    "section": "Program",
    "text": "Program\n              \n                \n                \n                    - \n                \n                \n                    -  -  Online\n                \n                \n                   300,00 € taxes excluded\n                \n              \n              \n              Enroll This Course\n            \n            \n             \n              Enroll Multiple Courses\n            \n            \n            \n             Our  general terms and conditions apply to all contracts and courses.  \n            \n\n Back"
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html",
    "href": "services/course/pipeline_advanced/index.html",
    "title": "Advanced Pipeline Development",
    "section": "",
    "text": "This advanced Viash course enhances your skills in parameterization, templating, and workflow composition, and teaches integration with the Nextflow scientific computing workflow management system."
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#subject",
    "href": "services/course/pipeline_advanced/index.html#subject",
    "title": "Advanced Pipeline Development",
    "section": "",
    "text": "This advanced Viash course enhances your skills in parameterization, templating, and workflow composition, and teaches integration with the Nextflow scientific computing workflow management system."
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#target-audience",
    "href": "services/course/pipeline_advanced/index.html#target-audience",
    "title": "Advanced Pipeline Development",
    "section": "Target Audience",
    "text": "Target Audience\n\nComputational Researchers\nBioinformaticians\nIT Developers"
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#location",
    "href": "services/course/pipeline_advanced/index.html#location",
    "title": "Advanced Pipeline Development",
    "section": "Location",
    "text": "Location\nOnline"
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#course-overview",
    "href": "services/course/pipeline_advanced/index.html#course-overview",
    "title": "Advanced Pipeline Development",
    "section": "Course Overview",
    "text": "Course Overview\n\nModule 1: Advanced Concepts in Data Pipeline Architecture\n\nReview of Pipeline Fundamentals\nComplex Pipeline Architectures\n\n\n\nModule 2: Deep Dive into Viash Functionality\n\nAdvanced Features of Viash\nCustom Components and Modularity\n\n\n\nModule 3: Integrating Diverse Data Types and Sources\n\nHandling Heterogeneous Data\nData Source Integration\n\n\n\nModule 4: Pipeline Optimization and Performance Tuning\n\nPerformance Analysis\nOptimization Techniques\n\n\n\nModule 5: Automation and Continuous Integration/Continuous Deployment (CI/CD)\n\nAutomating Pipeline Processes\nImplementing CI/CD in Pipeline Development\n\n\n\nModule 6: Case Studies and Real-World Applications\n\nComplex Use Cases\nIndustry and Research Application"
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#objectives",
    "href": "services/course/pipeline_advanced/index.html#objectives",
    "title": "Advanced Pipeline Development",
    "section": "Objectives",
    "text": "Objectives\n\nAdvance participants’ understanding and skills in complex data pipeline development.\nProvide deep insights into the optimization and management of high-performance pipelines in bioinformatics.\nEquip participants with the ability to integrate advanced tools and techniques into their pipeline development processes."
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#required-skills",
    "href": "services/course/pipeline_advanced/index.html#required-skills",
    "title": "Advanced Pipeline Development",
    "section": "Required skills",
    "text": "Required skills\n\nA solid foundation in data pipeline development with Viash and a good understanding of bioinformatics, data analysis, and scripting languages. Prior experience with pipeline development with Viash, is highly recommended."
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#conclusion",
    "href": "services/course/pipeline_advanced/index.html#conclusion",
    "title": "Advanced Pipeline Development",
    "section": "Conclusion:",
    "text": "Conclusion:\nParticipants of the “Advanced Data Pipeline Development” course will emerge with a profound expertise in creating, optimizing, and managing sophisticated data pipelines. They will gain practical knowledge in handling complex data types, integrating advanced features, and implementing automation and CI/CD in pipeline workflows. This course is aimed at empowering professionals to tackle the most challenging aspects of data pipeline development in the evolving field of bioinformatics and biotechnological research."
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#trainers",
    "href": "services/course/pipeline_advanced/index.html#trainers",
    "title": "Advanced Pipeline Development",
    "section": "Trainers",
    "text": "Trainers\n\nRobrecht Cannoodt\nToni Verbeiren"
  },
  {
    "objectID": "services/course/pipeline_advanced/index.html#program",
    "href": "services/course/pipeline_advanced/index.html#program",
    "title": "Advanced Pipeline Development",
    "section": "Program",
    "text": "Program\n              \n              \n                \n                    - \n                \n                \n                    -  -  Online\n                \n                \n                   500,00 € taxes excluded\n                \n              \n              \n              Enroll This Course\n            \n             \n             \n              Enroll Multiple Courses\n            \n            \n            \n             Our  general terms and conditions apply to all contracts and courses.  \n            \n\n Back"
  },
  {
    "objectID": "commontext/salary_culture.html",
    "href": "commontext/salary_culture.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Our competitive compensation package includes an attractive salary, numerous benefits, a potential company car, and an equity stake.\nThis position primarily involves remote work, offering flexibility tailored to your personal needs. Our team gathers approximately eight times a year at an enjoyable working location for essential meetings and in-person collaborations.\nYou will embrace a modern approach to remote work, working within our virtual office and effortlessly connecting and communicating with your colleagues for discussing any challenges or issues you may encounter.\nWe take pride in our distinctive company culture that emphasises open source, well-being, sustainability, and technology-driven innovation. We value our collaboration with non-profit clients, as it allows us to contribute to their mission and create a positive impact."
  },
  {
    "objectID": "contact/contact.html",
    "href": "contact/contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "Contact Us\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  I’m interested in your Data Pipelines services \n\n\n  I’m interested in a Viash support contract for non-profit \n\n\n  I’m interested in a Viash support contract for industry \n\n\n  I’m interested in Benchmarking \n\n\n\n\n  I’m interested in different services \n\n\n  I’d like to know more about Data Intuitive \n\n\n  I’d like to subscribe to the Data Intuitive e-mail list \n\n\n\n\n \n\n\n\n \n\n\nBy submitting this form, you agree to the collection and use of your personal data in accordance with our privacy policy. Your data will be used solely for the purpose of responding to your inquiry and will not be shared with third parties. You have the right to access, modify, or erase your personal data at any time."
  },
  {
    "objectID": "contact/thank_contact.html",
    "href": "contact/thank_contact.html",
    "title": "Data Intuitive",
    "section": "",
    "text": "Thank you for reaching out to us!\nWe have received your message and will respond to your inquiry as soon as possible."
  }
]