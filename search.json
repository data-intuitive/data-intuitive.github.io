[
  {
    "objectID": "insights/news/2023-02-05-sustain/index.html",
    "href": "insights/news/2023-02-05-sustain/index.html",
    "title": "DI’s Sustainability and Wellness Initiative",
    "section": "",
    "text": "As a telework-based company, Data Intuitive is committed to promoting both sustainability and the well-being of its employees. With the support of Ready2improve and Axudo, the company is developing a comprehensive personal health and sustainability plan in collaboration with its employees. Through organized group sessions during team-building days, the company and its partner aim to foster discussions and generate ideas for a better telework future.\nOne of the difficulties faced by a fully remote company is fostering a sense of unity and collaboration among employees. To address this, the company explores different work methods and leverages technology to create a virtual environment that mimics the experience of working together in one large shared space. Teleworking also helps reduce commute emissions, promote sustainability, and support personal well-being through flexible work arrangements. The plan places a strong emphasis on open-source technology and support for non-profit scientific organizations, further reinforcing Data Intuitive’s commitment to sustainability.\nThis effort showcases Data Intuitive’s dedication to sustainability and well-being, and its aim to create a better future for both its employees and the planet.\n\nBack"
  },
  {
    "objectID": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "href": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "title": "Fueled by VLAIO and Driven by innovation",
    "section": "",
    "text": "Data Intuitive is pleased to announce that we have received funding from VLAIO, which will bolster our research and development efforts as we continue to push the boundaries of data science and bioinformatics data pipelines. We are grateful for the support of VLAIO and are excited to see what the future holds as we continue to innovate and progress in our field.\nBack"
  },
  {
    "objectID": "insights/news/2022-12-15-fit_to_go_global/index.html",
    "href": "insights/news/2022-12-15-fit_to_go_global/index.html",
    "title": "Fit to Go Global",
    "section": "",
    "text": "We are thrilled to announce that this website is being launched with the funding of Flanders Investment and Trade (FIT). This funding will not only help us to create a professional and user-friendly website, but it will also support the international expansion of our company. We are grateful for the support and belief in our mission, and we look forward to working together as we continue to grow and develop on a global scale.\nBack"
  },
  {
    "objectID": "insights/use_cases/openpipelines/index.html",
    "href": "insights/use_cases/openpipelines/index.html",
    "title": "OpenPipelines",
    "section": "",
    "text": "Breakthroughs in single-cell genomics have been made possible by the simultaneous advancement of experimental and computational technologies. While experimental techniques standardize quickly, computational analysis pipelines are more challenging to compare and thus vary. Analysis platforms such as Seurat and Scanpy have facilitated pipeline building and introduced basic quality standards. Furthermore, we have made first attempts at standardizing workflows for scRNA-seq to bridge the language-gap between these platforms and their users.\nHowever, as new analysis methods are being proposed at an increasing rate and novel multi-omic sequencing technologies are becoming commonplace, there is an urgent need for new analysis standards for single-cell (multi-)omics data.\nOpenPipelines are best-practice workflows for single-cell single- and multi-omics data. To ensure these workflows are accessible to non-experts and can be deployed in a fast and reproducible way, we will build these into reproducible, modular and updatable best-practice analysis pipelines using industry-standard workflow tools, high-performance versions of popular methods and an interoperable, language-independent framework.\nBack"
  },
  {
    "objectID": "insights/use_cases/openproblems/index.html",
    "href": "insights/use_cases/openproblems/index.html",
    "title": "Open Problems for Single-Cell Analysis",
    "section": "",
    "text": "OpenProblems is an open-source platform for benchmarking and formalizing computational tasks in single-cell analysis. The platform aims to facilitate the development of new methods for single-cell omics by defining mathematical interpretations of tasks, providing publicly available gold-standard datasets in standardized formats, defining quantitative metrics to evaluate success, and ranking state-of-the-art methods in continuously updated leaderboards. OpenProblems is hosted on GitHub and evaluated using AWS with support from the Chan Zuckerberg Initiative.\nViash is being used in OpenProblems to streamline the development, execution, and sharing of methods, metrics and dataset loaders. It allows users to define their components as reusable modules that can be combined to form pipelines, and it manages dependencies, versioning, and execution on different computing environments. Viash also provides a user-friendly interface for configuring pipelines and visualizing results. By using Viash, OpenProblems can provide a flexible and scalable platform for benchmarking single-cell analysis methods.\nBack"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html",
    "title": "anndata: annotated data in R",
    "section": "",
    "text": "anndata is a commonly used Python package for keeping track of data and learned annotations, and can be used to read from and write to the h5ad file format. It is also the main data format used in the scanpy python package (Wolf, Angerer, and Theis 2018).\nHowever, using scanpy/anndata in R can be a major hassle. When trying to read an h5ad file, R users could approach this problem in one of two ways. A) You could read in the file manually (since it’s an H5 file), but this involves a lot of manual work and a lot of understanding on how the h5ad and H5 file formats work (also, expect major headaches from cryptic hdf5r bugs). Or B) interact with scanpy and anndata through reticulate, but run into issues converting some of the python objects into R.\nWe recently published anndata on CRAN, which is an R wrapper for the Python package – with some syntax sprinkled on top to make R users feel more at home.\nThe R anndata is still under active development at github.com/rcannood/anndata. If you encounter any issues, feel free to post an issue on GitHub!"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#installation",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#installation",
    "title": "anndata: annotated data in R",
    "section": "Installation",
    "text": "Installation\nInstalling anndata is not particularly hard. You do need R and Python to be installed. If you don’t have a working installation of Python, you can let reticulate install Miniconda.\n# install the R anndata package\ninstall.packages(\"anndata\")\n\n# skip this if you wish to use the local python installation\nreticulate::install_miniconda()\n\n# skip this if anndata is already installed\nanndata::install_anndata()"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#getting-started",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#getting-started",
    "title": "anndata: annotated data in R",
    "section": "Getting started",
    "text": "Getting started\nThe API of anndata is very similar to its Python counterpart. Check out ?anndata for a full list of the functions provided by this package.\nAnnData stores a data matrix X together with annotations of observations obs (obsm, obsp), variables var (varm, varp), and unstructured annotations uns.\nHere is an example of how to create an AnnData object with 2 observations and 3 variables.\n\nlibrary(anndata)\n\nad <- AnnData(\n  X = matrix(1:6, nrow = 2),\n  obs = data.frame(group = c(\"a\", \"b\"), row.names = c(\"s1\", \"s2\")),\n  var = data.frame(type = c(1L, 2L, 3L), row.names = c(\"var1\", \"var2\", \"var3\")),\n  layers = list(\n    spliced = matrix(4:9, nrow = 2),\n    unspliced = matrix(8:13, nrow = 2)\n  ),\n  obsm = list(\n    ones = matrix(rep(1L, 10), nrow = 2),\n    rand = matrix(rnorm(6), nrow = 2),\n    zeros = matrix(rep(0L, 10), nrow = 2)\n  ),\n  varm = list(\n    ones = matrix(rep(1L, 12), nrow = 3),\n    rand = matrix(rnorm(6), nrow = 3),\n    zeros = matrix(rep(0L, 12), nrow = 3)\n  ),\n  uns = list(\n    a = 1, \n    b = data.frame(i = 1:3, j = 4:6, value = runif(3)),\n    c = list(c.a = 3, c.b = 4)\n  )\n)\n\nad\n\nAnnData object with n_obs × n_vars = 2 × 3\n    obs: 'group'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\nYou can read the information back out using the $ notation.\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nad$obs\n\n   group\ns1     a\ns2     b\n\nad$obsm[[\"ones\"]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    1    1    1\n[2,]    1    1    1    1    1\n\nad$layers[[\"spliced\"]]\n\n   var1 var2 var3\ns1    4    6    8\ns2    5    7    9\n\nad$uns[[\"b\"]]\n\n  i j      value\n1 1 4 0.03063135\n2 2 5 0.97626754\n3 3 6 0.70391182\n\n\n\nUsing scanpy\nA nice side-effect of loading this package is that it is now super easy to interact with scanpy through these AnnData objects.\n\nlibrary(reticulate)\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nsc <- import(\"scanpy\")\nsc$pp$normalize_per_cell(ad)\n\nad$X\n\n       var1 var2     var3\ns1 1.166667  3.5 5.833333\ns2 1.750000  3.5 5.250000\n\n\n\n\nReading / writing AnnData objects\nRead from h5ad format:\nread_h5ad(\"pbmc_1k_protein_v3_processed.h5ad\")\n## AnnData object with n_obs × n_vars = 713 × 33538\n##     var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n##     uns: 'hvgParameters', 'normalizationParameters', 'pca', 'pcaParameters'\n##     obsm: 'X_pca'\n##     varm: 'PCs'\n\n\nCreating a view\nYou can use any of the regular R indexing methods to subset the AnnData object. This will result in a ‘View’ of the underlying data without needing to store the same data twice.\n\nview <- ad[, 2]\nview\n\nView of AnnData object with n_obs × n_vars = 2 × 1\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nview$is_view\n\n[1] TRUE\n\nad[,c(\"var1\", \"var2\")]\n\nView of AnnData object with n_obs × n_vars = 2 × 2\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nad[-1, ]\n\nView of AnnData object with n_obs × n_vars = 1 × 3\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\n\n\nAnnData as a matrix\nThe X attribute can be used as an R matrix:\n\nad$X[,c(\"var1\", \"var2\")]\n\n       var1 var2\ns1 1.166667  3.5\ns2 1.750000  3.5\n\nad$X[-1, , drop = FALSE]\n\n   var1 var2 var3\ns2 1.75  3.5 5.25\n\nad$X[, 2] <- 10\n\nYou can access a different layer matrix as follows:\n\nad$layers[\"unspliced\"]\n\n   var1 var2 var3\ns1    8   10   12\ns2    9   11   13\n\nad$layers[\"unspliced\"][,c(\"var2\", \"var3\")]\n\n   var2 var3\ns1   10   12\ns2   11   13\n\n\n\n\nNote on state\nIf you assign an AnnData object to another variable and modify either, both will be modified:\n\nad2 <- ad\n\nad$X[,2] <- 10\n\nlist(ad = ad$X, ad2 = ad2$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n$ad2\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n\nThis is standard Python behaviour but not R. In order to have two separate copies of an AnnData object, use the $copy() function:\n\nad3 <- ad$copy()\n\nad$X[,2] <- c(3, 4)\n\nlist(ad = ad$X, ad3 = ad3$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667    3 5.833333\ns2 1.750000    4 5.250000\n\n$ad3\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#future-work",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#future-work",
    "title": "anndata: annotated data in R",
    "section": "Future work",
    "text": "Future work\nIn some cases, this package may still act more like a Python package rather than an R package. Some more helper functions and helper classes need to be defined in order to fully encapsulate AnnData() objects. Examples are:\n\nad$chunked_X(1)\n\n<generator object AnnData.chunked_X at 0x7f8d3424eac0>\n\n\nFollowing functionality has not been tested:\nad$rename_categories(...)\nad$strings_to_categoricals(...)\nCurrently not implemented are the read_zarr() and ad$write_zarr() functions. I need some example data to test this functionality."
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#references",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#references",
    "title": "anndata: annotated data in R",
    "section": "References",
    "text": "References\nWolf, F Alexander, Philipp Angerer, and Fabian J Theis. 2018. “SCANPY: Large-Scale Single-Cell Gene Expression Data Analysis.” Genome Biology 19 (February): 15. https://doi.org/10.1186/s13059-017-1382-0.\nBack"
  },
  {
    "objectID": "insights/blog/2020-12-15-diflow/index.html",
    "href": "insights/blog/2020-12-15-diflow/index.html",
    "title": "DiFlow",
    "section": "",
    "text": "Developing and maintaining pipelines/workflows can be a genuine challenge. Doing this in a collaborative context adds even more to this complexity. We all dream of a flexible platform that allows us to easily express the computational requirements and is able to then run those optimally.\nData Intuitive is working in a project where such a pipeline is being developed. The choice of the platform is NextFlow and very early on we decided that we wanted to use DSL2 even if it was very early stages.\nIt turned out, though, that DSL2 in itself did not yet grant us enough flexibility. We ended up creating a set of conventions for creating modules that enable us to achieve our collaborative development goal.\nA DiFlow pipeline is a combination of modules:\n\nA module contains one step in a larger process\nEach module is independent\nA module can be tested\nA module runs in a dedicated and versioned container\nA module takes a triplet as argument:\n\n[ ID, data, config ]\nPlease refer to the GitHub repository for more information and documentation.\nBack"
  },
  {
    "objectID": "careers/data_workflow_developer/index.html",
    "href": "careers/data_workflow_developer/index.html",
    "title": "Bioinformatics Data Workflow Developer",
    "section": "",
    "text": "What we need\nData Intuitive is embarking on a new chapter and is seeking a highly skilled, scientifically minded IT professional to join our team. We are looking for an individual who is passionate about learning, eager to contribute to our team’s success, and excited to take on new challenges on this exciting new journey together.\nWhat we offer\nWe offer a very competitive salary, benefits, possibility of company car and equity stake. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nUnderstanding of fundamental biological principles\nA degree in bioinformatics, software engineering, computational biology, or other degree or proven experience in line with the requirements for this job\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical requirements\n\nScripting languages (shell, python, R, etc.)\nUsing Git and GitHub (creating branches, pull requests, managing issues, github actions)\nModern workflow management systems (Nextflow, Snakemake, …)\nContainerisation (Docker, Podman, or Singularity)\nData wrangling tabular/structured data (tidyverse or pandas)\nLinux based operating systems, libraries and tools\nWriting documentation in Markdown\n\nNice to haves knowledge\n\nHigh performance computing environment\nCloud computing\nSingle-cell omics (scRNA-seq, multi-omics)\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  },
  {
    "objectID": "careers/web_developer/index.html",
    "href": "careers/web_developer/index.html",
    "title": "Full Stack Web Developer",
    "section": "",
    "text": "What we need\nData Intuitive is currently intensively working on a new chapter and is seeking a motivated full stack web developer to join our team. We are looking for an individual who is passionate about learning, eager to contribute to our team’s success, and excited to take on new challenges on this exciting new journey together.\nWhat we offer\nWe offer a very competitive salary, benefits, possibility of company car and equity stake. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nA degree in software engineering or other degree or proven experience in line with the requirements for this job\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical requirements\n\nHTML, CSS, JavaScript, Node.js\nweb development frameworks and tools, such as Angular, React, Vue, Laravel, Ruby on Rails, Django, and Express.js.\nScripting languages (shell, python, R, etc.)\nUsing Git and GitHub (creating branches, pull requests, managing issues, github actions)\nContainerisation (Docker, Podman, or Singularity)\nLinux based operating systems, libraries and tools\nExperience working with databases, such as MySQL, PostgreSQL, and MongoDB.\n\nNice to haves knowledge\n\nHigh performance computing environment\nCloud computing\nModern workflow management systems (Nextflow, Snakemake, …)\nWriting in Markdown\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  }
]