[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "anndata: annotated data in R\n\n\nPorting anndata to R with reticulate.\n\n\n\n\nData science\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2021\n\n\nRobrecht Cannoodt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiFlow\n\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2020\n\n\nToni Verbeiren\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2021-02-02-anndata-for-r/index.html",
    "href": "blog/posts/2021-02-02-anndata-for-r/index.html",
    "title": "anndata: annotated data in R",
    "section": "",
    "text": "anndata is a commonly used Python package for keeping track of data and learned annotations, and can be used to read from and write to the h5ad file format. It is also the main data format used in the scanpy python package (Wolf, Angerer, and Theis 2018).\nHowever, using scanpy/anndata in R can be a major hassle. When trying to read an h5ad file, R users could approach this problem in one of two ways. A) You could read in the file manually (since it’s an H5 file), but this involves a lot of manual work and a lot of understanding on how the h5ad and H5 file formats work (also, expect major headaches from cryptic hdf5r bugs). Or B) interact with scanpy and anndata through reticulate, but run into issues converting some of the python objects into R.\nWe recently published anndata on CRAN, which is an R wrapper for the Python package – with some syntax sprinkled on top to make R users feel more at home.\nThe R anndata is still under active development at github.com/rcannood/anndata. If you encounter any issues, feel free to post an issue on GitHub!"
  },
  {
    "objectID": "blog/posts/2021-02-02-anndata-for-r/index.html#installation",
    "href": "blog/posts/2021-02-02-anndata-for-r/index.html#installation",
    "title": "anndata: annotated data in R",
    "section": "Installation",
    "text": "Installation\nInstalling anndata is not particularly hard. You do need R and Python to be installed. If you don’t have a working installation of Python, you can let reticulate install Miniconda.\n# install the R anndata package\ninstall.packages(\"anndata\")\n\n# skip this if you wish to use the local python installation\nreticulate::install_miniconda()\n\n# skip this if anndata is already installed\nanndata::install_anndata()"
  },
  {
    "objectID": "blog/posts/2021-02-02-anndata-for-r/index.html#getting-started",
    "href": "blog/posts/2021-02-02-anndata-for-r/index.html#getting-started",
    "title": "anndata: annotated data in R",
    "section": "Getting started",
    "text": "Getting started\nThe API of anndata is very similar to its Python counterpart. Check out ?anndata for a full list of the functions provided by this package.\nAnnData stores a data matrix X together with annotations of observations obs (obsm, obsp), variables var (varm, varp), and unstructured annotations uns.\nHere is an example of how to create an AnnData object with 2 observations and 3 variables.\n\nlibrary(anndata)\n\nad <- AnnData(\n  X = matrix(1:6, nrow = 2),\n  obs = data.frame(group = c(\"a\", \"b\"), row.names = c(\"s1\", \"s2\")),\n  var = data.frame(type = c(1L, 2L, 3L), row.names = c(\"var1\", \"var2\", \"var3\")),\n  layers = list(\n    spliced = matrix(4:9, nrow = 2),\n    unspliced = matrix(8:13, nrow = 2)\n  ),\n  obsm = list(\n    ones = matrix(rep(1L, 10), nrow = 2),\n    rand = matrix(rnorm(6), nrow = 2),\n    zeros = matrix(rep(0L, 10), nrow = 2)\n  ),\n  varm = list(\n    ones = matrix(rep(1L, 12), nrow = 3),\n    rand = matrix(rnorm(6), nrow = 3),\n    zeros = matrix(rep(0L, 12), nrow = 3)\n  ),\n  uns = list(\n    a = 1, \n    b = data.frame(i = 1:3, j = 4:6, value = runif(3)),\n    c = list(c.a = 3, c.b = 4)\n  )\n)\n\nad\n\nAnnData object with n_obs × n_vars = 2 × 3\n    obs: 'group'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\nYou can read the information back out using the $ notation.\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nad$obs\n\n   group\ns1     a\ns2     b\n\nad$obsm[[\"ones\"]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    1    1    1\n[2,]    1    1    1    1    1\n\nad$layers[[\"spliced\"]]\n\n   var1 var2 var3\ns1    4    6    8\ns2    5    7    9\n\nad$uns[[\"b\"]]\n\n  i j     value\n1 1 4 0.5621165\n2 2 5 0.9093555\n3 3 6 0.1762186\n\n\n\nUsing scanpy\nA nice side-effect of loading this package is that it is now super easy to interact with scanpy through these AnnData objects.\n\nlibrary(reticulate)\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nsc <- import(\"scanpy\")\nsc$pp$normalize_per_cell(ad)\n\nad$X\n\n       var1 var2     var3\ns1 1.166667  3.5 5.833333\ns2 1.750000  3.5 5.250000\n\n\n\n\nReading / writing AnnData objects\nRead from h5ad format:\nread_h5ad(\"pbmc_1k_protein_v3_processed.h5ad\")\n## AnnData object with n_obs × n_vars = 713 × 33538\n##     var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n##     uns: 'hvgParameters', 'normalizationParameters', 'pca', 'pcaParameters'\n##     obsm: 'X_pca'\n##     varm: 'PCs'\n\n\nCreating a view\nYou can use any of the regular R indexing methods to subset the AnnData object. This will result in a ‘View’ of the underlying data without needing to store the same data twice.\n\nview <- ad[, 2]\nview\n\nView of AnnData object with n_obs × n_vars = 2 × 1\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nview$is_view\n\n[1] TRUE\n\nad[,c(\"var1\", \"var2\")]\n\nView of AnnData object with n_obs × n_vars = 2 × 2\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nad[-1, ]\n\nView of AnnData object with n_obs × n_vars = 1 × 3\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\n\n\nAnnData as a matrix\nThe X attribute can be used as an R matrix:\n\nad$X[,c(\"var1\", \"var2\")]\n\n       var1 var2\ns1 1.166667  3.5\ns2 1.750000  3.5\n\nad$X[-1, , drop = FALSE]\n\n   var1 var2 var3\ns2 1.75  3.5 5.25\n\nad$X[, 2] <- 10\n\nYou can access a different layer matrix as follows:\n\nad$layers[\"unspliced\"]\n\n   var1 var2 var3\ns1    8   10   12\ns2    9   11   13\n\nad$layers[\"unspliced\"][,c(\"var2\", \"var3\")]\n\n   var2 var3\ns1   10   12\ns2   11   13\n\n\n\n\nNote on state\nIf you assign an AnnData object to another variable and modify either, both will be modified:\n\nad2 <- ad\n\nad$X[,2] <- 10\n\nlist(ad = ad$X, ad2 = ad2$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n$ad2\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n\nThis is standard Python behaviour but not R. In order to have two separate copies of an AnnData object, use the $copy() function:\n\nad3 <- ad$copy()\n\nad$X[,2] <- c(3, 4)\n\nlist(ad = ad$X, ad3 = ad3$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667    3 5.833333\ns2 1.750000    4 5.250000\n\n$ad3\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000"
  },
  {
    "objectID": "blog/posts/2021-02-02-anndata-for-r/index.html#future-work",
    "href": "blog/posts/2021-02-02-anndata-for-r/index.html#future-work",
    "title": "anndata: annotated data in R",
    "section": "Future work",
    "text": "Future work\nIn some cases, this package may still act more like a Python package rather than an R package. Some more helper functions and helper classes need to be defined in order to fully encapsulate AnnData() objects. Examples are:\n\nad$chunked_X(1)\n\n<generator object AnnData.chunked_X at 0x7f8dc9799cb0>\n\n\nFollowing functionality has not been tested:\nad$rename_categories(...)\nad$strings_to_categoricals(...)\nCurrently not implemented are the read_zarr() and ad$write_zarr() functions. I need some example data to test this functionality."
  },
  {
    "objectID": "blog/posts/2021-02-02-anndata-for-r/index.html#references",
    "href": "blog/posts/2021-02-02-anndata-for-r/index.html#references",
    "title": "anndata: annotated data in R",
    "section": "References",
    "text": "References\nWolf, F Alexander, Philipp Angerer, and Fabian J Theis. 2018. “SCANPY: Large-Scale Single-Cell Gene Expression Data Analysis.” Genome Biology 19 (February): 15. https://doi.org/10.1186/s13059-017-1382-0."
  },
  {
    "objectID": "blog/posts/2020-12-15-diflow/index.html",
    "href": "blog/posts/2020-12-15-diflow/index.html",
    "title": "DiFlow",
    "section": "",
    "text": "Developing and maintaining pipelines/workflows can be a genuine challenge. Doing this in a collaborative context adds even more to this complexity. We all dream of a flexible platform that allows us to easily express the computational requirements and is able to then run those optimally.\nData Intuitive is working in a project where such a pipeline is being developed. The choice of the platform is NextFlow and very early on we decided that we wanted to use DSL2 even if it was very early stages.\nIt turned out, though, that DSL2 in itself did not yet grant us enough flexibility. We ended up creating a set of conventions for creating modules that enable us to achieve our collaborative development goal.\nA DiFlow pipeline is a combination of modules:\n\nA module contains one step in a larger process\nEach module is independent\nA module can be tested\nA module runs in a dedicated and versioned container\nA module takes a triplet as argument:\n\n[ ID, data, config ]\nPlease refer to the Github repository for more information and documentation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Data Intuitive",
    "section": "",
    "text": "Founded in 2011, Data Intuitive is a Belgium-based company that specializes in bioinformatics, data science, and pipeline development. We are a team of experienced data scientists, bioinformaticians, and software engineers who are dedicated to helping our clients unlock the full potential of their data.\nOur company was founded on the belief that data has the power to transform the way we understand and approach complex biological and medical problems. Through the use of advanced data analysis techniques, we help our clients gain valuable insights and make data-driven decisions.\nOur team has extensive experience working with a wide range of data types, including genomics, proteomics, and clinical data. We are experts in developing custom pipelines and tools to help our clients manage, analyze, and interpret their data.\nIn addition to our core services in bioinformatics and data science, we also offer a range of support services, including training and consultation. We are committed to helping our clients build in-house capabilities and expertise in data analysis and interpretation.\nAt Data Intuitive, we are passionate about using data to drive progress in the fields of biology and medicine. We strive to be at the forefront of innovation in our field, and are always looking for new challenges and opportunities to make an impact."
  },
  {
    "objectID": "products/compass.html",
    "href": "products/compass.html",
    "title": "ComPass",
    "section": "",
    "text": "Compass is a powerful application that allows you to easily query and analyze L1000 data, as well as potentially related data and information. With Compass, you can quickly and easily gain insights into your data, making it a valuable resource for researchers and data scientists alike.\nCompass is comprised of two main components – LuciusWeb and LuciusAPI. LuciusWeb is the web-based component of Compass, allowing you to easily query and interact with your L1000 data from any web browser. LuciusAPI, on the other hand, is a Spark Jobserver project that provides the underlying data processing power for Compass.\nWith Compass, you can easily:\n\nQuery and analyze L1000 data in an interactive manner\nExplore potentially related data and information\nGain valuable insights into your data\nAnd more!\n\nIf you’re interested in learning more about Compass and seeing it in action, please don’t hesitate to contact us for a demo. Our team would be happy to show you how Compass can help you with your L1000 data analysis needs."
  },
  {
    "objectID": "products/viash.html",
    "href": "products/viash.html",
    "title": "Viash",
    "section": "",
    "text": "Introducing Viash – the tool that simplifies the creation of data pipelines. With Viash, you can easily turn your scripts into modular components that can be used seamlessly in any data pipeline.\nViash uses code generation to automatically wrap your script in a data workflow module, complete with built-in interfaces for inputs and outputs. This means that you can focus on the functionality of your script, without worrying about the data workflow framework.\nWith Viash, even those without expert knowledge in data workflow can create high quality modules that can be easily integrated into any pipeline. So why spend hours trying to figure out the ins and outs of data workflow frameworks, when you can use Viash to quickly and easily create modular components?\nTry Viash today and see how it can make your data pipeline creation process a breeze. For more info, visit viash.io."
  },
  {
    "objectID": "careers/index.html",
    "href": "careers/index.html",
    "title": "Careers",
    "section": "",
    "text": "Join the Data Intuitive Team and Collaborate on Innovative Projects\nOur company is dedicated to research and development, and offers a range of services to help clients build their own expertise in bioinformatics and data science. We are known for our agile approach to projects, and we are always looking for new ways to solve challenging problems and make a difference in the industry. When you join us, you will have the chance to work on both client and in-house projects with the support of your colleagues at every stage of the process.\nAt Data Intuitive, we value the expertise and skills of our employees, which is why we offer highly competetive pay and seek to hire the strongest technical profiles. As a candidate, you will need to pass a technical test and participate in a technical interview to demonstrate your knowledge and ability to handle complex IT techniques. As a member of our team, you can expect to work at a high technical level with talented colleagues.\nOur team is made up of remote workers who come together for in-person meetings six times per year to strengthen relationships and improve communication.\nCurrent open positions\n\n\n\n\n\n\n\n\n\n\nBioinformatics Data Workflow Developer\n\n\nFlanders, Belgium\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n\nApplication Form\n\n\nFirst name *  Last Name *   Email *   Upload your CV in PDF format  Message *\n\n\n \n\nBy submitting this form, you agree to the collection and use of your personal data in accordance with our privacy policy. Your data will be used solely for the purpose of responding to your inquiry and will not be shared with third parties. You have the right to access, modify, or erase your personal data at any time."
  },
  {
    "objectID": "careers/data_workflow_developer/index.html",
    "href": "careers/data_workflow_developer/index.html",
    "title": "Bioinformatics Data Workflow Developer",
    "section": "",
    "text": "What we need\nData Intuitive is embarking on a new chapter and is seeking a highly skilled, scientifically minded IT professional to join our team. We are looking for an individual who is passionate about learning, eager to contribute to our team’s success, and excited to take on new challenges on this exciting new journey together.\nWhat we offer\nWe offer a very competitive salary, benefits, possibility of company car and equity stake. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nUnderstanding of fundamental biological principles\nA degree in bioinformatics, software engineering, computational biology, or other degree or proven experience in line with the requirements for this job\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical requirements\n\nScripting languages (shell, python, R, etc.)\nUsing Git and GitHub (creating branches, pull requests, managing issues, github actions)\nModern workflow management systems (Nextflow, Snakemake, …)\nContainerisation (Docker, Podman, or Singularity)\nData wrangling tabular/structured data (tidyverse or pandas)\nLinux based operating systems, libraries and tools\nWriting documentation in Markdown\n\nNice to haves knowledge\n\nHigh performance computing environment\nCloud computing\nSingle-cell omics (scRNA-seq, multi-omics)\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "First name *  Last Name *   Company email *   Phone Number  \n\n\n   I’m interested in your Data Pipelines services \n   I’m interested in a Viash support contract \n   I’m interested in Compass \n   I’m interested in different services \n   I’d like to know more about Data Intuitive \n   I’d like to subscribe to the Data Intuitive email list \n\n\nCompany name *  What are you looking for? \n\n\n \n\nBy submitting this form, you agree to the collection and use of your personal data in accordance with our privacy policy. Your data will be used solely for the purpose of responding to your inquiry and will not be shared with third parties. You have the right to access, modify, or erase your personal data at any time."
  }
]