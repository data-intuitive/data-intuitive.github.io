[
  {
    "objectID": "careers/web_developer/index.html",
    "href": "careers/web_developer/index.html",
    "title": "Full Stack Web Developer",
    "section": "",
    "text": "What we need\nData Intuitive is currently intensively working on a new chapter and is seeking a motivated full stack web developer to join our team. We are looking for an individual who is passionate about learning, eager to contribute to our team’s success, and excited to take on new challenges on this exciting new journey together.\nWhat we offer\nWe offer a very competitive salary, benefits, possibility of company car and equity stake. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nA degree in software engineering or other degree or proven experience in line with the requirements for this job\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical requirements\n\nHTML, CSS, JavaScript, Node.js\nweb development frameworks and tools, such as Angular, React, Vue, Laravel, Ruby on Rails, Django, and Express.js.\nScripting languages (shell, python, R, etc.)\nUsing Git and GitHub (creating branches, pull requests, managing issues, github actions)\nContainerisation (Docker, Podman, or Singularity)\nLinux based operating systems, libraries and tools\nExperience working with databases, such as MySQL, PostgreSQL, and MongoDB.\n\nNice to haves knowledge\n\nHigh performance computing environment\nCloud computing\nModern workflow management systems (Nextflow, Snakemake, …)\nWriting in Markdown\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  },
  {
    "objectID": "careers/data_workflow_developer/index.html",
    "href": "careers/data_workflow_developer/index.html",
    "title": "Bioinformatics Data Workflow Developer",
    "section": "",
    "text": "What we need\nData Intuitive is embarking on a new chapter and is seeking a highly skilled, scientifically minded IT professional to join our team. We are looking for an individual who is passionate about learning, eager to contribute to our team’s success, and excited to take on new challenges on this exciting new journey together.\nWhat we offer\nWe offer a very competitive salary, benefits, possibility of company car and equity stake. This is a full-time role, working remotely predominantly with a lot of flexibility in different ways to suit your personal circumstances. The team meets in the region of Ghent at a shared working space at key times throughout the year.\nJob requirements\n\nUnderstanding of fundamental biological principles\nA degree in bioinformatics, software engineering, computational biology, or other degree or proven experience in line with the requirements for this job\nStrong analytical skills\nStrong problem-solving skills\nIndependent and team player\nDriven by a technological curiosity and desire to make things better\nFluent in English\n\nTechnical requirements\n\nScripting languages (shell, python, R, etc.)\nUsing Git and GitHub (creating branches, pull requests, managing issues, github actions)\nModern workflow management systems (Nextflow, Snakemake, …)\nContainerisation (Docker, Podman, or Singularity)\nData wrangling tabular/structured data (tidyverse or pandas)\nLinux based operating systems, libraries and tools\nWriting documentation in Markdown\n\nNice to haves knowledge\n\nHigh performance computing environment\nCloud computing\nSingle-cell omics (scRNA-seq, multi-omics)\n\nSounds like you? Reach out to us at info@data-intuitive.com\nBack Apply"
  },
  {
    "objectID": "insights/use_cases/openproblems/index.html",
    "href": "insights/use_cases/openproblems/index.html",
    "title": "Open Problems for Single-Cell Analysis",
    "section": "",
    "text": "OpenProblems is an open-source platform for benchmarking and formalizing computational tasks in single-cell analysis. The platform aims to facilitate the development of new methods for single-cell omics by defining mathematical interpretations of tasks, providing publicly available gold-standard datasets in standardized formats, defining quantitative metrics to evaluate success, and ranking state-of-the-art methods in continuously updated leaderboards. OpenProblems is hosted on GitHub and evaluated using AWS with support from the Chan Zuckerberg Initiative.\nViash is being used in OpenProblems to streamline the development, execution, and sharing of methods, metrics and dataset loaders. It allows users to define their components as reusable modules that can be combined to form pipelines, and it manages dependencies, versioning, and execution on different computing environments. Viash also provides a user-friendly interface for configuring pipelines and visualizing results. By using Viash, OpenProblems can provide a flexible and scalable platform for benchmarking single-cell analysis methods.\n\nBack"
  },
  {
    "objectID": "insights/use_cases/openpipelines/index.html",
    "href": "insights/use_cases/openpipelines/index.html",
    "title": "OpenPipelines",
    "section": "",
    "text": "Breakthroughs in single-cell genomics have been made possible by the simultaneous advancement of experimental and computational technologies. While experimental techniques standardize quickly, computational analysis pipelines are more challenging to compare and thus vary. Analysis platforms such as Seurat and Scanpy have facilitated pipeline building and introduced basic quality standards. Furthermore, we have made first attempts at standardizing workflows for scRNA-seq to bridge the language-gap between these platforms and their users.\nHowever, as new analysis methods are being proposed at an increasing rate and novel multi-omic sequencing technologies are becoming commonplace, there is an urgent need for new analysis standards for single-cell (multi-)omics data.\nOpenPipelines are best-practice workflows for single-cell single- and multi-omics data. To ensure these workflows are accessible to non-experts and can be deployed in a fast and reproducible way, we will build these into reproducible, modular and updatable best-practice analysis pipelines using industry-standard workflow tools, high-performance versions of popular methods and an interoperable, language-independent framework.\n\nBack"
  },
  {
    "objectID": "insights/news/2023-02-05-sustain/index.html",
    "href": "insights/news/2023-02-05-sustain/index.html",
    "title": "DI’s Sustainability and Wellness Initiative",
    "section": "",
    "text": "As a telework-based company, Data Intuitive is committed to promoting both sustainability and the well-being of its employees. With the support of Ready2improve and Axudo, the company is developing a comprehensive personal health and sustainability plan in collaboration with its employees. Through organized group sessions during team-building days, the company and its partner aim to foster discussions and generate ideas for a better telework future.\nOne of the difficulties faced by a fully remote company is fostering a sense of unity and collaboration among employees. To address this, the company explores different work methods and leverages technology to create a virtual environment that mimics the experience of working together in one large shared space. Teleworking also helps reduce commute emissions, promote sustainability, and support personal well-being through flexible work arrangements. The plan places a strong emphasis on open-source technology and support for non-profit scientific organizations, further reinforcing Data Intuitive’s commitment to sustainability.\nThis effort showcases Data Intuitive’s dedication to sustainability and well-being, and its aim to create a better future for both its employees and the planet.\n\n\nBack"
  },
  {
    "objectID": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "href": "insights/news/2022-12-25-fueled_by_vlaio/index.html",
    "title": "Fueled by VLAIO and Driven by innovation",
    "section": "",
    "text": "Data Intuitive is pleased to announce that we have received funding from VLAIO, which will bolster our research and development efforts as we continue to push the boundaries of data science and bioinformatics data pipelines. We are grateful for the support of VLAIO and are excited to see what the future holds as we continue to innovate and progress in our field.\n\nBack"
  },
  {
    "objectID": "insights/news/2022-12-15-fit_to_go_global/index.html",
    "href": "insights/news/2022-12-15-fit_to_go_global/index.html",
    "title": "Fit to Go Global",
    "section": "",
    "text": "We are thrilled to announce that this website is being launched with the funding of Flanders Investment and Trade (FIT). This funding will not only help us to create a professional and user-friendly website, but it will also support the international expansion of our company. We are grateful for the support and belief in our mission, and we look forward to working together as we continue to grow and develop on a global scale.\n\nBack"
  },
  {
    "objectID": "insights/blog/2010-10-19-happy-with-the-current-result/index.html",
    "href": "insights/blog/2010-10-19-happy-with-the-current-result/index.html",
    "title": "Happy with the current result…",
    "section": "",
    "text": "After about 2 hours of hacking WordPress, I’m happy with the result. I got the original look and feel back and from a functionality point of view it offers what I need.\nI’m quite certain that switching to WordPress was the right thing to do…"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html",
    "title": "anndata: annotated data in R",
    "section": "",
    "text": "anndata is a commonly used Python package for keeping track of data and learned annotations, and can be used to read from and write to the h5ad file format. It is also the main data format used in the scanpy python package (Wolf, Angerer, and Theis 2018).\nHowever, using scanpy/anndata in R can be a major hassle. When trying to read an h5ad file, R users could approach this problem in one of two ways. A) You could read in the file manually (since it’s an H5 file), but this involves a lot of manual work and a lot of understanding on how the h5ad and H5 file formats work (also, expect major headaches from cryptic hdf5r bugs). Or B) interact with scanpy and anndata through reticulate, but run into issues converting some of the python objects into R.\nWe recently published anndata on CRAN, which is an R wrapper for the Python package – with some syntax sprinkled on top to make R users feel more at home.\nThe R anndata is still under active development at github.com/rcannood/anndata. If you encounter any issues, feel free to post an issue on GitHub!"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#installation",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#installation",
    "title": "anndata: annotated data in R",
    "section": "Installation",
    "text": "Installation\nInstalling anndata is not particularly hard. You do need R and Python to be installed. If you don’t have a working installation of Python, you can let reticulate install Miniconda.\n# install the R anndata package\ninstall.packages(\"anndata\")\n\n# skip this if you wish to use the local python installation\nreticulate::install_miniconda()\n\n# skip this if anndata is already installed\nanndata::install_anndata()"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#getting-started",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#getting-started",
    "title": "anndata: annotated data in R",
    "section": "Getting started",
    "text": "Getting started\nThe API of anndata is very similar to its Python counterpart. Check out ?anndata for a full list of the functions provided by this package.\nAnnData stores a data matrix X together with annotations of observations obs (obsm, obsp), variables var (varm, varp), and unstructured annotations uns.\nHere is an example of how to create an AnnData object with 2 observations and 3 variables.\n\nlibrary(anndata)\n\nad <- AnnData(\n  X = matrix(1:6, nrow = 2),\n  obs = data.frame(group = c(\"a\", \"b\"), row.names = c(\"s1\", \"s2\")),\n  var = data.frame(type = c(1L, 2L, 3L), row.names = c(\"var1\", \"var2\", \"var3\")),\n  layers = list(\n    spliced = matrix(4:9, nrow = 2),\n    unspliced = matrix(8:13, nrow = 2)\n  ),\n  obsm = list(\n    ones = matrix(rep(1L, 10), nrow = 2),\n    rand = matrix(rnorm(6), nrow = 2),\n    zeros = matrix(rep(0L, 10), nrow = 2)\n  ),\n  varm = list(\n    ones = matrix(rep(1L, 12), nrow = 3),\n    rand = matrix(rnorm(6), nrow = 3),\n    zeros = matrix(rep(0L, 12), nrow = 3)\n  ),\n  uns = list(\n    a = 1, \n    b = data.frame(i = 1:3, j = 4:6, value = runif(3)),\n    c = list(c.a = 3, c.b = 4)\n  )\n)\n\nad\n\nAnnData object with n_obs × n_vars = 2 × 3\n    obs: 'group'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\nYou can read the information back out using the $ notation.\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nad$obs\n\n   group\ns1     a\ns2     b\n\nad$obsm[[\"ones\"]]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    1    1    1    1\n[2,]    1    1    1    1    1\n\nad$layers[[\"spliced\"]]\n\n   var1 var2 var3\ns1    4    6    8\ns2    5    7    9\n\nad$uns[[\"b\"]]\n\n  i j      value\n1 1 4 0.03063135\n2 2 5 0.97626754\n3 3 6 0.70391182\n\n\n\nUsing scanpy\nA nice side-effect of loading this package is that it is now super easy to interact with scanpy through these AnnData objects.\n\nlibrary(reticulate)\n\nad$X\n\n   var1 var2 var3\ns1    1    3    5\ns2    2    4    6\n\nsc <- import(\"scanpy\")\nsc$pp$normalize_per_cell(ad)\n\nad$X\n\n       var1 var2     var3\ns1 1.166667  3.5 5.833333\ns2 1.750000  3.5 5.250000\n\n\n\n\nReading / writing AnnData objects\nRead from h5ad format:\nread_h5ad(\"pbmc_1k_protein_v3_processed.h5ad\")\n## AnnData object with n_obs × n_vars = 713 × 33538\n##     var: 'gene_ids', 'feature_types', 'genome', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n##     uns: 'hvgParameters', 'normalizationParameters', 'pca', 'pcaParameters'\n##     obsm: 'X_pca'\n##     varm: 'PCs'\n\n\nCreating a view\nYou can use any of the regular R indexing methods to subset the AnnData object. This will result in a ‘View’ of the underlying data without needing to store the same data twice.\n\nview <- ad[, 2]\nview\n\nView of AnnData object with n_obs × n_vars = 2 × 1\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nview$is_view\n\n[1] TRUE\n\nad[,c(\"var1\", \"var2\")]\n\nView of AnnData object with n_obs × n_vars = 2 × 2\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\nad[-1, ]\n\nView of AnnData object with n_obs × n_vars = 1 × 3\n    obs: 'group', 'n_counts'\n    var: 'type'\n    uns: 'a', 'b', 'c'\n    obsm: 'ones', 'rand', 'zeros'\n    varm: 'ones', 'rand', 'zeros'\n    layers: 'spliced', 'unspliced'\n\n\n\n\nAnnData as a matrix\nThe X attribute can be used as an R matrix:\n\nad$X[,c(\"var1\", \"var2\")]\n\n       var1 var2\ns1 1.166667  3.5\ns2 1.750000  3.5\n\nad$X[-1, , drop = FALSE]\n\n   var1 var2 var3\ns2 1.75  3.5 5.25\n\nad$X[, 2] <- 10\n\nYou can access a different layer matrix as follows:\n\nad$layers[\"unspliced\"]\n\n   var1 var2 var3\ns1    8   10   12\ns2    9   11   13\n\nad$layers[\"unspliced\"][,c(\"var2\", \"var3\")]\n\n   var2 var3\ns1   10   12\ns2   11   13\n\n\n\n\nNote on state\nIf you assign an AnnData object to another variable and modify either, both will be modified:\n\nad2 <- ad\n\nad$X[,2] <- 10\n\nlist(ad = ad$X, ad2 = ad2$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n$ad2\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000\n\n\nThis is standard Python behaviour but not R. In order to have two separate copies of an AnnData object, use the $copy() function:\n\nad3 <- ad$copy()\n\nad$X[,2] <- c(3, 4)\n\nlist(ad = ad$X, ad3 = ad3$X)\n\n$ad\n       var1 var2     var3\ns1 1.166667    3 5.833333\ns2 1.750000    4 5.250000\n\n$ad3\n       var1 var2     var3\ns1 1.166667   10 5.833333\ns2 1.750000   10 5.250000"
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#future-work",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#future-work",
    "title": "anndata: annotated data in R",
    "section": "Future work",
    "text": "Future work\nIn some cases, this package may still act more like a Python package rather than an R package. Some more helper functions and helper classes need to be defined in order to fully encapsulate AnnData() objects. Examples are:\n\nad$chunked_X(1)\n\n<generator object AnnData.chunked_X at 0x7f8d3424eac0>\n\n\nFollowing functionality has not been tested:\nad$rename_categories(...)\nad$strings_to_categoricals(...)\nCurrently not implemented are the read_zarr() and ad$write_zarr() functions. I need some example data to test this functionality."
  },
  {
    "objectID": "insights/blog/2021-02-02-anndata-for-r/index.html#references",
    "href": "insights/blog/2021-02-02-anndata-for-r/index.html#references",
    "title": "anndata: annotated data in R",
    "section": "References",
    "text": "References\nWolf, F Alexander, Philipp Angerer, and Fabian J Theis. 2018. “SCANPY: Large-Scale Single-Cell Gene Expression Data Analysis.” Genome Biology 19 (February): 15. https://doi.org/10.1186/s13059-017-1382-0.\n\nBack"
  },
  {
    "objectID": "insights/blog/2013-08-16-turning-cloudera-quickstart-vm-into-an-r-studio-server/index.html",
    "href": "insights/blog/2013-08-16-turning-cloudera-quickstart-vm-into-an-r-studio-server/index.html",
    "title": "Turning Cloudera Quickstart VM into an R Studio Server",
    "section": "",
    "text": "Downloading the image (in my case for VirtualBox) is easy enough. Make sure that the VM has a connection to the internet.\nsudo yum -y install R\nsudo yum -y install wget\nFrom the R Studio Server website:\nwget http://download2.rstudio.org/rstudio-server-0.97.551-x86_64.rpm\nsudo yum install --nogpgcheck rstudio-server-0.97.551-x86_64.rpm\nReady! Make sure to connect using a local user (if necessary, create one).\nThere’s one caveat though. R version 3.0.1 is installed and most packages for working with Hadoop are not yet ported to version 3.\nA quick workaround is the following:\nWithin R:\ninstall.packages(c('Rcpp','RJSONIO','bitops','digest','functional','stringr','plyr','reshape2','rJava'))\nFrom the console:\nwget https://github.com/RevolutionAnalytics/rmr2/raw/master/build/rmr2_2.2.2.tar.gz\n\n\n\n\nwget https://github.com/RevolutionAnalytics/rhdfs/raw/master/build/rhdfs_1.0.6.tar.gz\n\n\n\n\nwget https://github.com/RevolutionAnalytics/rhbase/raw/master/build/rhbase_1.2.0.tar.gz\nFrom the console:\nsudo R CMD INSTALL r*.tar.gz\nThis should do the trick!"
  },
  {
    "objectID": "insights/blog/2012-12-05-slm-issues-with-traditional-slm-part-2/index.html",
    "href": "insights/blog/2012-12-05-slm-issues-with-traditional-slm-part-2/index.html",
    "title": "SLM: Issues with traditional SLM (Part 2)",
    "section": "",
    "text": "In our series of posts concerning SLM, SLAs, etc. we have started considering aspects of traditional SLM that lead us astray. We continue with the quantitative versus qualitative discussion.\n\nLack of representativeness\nThe reason for defining metrics and KPIs was to measure the quality of a service. Quality is an abstract and broad notion that may have a subjective connotation that is hard to put in words. In most cases, KPIs are a representation of an aspect of the service, not the quality of the service as a whole. Performance related metrics that deal with speed or turnover, for instance, are often used as KPIs (see examples 1, 2, 4, 5).\nPerformance (typically measured by the quantitative types of metrics, see here) is only part of the quality that is expected. Consider the following two examples:\n\nA ticket can be solved in the requested amount of time but using a temporary solution. Overall quality is suffering, having an impact on customer satisfaction.\nThe time on hold at a service desk is too high according to company standards, but most cases are resolved during this one call meaning that the effort pays and customers are positive about the service.\n\nIn other words, the lack of representation can go both ways: the quality may be higher or lower than expected, based on  the performance KPI.\nAnother example: It is true that, on average, a customer that has to wait long on the phone before getting someone on the phone will likely be less happy than someone who does not have to wait at all. It is true that an IT issue that is resolved in 1 hour will be more appreciated than one that takes 2 days. But living in the illusion that fixing all IT incidents within an hour is the way to have a perfect IT service desk, is doomed to fail."
  },
  {
    "objectID": "insights/blog/2007-12-17-performance-monitoring-more-about-peaks/index.html",
    "href": "insights/blog/2007-12-17-performance-monitoring-more-about-peaks/index.html",
    "title": "Performance Monitoring: More about Peaks",
    "section": "",
    "text": "In a previous post, we talked about averages (types of averages) and peaks and how peaks can tell you something about the spreading (variance, standard deviation) of the data.\nInformation about peaks is required (especially in capacity planning situations) to understand the sizing of the platform you’re running on. On the other hand, having a peak utilization of (say) 80% and an average of 20% still does not tell you that much: how long was the system running at high CPU levels? Maybe only for 10 seconds during the day (a scheduled database operation, backup procedures, etc.)? Is it crucial for our service that this high level of CPU can be guaranteed at that moment, or is it affordable to let the application/server wait a little longer for CPU requests? Think of a mail server, for instance, where it wouldn’t be a big deal if the server would forward your mail a few milliseconds later or earlier (would it?).\nBasically, what we need is a load profile for a server. A load profile contains information like:\n\nLoad during hour, day, week, month (or any other relevant period for this server)\nExpected response times instead of observed response times (basically, a cutoff on the resources)\nCurrent hardware inventory\nCurrent ‘scaled’ hardware inventory (20% CPU usage is different for a quad core than a single core, a scaled inventory takes that into account and enables easy comparison of systems)\nEtc.\n\nMore about load profiles later…"
  },
  {
    "objectID": "insights/blog/2015-05-29-code-snippet-repository/index.html",
    "href": "insights/blog/2015-05-29-code-snippet-repository/index.html",
    "title": "Code Snippet Repository",
    "section": "",
    "text": "I’m jumping between Scala/Spark coding, some Javascript in between, Python/PySpark and then some R every now and then. This in itself is already a challenge, but the worst thing is that I frequently encounter situations where I think: I’ve encountered this situation before. In many cases, it’s a situation that required quite some work to resolve. You end up with two possibilities: 1) retrieve the solution from some code somewhere on your harddisk or 2) start finding the solution again from scratchGoogle.\nSo I’m now wondering if this could not be organized better… Of course it can, but how? Github has the ability to store snippets of code, I could just keep a file handy for every programming environment/language. What are you using?\nHere’s an example of a snippet of Scala code that I need a lot: configuring a Spark context to use my credentials (store in environment variable) to connect to Amazon S3:\nval fs_s3_awsAccessKeyId = sys.env.get(\"AWS_ACCESS_KEY_ID\").getOrElse(\"<key\")\nval fs_s3_awsSecretAccessKey = sys.env.get(\"AWS_SECRET_ACCESS_KEY\").getOrElse(\"<key>\")\nsc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", fs_s3_awsAccessKeyId)\nsc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", fs_s3_awsSecretAccessKey)\nHow do you cope with this challenge?"
  },
  {
    "objectID": "insights/blog/2014-07-10-publishing-html-presentations-on-github/index.html",
    "href": "insights/blog/2014-07-10-publishing-html-presentations-on-github/index.html",
    "title": "Publishing html presentations on Github",
    "section": "",
    "text": "You’ve seen those fancy html presentations on the web? Reveal.js is a framework to create such things of beauty. And it goes along well with my Markdown based style of writing, even for presentation slides.\nI usually create the presentations on my laptop, using Pandoc to convert to html. In principle, this should be ready for the web by default. If only I had an easy hosting solution to move it to.\nGithub allows you to host html files and even complete web sites. I had never tried it myself, but now I did. It’s really simple. Move your repository into the branch gh-pages (you can do this on the Github website) and finished. The web site is accessible via http://<username>.github.io/<projectname>. This is what Github calls a Project Page.\nBehind the scenes, Github uses Jekyll for building static website from source files in, e.g., Markdown format. When you publish something to the gh-pages branch, it automatically kicks in… and gave very vague errors in my case.\nAfter some trial and error, I found out, the best approach is to install jekyll yourself and launch it locally. This immediately gives a readable error message. It turned out I had a stale symlink in my directory tree. Removing this removed the building issue for Github.\nThe result can be seen here: http://tverbeiren.github.io/BigDataBe-Spark.\nBTW, in order to install jekyll on my MacBook Air, I had to install a newer version of Ruby (tip: use rvm for this, link)."
  },
  {
    "objectID": "insights/blog/2015-10-29-thoughts-on-the-spark-summit-europe-2015/index.html",
    "href": "insights/blog/2015-10-29-thoughts-on-the-spark-summit-europe-2015/index.html",
    "title": "Thoughts on the Spark Summit Europe 2015",
    "section": "",
    "text": "In this post, I summarize some of the things I picked up at the Spark Summit. Some of these require more thought or research, but at least I’ll have a to do list of things to look back at.\nIn the meanwhile, a lot of the talks are posted already on Youtube.\nSo, let’s get started.\n\nDataframes\nI’ve been putting off using dataframes for a year now. There are two main reasons for doing this:\n\nI like the (functional) API of the RDD’s more than the dataframes API.\nI can do more with the RDD API which I need for most of the applications we’re working on.\n\nUnfortunately, a lot of the cool work on Spark and especially the machine learning (ML) and optimization (Tungsten) work is currently focussed on dataframes. I understand that, multiple people have explained why that is, but still I keep on thinking about the two reasons above.\nIn the next release (1.6), a new API will be available: DataSets. And it is exactly what I need to be happy using dataframes: The dataframe API, extended such that you can use the traditional functional API as well.\n\n\nsparkR\nFor the same reason I hadn’t used dataframes yet, I have not yet tried out using sparkR: it uses dataframes. The limitations of the dataframe means that when it comes to transforming the data in a dataframe, you are limited to the functions that are provided.\nIn Scala, it’s possible to define a UDF (User Defined Function), but until now that was not possible in sparkR. The next version of Spark, however, should support just that.\n\n\nDatabricks Cloud\nI’ve been really impressed by the Databricks notebooks. Having worked with different Open Source notebooks already (iPython, Zeppelin, Spark-Notebook), this one is definitely better.\nSome highlights:\n\nVery intuitive interface and extremely easy to attach a cluster to a notebook\nMultiple users on the same notebook works seamlessly, and much like editing a Google Document\nGood revision history\nImport code from files (%run ...) or from Github\nIncluded REST Server\nInteroperability between different notebooks\n\n\n\nSpark Streaming\nSpark streaming has long been the part of the Spark package that was least covered and used. Everyone found it cool and interesting.\nNow, during the talks, it became clear that the streaming aspect of Spark is very important. Lot’s of use cases described how they use Spark Streaming in their application, even in production.\n\n\nML pipelines\nI read about it. It looked cool on paper. Turns out it is even better in practice.\nMany times, different steps occur when going from a dataset to a model and predictions. Often-times these steps are poorly documented and as a consequence not reproducible.\nNow, with pipelines, you get a high-level API for configuring and running the different stages of a process. Downside is that it is yet another API to learn. But in this business, we don’t mind learning a few APIs…\nIt becomes even more interesting when the set of underlying algorithms and methods gets extended: hyper-parameter tuning, ML models, …"
  },
  {
    "objectID": "insights/blog/2010-10-19-open-letter-to-data-scientists/index.html",
    "href": "insights/blog/2010-10-19-open-letter-to-data-scientists/index.html",
    "title": "Open Letter to Data Scientists",
    "section": "",
    "text": "I share your passion for data, visualization, design, information, etc. Hell, I do. The same way I share the Business Intelligence interests with all the folks in business, small and large. Moreover, I turn out to be perfectly suited.\nI do not agree, however, with the apparent rationalization of all things, tangible and not.\nThis site is dedicated to finding the balance between data and intuition, between rational and emotional, between left and right brain thinking.\nCome back soon for more…\nKind regards,\nToni"
  },
  {
    "objectID": "insights/blog/2007-12-14-performance-monitoring-averages-and-peaks/index.html",
    "href": "insights/blog/2007-12-14-performance-monitoring-averages-and-peaks/index.html",
    "title": "Performance Monitoring: Averages and Peaks",
    "section": "",
    "text": "Now that we are into the topic of performance (of capacity) monitoring and planning, let us continue with something that has kept me busy the last couple of days: averages of performance data (and other statistical information) versus peaks.\nThis goes back to a classic textbook example in statistics, where the mean value of a series of data points is completely irrelevant as a representation of the data points itself. Let us consider the following example. Given a series of data like in the table below:\n\nX Y\nA 1\nB 2\nC 4\nD 7\nE 100\nF 4\nG 9\nH 7\nI 3\nJ 5\n\nThese may, for instance, represent scores (0-100) given to students (by a very strange teacher). In a graph, this is presented below:\n\n\n\n\n\nThe red line represents the average value. It is clear that everyone (except the teacher’s little friend with 100 points) is below the average. As a consequence, the average is not a good representation of the data as a whole. Some say it is too much influenced by the extreme values. In fact, this average (sum of the data values divided by the total amount of data points) is called the arithmetic mean. There is another notion of ‘an average’ which is called ‘geometric mean’. In the example above, the value of it would be 5.4 which is much more relevant. The median would even better define the data set, but that would lead us too far.\nBasically, the fact that the arithmetic mean does not give a good indication of the data set is caused by the large spread of the data. In statistics, there is another indicator for this: the variance, or standard deviation. It is a measure for how close or far apart the values are. In the example above, the standard deviation. In our example above, it would read 30.2. Suppose the value of E would be 10 instead of 100, the standard deviation becomes 3. In others words, the lower is the standard deviation, the more the data is ‘close’.\nThe above brings me back to performance monitoring. Somehow, I want to summarize the performance data of a server by a small set of indicators (averages of time etc.) that give a reasonable picture of the actual performance or in other words: that are representable for the system’s actual performance. Typically, when looking at the percentage a CPU is used over time, we see fluctuations that are similar to the figure above (don’t believe me, below is an actual example of a system with some high peaks and further sitting idle for most of the time - this server has 4 cores by the way). We conclude that, therefore, it does not make much sense to look at simple averages of the performance counters in order to get an idea of the behavior of the system..\n\n\n\n\n\nComing back to VMware Capacity Planner: the tool keeps track of the average value (yes, the simple average that does not say much) but also internally uses the geometric mean but not the variance (as far as I can tell). From this perspective, the whole performance gathering using this tool would be worthless. Luckily, it also keeps track of peak values (and calculates averages over these peaks but that is another story). Comparing the peak values with the average tells us a lot about the spread of the data points. The system behind the graph above, has an average CPU value of less than 20% while the peak CPU utilization is higher than 90%! This tells us that the variance/spreading in data points is large.\nIn a virtualization/consolidation assessment, these cases have to be taken into account, as we do not want our systems to become unresponsive because they have their peaks at the same time. More about this and other topics later…"
  },
  {
    "objectID": "insights/blog/2008-01-08-capacity-monitoring-for-desktops/index.html",
    "href": "insights/blog/2008-01-08-capacity-monitoring-for-desktops/index.html",
    "title": "Capacity Monitoring for Desktops?",
    "section": "",
    "text": "With the rise of VDI (Virtual Desktop Infrastructure, see here for a comprehensive overview) and the momentum it has, one starts to ask similar questions as with conventional server consolidation: what type of virtualization platform is required, how many users/desktops can I host, will there still be room for scaling, etc.\nThe way to approach this in server virtualization projects is by means of capacity monitoring and planning using virtualization scenarios. We refer to earlier posts for more information about this topic.\nThe question we are asking: can we use the same concepts and ideas for desktop virtualization? My answer is ‘NO’, because:\n\nA user acts completely different than a process or service: less predictable, depending on our mood, depending on the time of the day, etc.\nSome end-user applications ask 1OO% of the CPU even while they are not doing anything. The classic example used to be the game ‘Pinball’. Even a screensaver can take a large amount of CPU power.\nIn a VDI context, this becomes even more important. For instance: why would you scale your virtual desktop CPU and memory to include desktop search if really nothing personal can be found on the hosted desktop?\nWhen starting and running 10 applications at the same time, we will probably only use 5 of them later, but still… they require CPU power (and memory) while seeming idle.\nIf a user has meetings half the time, does that mean his session is closed? Does it still require processing power? How can this be analyzed?\nEtc.\n\nIn other words, a desktop environment is inherently different from a server environment. This is why in my opinion it is harder to maintain a Citrix farm than a VMware farm: applications and users tend to be less predictable and ‘stable’ than servers.\nDoes that mean that we can not do any sizing or planning in a VDI context? On the contrary, we should only keep in mind that applying exactly the same reasoning as with server consolidation planning is not a good idea.\nA few more tips:\n\nMake sure you have an overview of running processes and their CPU/memory utilization. This helps in deciding what is really important.\nBe careful with averages and peaks: a PC that is running all the time will have a low average, but may be used heavily during the day and a PC may be 100% used during the day because of some applications.\nTake into account inactive time during the day.\nDo not try to analyze the ‘average’ user, instead create classes (say 5 or so) that each have typical characteristics. Use these classes to create the virtualization scenarios."
  },
  {
    "objectID": "insights/blog/2010-02-24-life-expectancy-does-my-insurance-company-know-about-this/index.html",
    "href": "insights/blog/2010-02-24-life-expectancy-does-my-insurance-company-know-about-this/index.html",
    "title": "Life Expectancy: Does my Insurance Company know about This?",
    "section": "",
    "text": "In the previous post about life expectancy, we looked at some data of the last 20 years and extrapolated linearly. It seemed as if men and women would finally live equally long somewhere in the next decennium.\nWhile thinking and reading about the way life expectancy is calculated, it struck me that the calculation is not fair. I started out describing the way the calculation is done and why I think it is wrong. During this, I found out that the Wikipedia page about this topic already contained the answer:\n\nIt is important to note that this statistic is usually based on past mortality experience, and assumes that the same age-specific mortality rates will continue into the future. Thus such life expectancy figures are not generally appropriate for calculating how long any given individual of a particular age is expected to live. But they are a useful statistic to summarize the current health status of a population.  > > (from http://en.wikipedia.org/wiki/Life_expectancy) > >\n\nBasically, in the calculation, one assumes that when you are born does not influence the probability with which you will die (at a certain age). This is obviously false.\nThe article further describes that models exist to adjust the probabilities used in the calculations in order to correct for this systematic underestimation.\nCan someone guarantee me that my insurance company uses a corrected statistic instead of the original one? I’m afraid they think I’ll die 10 years earlier than statistically expected and thus charge me too much money?!"
  },
  {
    "objectID": "insights/blog/2007-12-17-performance-monitoring-correlations/index.html",
    "href": "insights/blog/2007-12-17-performance-monitoring-correlations/index.html",
    "title": "Performance Monitoring: Correlations",
    "section": "",
    "text": "Although I have been arguing that performance monitoring and capacity planning require a decent server montoring environment, it also requires more. This extra part comes from the fact that often services depend on each other. A web service connects to a database (hosted on a different server) and fetches data from the file server (local, or SAN/NAS). Often, one part in the chain is a bottleneck for the whole of the process. This is a shame and can be avoided by careful analysis of the correlations between performance data.\nAgain, this is an argument in favor of what I called a ‘load profile’ earlier. By modeling a server by means of a load profile, we get a representation of that server in terms of measurable quantities. Statistics and mathematics in general can then help us analyze the correlations between those load profiles."
  },
  {
    "objectID": "insights/blog/2020-12-15-diflow/index.html",
    "href": "insights/blog/2020-12-15-diflow/index.html",
    "title": "DiFlow",
    "section": "",
    "text": "Developing and maintaining pipelines/workflows can be a genuine challenge. Doing this in a collaborative context adds even more to this complexity. We all dream of a flexible platform that allows us to easily express the computational requirements and is able to then run those optimally.\nData Intuitive is working in a project where such a pipeline is being developed. The choice of the platform is NextFlow and very early on we decided that we wanted to use DSL2 even if it was very early stages.\nIt turned out, though, that DSL2 in itself did not yet grant us enough flexibility. We ended up creating a set of conventions for creating modules that enable us to achieve our collaborative development goal.\nA DiFlow pipeline is a combination of modules:\n\nA module contains one step in a larger process\nEach module is independent\nA module can be tested\nA module runs in a dedicated and versioned container\nA module takes a triplet as argument:\n\n[ ID, data, config ]\nPlease refer to the GitHub repository for more information and documentation.\n\nBack"
  },
  {
    "objectID": "insights/blog/2007-11-21-word-vs-excel/index.html",
    "href": "insights/blog/2007-11-21-word-vs-excel/index.html",
    "title": "Word vs. Excel",
    "section": "",
    "text": "I stumbled upon the iWork suite (aka Office for Mac) already some time ago, but today thought back of it as I was thinking about Excel and Word…\nThe main problem I have with Excel and Word is that there is nothing in-between. Let me explain what I mean by means of some examples:\nText-based tables: Excel (as most spreadsheets) is a numerical calculation sheet application. Although it has some functions to work with strings, it is not the primary goal of the application. This is fine, but in practice, we see that a lot of people (and even big companies) use it to store text-based data (e.g.: IP ranges, Application overview, user accounts, passwords, etc.)\n** Side-by-side comparison:** this is in fact an example of the previous point: how many times do we not want to compare pros and cons of something? Excel is not really optimized for this, but a table in Word is even worse.\nDocuments with a lot of tables: This really is on the boundary of both products. In order to keep the exact formatting as set in Excel, I usually copy/past the table into Word as a picture. This is not very efficient in terms of having to change some numbers in the tables. Using the OLE features with a conventional copy/paste is usually not an option as the table in Word does not look half as nice as it should.\nReports: This is again a special case of the previous point: sometimes the data in the tables is expected to change because it contains for instance KPI information, or extracts from a database. Using Excel to nicely print a paper/PDF report is hard, copying all the tables and graphs to a Word document every day or week is even harder. In practice, I solve this by using a macro that automatically creates a Word document and pastes the tables and graphs (as pictures) where they belong. This approach has been the most successful for me up till now.\nIt seems iNumber makes a step in the right direction. Take a look at some of the demos on the page and see for yourself: graphics, text and tables are mixed easily.\nIs this another ‘sign’ I have to switch to the Mac?"
  },
  {
    "objectID": "insights/blog/2015-01-15-transposing-a-spark-rdd/index.html",
    "href": "insights/blog/2015-01-15-transposing-a-spark-rdd/index.html",
    "title": "Transposing a Spark RDD",
    "section": "",
    "text": "I have been using Spark quite a lot for the last year. At first using the Scala interface, but lately more using the Python one.\nIn one of my recent projects, I received a dataset that contains expression profiles of chemical compounds on genes. That is to say, I got a dataset which had this data transposed, i.e., genes versus compounds, but that is not a handy format to work with. I load the original data into an RDD, but then I have to transpose this RDD.\nI have been looking on the web but found no complete solution. Recently, a similar question came up on the Spark mailinglist. So I thought it is about time that I posted my approach.\nThis is the code for the function that transposes an RDD and returns a new RDD. There are other approaches, and there is room for optimisation as well. But this already gets the work done.\ndef rddTranspose(rdd):\n    rddT1 = rdd.zipWithIndex()\n            .flatMap(lambda (x,i): [(i,j,e) for (j,e) in enumerate(x)])\n    rddT2 = rddT1.map(lambda (i,j,e): (j, (i,e)))\n            .groupByKey().sortByKey()\n    rddT3 = rddT2.map(lambda (i, x): sorted(list(x), \n                        cmp=lambda (i1,e1),(i2,e2) : cmp(i1, i2)))\n    rddT4 = rddT3.map(lambda x: map(lambda (i, y): y , x))\n    return rddT4.map(lambda x: np.asarray(x))\nThis code converts the rows to numpy arrays in the return statement, so you need to import numpy as np. This step is strictly speaking not necessary, but it does make subsequent random access inside the rows faster. It must be noted as well that the procedure only works when one row (one element of the original RDD as well as the transposed RDD) fits into the JVM memory of the workers.\nI left out the comments in my code, to keep it a little exciting for you…"
  },
  {
    "objectID": "insights/blog/2012-12-06-slm-issues-with-traditional-slm-part-3/index.html",
    "href": "insights/blog/2012-12-06-slm-issues-with-traditional-slm-part-3/index.html",
    "title": "SLM: Issues with traditional SLM (Part 3)",
    "section": "",
    "text": "Considering further some aspects of (traditional) SLM that should be fixed.\n\nThe wrong way around\nThe practice of defining (mainly performance-based) KPIs is wide-spread, so wide-spread in fact that often the reasoning is reversed: Instead of measuring a set of KPIs that may have a positive influence quality, one often (implicitly) states that the quality of a service is defined as the outcome of these KPIs.\nTo give an extreme example: Suppose the weather is proven to have an impact on customer satisfaction of a call center. Is it rational to state that the wheather defines quality? Should the Service Level be calculated based on the weather statistics? I don’t think many managers would buy this.\nOn the other hand, when Service Levels are defined, one often finds quality defined as just one parameter like, e.g., ‘waiting time on-hold’ or ‘system uptime’."
  },
  {
    "objectID": "insights/blog/2014-09-23-impact-in-risk-assessment/index.html",
    "href": "insights/blog/2014-09-23-impact-in-risk-assessment/index.html",
    "title": "Impact in Risk Assessment",
    "section": "",
    "text": "How many times have you seen project charters or business cases where a form of risk assessment has been provided? In many cases, people try to make the risks tangible and actionable by means of a risk matrix. In this approach, every risk gets 2 parameters associated to it: probability of the risk occurring and impact when the risk occurs. In most cases, 3 possibilities are given for both parameters: low, medium, high.\nThere are two very big advantages to this approach:\n\nIt gives people a framework, a way to quantify risks that is easy to understand\nThe method not only looks at the probability of an event, but also at its impact. The latter has practical consequences.\n\nThere are also disadvantages to this approach. Some are mentioned already on the Wikipedia page, but three are missing, in my opinion:\n\nMany events with high impact a) have often not been taught of as a risk or b) are the consequence of a combination of factors.\nUsually, events that are rare have a bigger impact. But the probability of rare events is very hard to assess.\nImpact appears to be a linear function, but it is not.\n\nLet us give an example of the latter point. One computer failing in the office has small impact because the user may get a replacement lying around in 10 minutes or less. 10 computers failing in the office will require more time, because no 10 people are standby to service them in parallel and spare computers may be lacking. Imagine a fire, a hack, or something we can not think of now (see above)? When hundreds of computers need to be replaced?\nThis is a situation where the impact of an event increases faster than linear (polynomial? exponential?). It may get worse, when an event causes an impact that is not recoverable anymore: too much electricity can kill a person, too many losses can ruin even a bank or a state, etc.\nThe first two arguments can be thought of as applications of the Black Swan concept to projects and risk. The last argument is an application of the concept that is elaborated on in a book by the same author: Antifragile. Books could be written just applying concepts from these books!\nWhat does it tell us? When considering mitigation of risks, think about what are possible consequences on a large scale, not on the scale of individual events. Think about consequences like: nobody is able to work anymore, the whole building is destroyed, our competitor has leapfrogged us, etc. Thinking about mitigation of these consequences may tell you more about the underlying risks and events than the other way around.\nOn a slightly cynical note: While doing so, you may find that more (advanced) technology is not always a solution because it usually makes things more complex and thus prone to more complex risks rather than avoiding them."
  },
  {
    "objectID": "insights/blog/2007-10-25-vmware-capacity-planner-taking-the-data-offline-part-1-introduction/index.html",
    "href": "insights/blog/2007-10-25-vmware-capacity-planner-taking-the-data-offline-part-1-introduction/index.html",
    "title": "VMware Capacity Planner: taking the data offline (Part 1: Introduction)",
    "section": "",
    "text": "Lately, I have been involved in some VMware Capacity Planner (VMCP) tracks. VMCP deals with monitoring a set of (physical) servers in order to assess possible virtualization scenarios.\nVMCP works in the following way: a monitoring server is set-up with a tool that does (but is not limited to) regular performance measurements. This tools sends the data to a VMware database through the web (over a secure channel). Via a web interface, one can then query the information, get reports, view trends, show graphs, configure and create consolidation scenarios, etc. Usually, we let the system run for around 4 weeks to get a realistic idea of the performance characteristics.\nWhat I like about VMCP is that the data is not located at the customer site, and available at all times (once it has been uploaded). This gives me the opportunity to regularly check on the status of the performance measurements.\nThe biggest disadvantage of VMCP is that the web interface is not the most flexible and fast interface around. Some things I would like to do are not available (lack of flexibility) but could easily be done in, e.g., Excel and at times when everyone in the world is awake it takes ages to refresh a page and get the result of a query. Moreover, it is not easy to get good-looking information to paste in a document.\nWhen it comes to writing a report, the customer is obviously not only interested in a statement like: you will need 5 ESX server of type X to cope with the load. Therefore, I like to add tables with the most useful metrics (CPU, network, Disk I/O, …) for later reference. I add this information as an appendix.\nThis is where I would spend at least half a day exporting CSV files from the VMCP website, loading them in Excel, laying it out as a nice table and paste it in the document. I started thinking about automating some of the steps required, and I covered the most time-consuming already: exporting the information from the website as a CSV file.\nIn the following part, I’ll explain how I started this little adventure…"
  },
  {
    "objectID": "insights/blog/2012-01-25-fight-against-tax-fraud-in-belgium/index.html",
    "href": "insights/blog/2012-01-25-fight-against-tax-fraud-in-belgium/index.html",
    "title": "Fight against Tax Fraud in Belgium",
    "section": "",
    "text": "The (brand new) Belgian government has decided to raise the stakes in the fight on tax fraud. That’s a good thing, especially in times of tight government budgets.\nIn the news last week, it is said that the group of people actively involved in detecting tax fraud will be enlarged by a quarter (a little less than 300 new hires). That means that in total, for Belgium alone, more than a thousand man and women are looking for ways to spot fraud. The involved government agencies will be looking for Masters in fields like ‘accountanting, law but also informatics’.\nThis sounds like the good old ‘brute force’ method: throw a bunch of people on a problem and when the problem gets bigger, throw more people at it.\nBasically, there’s one major field of interest missing from the list profiles in the press release: statistics. People trained in statistical analysis may have heard of Benford’s Law or related concepts. They may take a completely different approach to dealing with large data sets. Statisticians may propose to use relatively simple heuristics first, in order to select the subset of targets with the highest probability of fraud.\nAn example of what the process might look like:\n\nFrom a very large dataset, find a way to select a small subset that a) has the highest probability of fraud, b) has the highest probability of being able to prove fraud and c) actually benefits the government by selecting only the ‘big fish’.\nGet the informatics specialists to implement this heuristic algorithm as fast as possible, with ways to tune the individual parameters for deeper analysis.\nFrom the resulting small selection, invite accountants and lawyers to focus on those.\n\nHave I forgotten something? I’m tempted to try and contact the federal government agency responsible for tax fraud and propose to start with 1.\nUpdate: Thank you Paul for correcting some errors."
  },
  {
    "objectID": "insights/blog/2012-09-25-rational-decision-making-in-business/index.html",
    "href": "insights/blog/2012-09-25-rational-decision-making-in-business/index.html",
    "title": "Rational Decision Making in Business?",
    "section": "",
    "text": "In business contexts, generally speaking, we believe we act as rational beings. The rise of Business Intelligence and the wonders of Number Crunching and Big Data make us believe we have entered the era of rational decision making.\nIt appears to me that this way of thinking is similar to what has been observed in the economic sciences. For long, men was thought of as a rational being, making rational choices. It has taken Kahneman, Tversky and others several decades to counter this supposition.\nCould it be that the belief in a solely rational business logic is equally false? We have reasons to believe so. The good thing is, one can be a fan of Big Data but nevertheless keep in mind that:\n\nRationality is just like perfectionism: It can be nice to aim for, but attaining it may take too much effort.\n\nPS: Please note that non-rational does not imply irrational."
  },
  {
    "objectID": "insights/blog/2013-01-18-example-of-slm-issue/index.html",
    "href": "insights/blog/2013-01-18-example-of-slm-issue/index.html",
    "title": "Example of SLM issue",
    "section": "",
    "text": "This morning, I stumbled upon the following tweet:\nhttps://twitter.com/peteskomoroch/status/292140306806231040\nIt points to a blog post by Peter Skomorochin which he gives a nice example of the SLM issue we encountered in our last post. I especially liked the part where he quotes a former lecturer of his:\n\n“You’ll start off wanting to measure what you value, but you’ll end up valuing what you can measure”\n\nSounds familiar?"
  },
  {
    "objectID": "insights/blog/2013-01-23-slm-issues-with-traditional-slm-part-6/index.html",
    "href": "insights/blog/2013-01-23-slm-issues-with-traditional-slm-part-6/index.html",
    "title": "SLM: Issues with traditional SLM (Part 6)",
    "section": "",
    "text": "Gaming the system\nWe start by considering metric 1 in the examples given before, which is related to the on-hold time of a call. There are several ways to avoid the penalty of crossing the threshold for this metric:\n\nOne such way is to drop the call (a technical error is always possible after all).\nAnother way is to have an automated answering machine ask the user some (possibly irrelevant) questions.\nA third option is to answer the call but forward it to a different team as soon as the user starts explaining the question or problem. In none of these cases, the service can be called qualitative, but the appropriate metrics are perfect. Other metrics can be gamed in similar ways.\n\nTo make things worse: Since usually only a small amount of KPIs are selected for monitoring service quality, they are easy to keep an eye on and follow-up. It is thus possible to make sure most of the metrics are managed in such a way that success is guaranteed.\nIf you combine this effect with the earlier issues one gets a nice collection of, say 5 KPIs and concludes that the service quality is optimal."
  },
  {
    "objectID": "insights/blog/2012-12-04-slm-introduction-part-4/index.html",
    "href": "insights/blog/2012-12-04-slm-introduction-part-4/index.html",
    "title": "SLM: Introduction (part 4)",
    "section": "",
    "text": "We introduced the service level in a previous post. A Service Level can be defined/calculated for one service or a set of services.\n\nService Level Agreement\nA contract with a service provider typically includes several services. An agreement about each of these services and their respective targets and quality parameters is called a Service Level Agreement (SLA). Often, though, the term SLA is used not to denote the contract as such, but rather the Service Level calculated for all the services is in the contract."
  },
  {
    "objectID": "insights/blog/2008-09-15-how-to-measure-productivity/index.html",
    "href": "insights/blog/2008-09-15-how-to-measure-productivity/index.html",
    "title": "How to Measure Productivity?",
    "section": "",
    "text": "A comment by reader ‘rbis’ on my previous article about the definition of productivity refers to the website and blog of productivity guru Matthew Cornell. The reference reminded me of an article by Matt in which he asks the question: How do you measure personal productivity?.\n\nThe 3 Layers Again\nIn my previous article, I discussed the existence of 3 layers in what people consider (personal) productivity: Layer 1 (L1) deals with so-called life hacks, tricks on how to deal with tasks and things in a smarter way. Layer 2 (L2) is about the approach to these tasks and the organization of them, or in other words the process. In Layer 3 (L3), we look at the purpose behind everything, the driving force behind the whole thing.\nWhen thinking about these layers and the measurement of productivity, two conclusions pop up\n\n\nMeasurement Depends on the Layer\nIt is easier to measure in L1 than it is in L3. To give an example: testing ones speed of typing, or the number of blogs topics read is just a matter of counting. The fact that ones purpose is not clear or not lived out, is a much harder nut to crack.\nThis may sound obvious, but in practice, productivity is often seen as one big beast which has to be tackled with one method.\nTo come back to the article by Matt, careful reading reveals that if the layered approach had been used, things would have turned out even more transparent. Where he discusses that measures are required, he mostly deals with layer 1 and 2 activities (email processed, poor planning, inneficient meetings), whereas when talking about why measurements are hard, layer 3 comes into play (personal goals, quality instead of quantity).\n\n\nLayer 2 and Layer 3 are not about Counting\nIt will be clear by now that L3 can not be measured by numbers, but qualitative aspects are important. And qualitative by definition means subjective. This is a good thing, but it also makes it hard and confusing.\nStephen Covey may try hard to make sure we get our personal mission statement clear, but how many of us really have one? Or how many times a year is this mission statement revised and if needed adapted? Ok, I blame guilty myself.\nI think we feel it when our mission or life’s purpose is clear and our activities support it. I have noticed lately that two famous bloggers, in two different areas come to a similar conclusion: They were no longer certain that what they were blogging about really made sense to them, that it is what they wanted to do. An insightful article by Robert Scoble in this sense can be found here, whereas Merlin Mann discusses some of the thought process in this article.\nEnough about layers, let’s get productive!"
  },
  {
    "objectID": "insights/blog/2011-03-15-the-economist-on-corporate-governance/index.html",
    "href": "insights/blog/2011-03-15-the-economist-on-corporate-governance/index.html",
    "title": "The Economist on Corporate Governance",
    "section": "",
    "text": "In his article for The Economist, Schumpeter argues (based on observations and scientific research) that the corporate world _with _governance structures is not always better off than without:\n\nSome banks which performed best during the crisis flouted the rules of good corporate governance: Santander had a familial culture and a powerful executive chairman. And some banks which did worse were paragons of good governance: Citigroup and Lehman Brothers employed powerful outside directors (including, in Lehman’s case, an economist known as “Dr Doom”).\n\nA lot of factors are into play and corporate culture is probably more important than any monitoring system put in place. From the article:\n\n(..) the study does at least suggest that one should not expect too much from corporate governance. Good corporate governance on its own will not protect companies from taking excessive risks. They need to tackle the problem directly, by setting up better risk control, rather than indirectly by ticking various corporate-governance boxes.\n\nThis conclusion is a very interesting one. It comes down to acknowledging that dealing with a multi-dimensional world (risk, opportunity, strategy, etc.) can not be replaced by one metric under the codename governance. This is a mistake often made in a different setting. We’ll definitely come back to this theme in a later post…"
  },
  {
    "objectID": "insights/blog/2008-01-02-chargeback/index.html",
    "href": "insights/blog/2008-01-02-chargeback/index.html",
    "title": "Chargeback",
    "section": "",
    "text": "On the VMworld website, you can find a featured presentation concerning chargeback. The presentation is by the CEO of VKernel, a company that I have mentioned on this blog before.\nChargeback, as pointed out in the presentation, is about ‘measuring’ and accounting for two types of costs: Consumables and non-consumables. The latter has to do with floor space, licenses, support and administration, etc. Consumables refers to utilization of CPU, memory, power, etc. Obviously, performance monitoring is critical in this aspect. Check out the presentation (registration required)!"
  },
  {
    "objectID": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html",
    "href": "insights/blog/2016-05-12-a-practical-approach-to-model-error/index.html",
    "title": "A Practical Approach to Model Error",
    "section": "",
    "text": "In this post, I want to get a better sense of the effects of model error by simulating a very simple model for the spreading of a virus. It’s based on an analysis performed by Nassim Taleb. I used the simulation below in the scope of a workshop paper on the effect of cognitive biases. The published version of the paper can be found here.\n\nIntroduction\nI don’t want to discuss the biology of a virus or its spreading, even though virus spreading is the topic of the simulation we’re about to do. I invite you to look elsewhere for that.\nWe mainly want to get a feel for the main message Taleb’s argument in his analysis linked above: The dangers of basing decisions on simple models for the spreading of virusses and the lack discussing the risks involved in analysing these models.\nWe do this by modelling the spreading of a virus by means of a simple geometric Brownian motion process. It’s the same process that is used in the (in)famous Black-Scholes model.\nThe differential form of the process is as follows:\n\\[ d S(t) = S(t) \\mu dt + S(t) \\sigma d W(t)  \\]\nIn this formula, \\(\\mu\\) and \\(\\sigma\\) are generally referred to as respectively drift and volatility. \\(W(t)\\) is a Wiener process, i.e. a continuous random walk.\nBoth \\(\\mu\\) and \\(\\sigma\\) can be dependant on time or on the current state. But for the sake of the argument, this need not be the case.\nWe can discretize this stochastic differential equation simply by replacing the differentials \\(dt\\) with small finite differences in time \\(\\Delta t\\). The Wiener process can be approximated by a random walk with finite \\(\\Delta t\\) as:\n\\[ \\Delta W(t) \\approx \\sqrt{\\Delta t} \\, z \\]\nwhere \\(z\\) is normally distributed with mean 0 and variance 1, i.e.,\n\\[ z \\sim\\mathcal{N} (0,1) \\]\nIn other words, we can simulate a continous geometric Brownian motion process defined above by means of the following discretized version:\n\\[ \\Delta S(t) = S(t) \\mu \\Delta t + S(t) \\sigma \\sqrt{\\Delta t} \\, z  \\]\nTaleb uses a different representation of this formula, based on the solution of this equation:\n\\[ S(t + \\Delta t) = S(t) \\exp \\left[ \\left( \\mu - \\frac{1}{2} \\sigma^2 \\right) \\Delta t  + \\sigma \\sqrt{\\Delta t} \\, z \\right] \\]\nEnough formulas for now, let us take a look at the simulation\n\n\nSimulation Approach\nFrom a technical point of view, the simulation that follows in fact is run in a notebook run on Spark-Notebook. We started out developing an object-based representation of the problem, but quickly turned to a purely functional (recursive) approach since it is simpler to understand and reason about. We start, as Taleb does, from the assumption that every 20 days the number of infected people is doubled (doubling time of 20 days).\nIn a follow-up post, I may delve a little deeper in the actual implementation. Here, we want to focus on the results.\n\n\nNon-stochastic version\nWithout any randomness in the system, the above dynamics reduces to:\n\\[ S(t) = S(t_0) e^{ \\mu t}  \\]\nWith a doubling rate of say 20 days, we get over 300K after a year. That’s a lot. A creepy amount. And note that this is only for a a doubling rate of 20 days. In practice, many diseases may be spreading much faster.\nThe result is the plot below.\n\n\n\nStochastic version\nWe now add the stochasticity mentioned earlier. Adding this type of multiplicative noise corresponds for instance to uncertainty in the spreading of the virus. We generated 1000 timelines. The plot can be found below:\n\nIt immediately becomes clear that some of the curves tend to grow very rapidly! The worst case (given 1000 random histories) is over 20 milion after one year. Compare that to the just over 300K in the case of the non-stochastic version.\nPlease note that adding stochasticity does not influence the average over the different timelines. The average value of the 1000 sample trajectories after one year is by definition around the value derived for the non-stochastic version. In other words, the distribution is highly skewed. In this case, we are even dealing with a fat-tailed distribution.\n\n\nDiscussion\nThis post is becoming quite long already. Let us conclude with one important consequence of the above and leave further dicussion for a later stage. Although the above simulation is very rudimentary, it shows that relatively small variations1 to the rate of spreading cause the worst case scenario to be extremely far off the average scenario.\n\n\n\n\n\nFootnotes\n\n\nEven variations that are normally distributed, i.e., thin tailed.↩︎"
  },
  {
    "objectID": "insights/blog/2013-01-21-slm-issues-with-traditional-slm-part-5/index.html",
    "href": "insights/blog/2013-01-21-slm-issues-with-traditional-slm-part-5/index.html",
    "title": "SLM: Issues with traditional SLM (Part 5)",
    "section": "",
    "text": "Focus on the wrong instances\nA consequence of dealing with averages and aggregate metrics is that in some cases, when a threshold value is reached there is no longer an incentive to act on the individual instance that went wrong. As an example, consider example 6 given before. If the resolution time of an incident is above threshold, it is counted in the percentage and it will not get any better over time, but also not worse! In other words, there is no longer a reason to fix the incident as soon as possible. Rather, it may be better to focus on other incidents that can still be fixed within threshold.\nThe result may be a large number of open incidents and thus unhappy customers, and rightly so."
  },
  {
    "objectID": "insights/blog/2013-01-25-slm-issues-with-traditional-slm-part-7/index.html",
    "href": "insights/blog/2013-01-25-slm-issues-with-traditional-slm-part-7/index.html",
    "title": "SLM: Issues with traditional SLM (Part 7)",
    "section": "",
    "text": "Avoid Accountability\nWhen all goes wrong, and the KPIs are showing bad performance, there is one last option: avoid accountability. For instance, because other teams have not done their job which caused the resolution time to be above threshold. This can quickly lead to long discussions and eventually mistrust between teams and companies."
  },
  {
    "objectID": "insights/blog/2014-09-24-writing-workflow-and-reproducible-data-analysis/index.html",
    "href": "insights/blog/2014-09-24-writing-workflow-and-reproducible-data-analysis/index.html",
    "title": "Writing Workflow and Reproducible Data Analysis",
    "section": "",
    "text": "I’ve been writing about my writing workflow before. Since some aspects of it are related to reproducible research and especially reproducible data analysis, I have collected some material and tips in a presentation I gave last week on my Github:\n\n\n\nRR\n\n\nOne aspect that I did not yet mention there, is how I approach this on my Mac. This depends a little bit on what type of text I’m writing. Data analysis is usually done within RStudio. It has very good functionality for generating PDFs and the like, but I still prefer my own Makefile and knitr/Pandoc combination.\nLess technical texts are usually writing using iAWriter, but sometimes also in Sublime Text. iAWriter by default has support for Markdown, Sublime Text can be configured with a very good Markdown plugin. I use Marked for previewing, proof-reading, etc."
  },
  {
    "objectID": "insights/blog/2009-04-18-a-new-project/index.html",
    "href": "insights/blog/2009-04-18-a-new-project/index.html",
    "title": "A new project…",
    "section": "",
    "text": "Long time ago… I used to blog every now and then. It used to be about technology.\nI changed focus in my professional career and in the meanwhile started a few other projects that limit my time to spend blogging. It is not that I do not have any ideas anymore, quite the contrary, but they usually involve extensive thinking/writing.\nOne project recently started is the writing of a book. When people ask me where I got the idea of writing a book, I usually answer that it has always been there, I only needed a topic to write about. Chatting with my good friend Koen Vermeir (see for instance here) we came to the conclusion that we were talking a lot about the same things, from different perspectives. We decided it was time to note that down.\nDon’t expect a printed copy any time soon, we gave ourselves 5 years! Stay tuned for more info though…"
  },
  {
    "objectID": "insights/blog/2012-09-18-choosing-the-right-cover-photo/index.html",
    "href": "insights/blog/2012-09-18-choosing-the-right-cover-photo/index.html",
    "title": "Choosing the right Cover Photo",
    "section": "",
    "text": "Say you are the editor in chief of a magazine and you have to decide among two possible cover photos. The magazine draws a significant part of its sales from customers that are not subscribers. In other words, you know the cover is important. Which one do you choose?\nOne editor in chief of a magazine I met recently takes the following approach in this situation: they send both photos to a large number of readers asking them which one they like most. The majority vote wins.\nThis is a typical crowdsourcing approach and probably the best option available. James Surowieckidiscusses crowdsourcing in detail in his book ‘The Wisdom of Crowds: Why the Many Are Smarter Than the Few’.\nAdditionally, I argue that the crowdsourcing approach yields a better customer experience because readers are involved in the selection process."
  },
  {
    "objectID": "insights/blog/2008-11-26-why-personal-productivity-may-be-hard-and-the-corporation-has-the-answer/index.html",
    "href": "insights/blog/2008-11-26-why-personal-productivity-may-be-hard-and-the-corporation-has-the-answer/index.html",
    "title": "Why Personal Productivity may be Hard and ‘The Corporation’ has the answer",
    "section": "",
    "text": "Eli has drawn my attention to ‘The Corporation’. Shame on me for not knowing and having seen this documentary before (1). One word struck me when I heard it, because it is a topic I wanted to discuss already for quite some time in a different context: Accountability.\nWhat does it have to do with personal productivity then?\nI’m involved in quite some thinking about IT and business processes lately. Defining the process is generally easy, the measurement of their performance both from a process as well as a quality/content point of view is much more difficult (2). One thing, though, that is a core component of every step in the process is defining the person who is accountable, which is usually different from the person who is responsible for doing the work. Usually, people use a so-called RACI diagram to define the respective roles for every step in a process or task.\nPersonal Productivity is all about processes (think GTD, for instance) and personal work flow. Similarly, it’s not easy to measure the quality of the process or deliverable. But what really makes personal productivity hard is the fact that one person is both accountable and responsible, or in other words: we have to ‘control’ our own work.\nMaybe that is why we need a personal assistent?\n\nNo, I’m not going into the details of the documentary, and I am not commenting on the reasoning of Eli at this time.\n\nNo, that is also not what I wanted to talk about now (but will do in the future)."
  },
  {
    "objectID": "insights/blog/2012-12-04-slm-issues-with-traditional-slm-part-1/index.html",
    "href": "insights/blog/2012-12-04-slm-issues-with-traditional-slm-part-1/index.html",
    "title": "SLM: Issues with traditional SLM (Part 1)",
    "section": "",
    "text": "We introduced some basic aspects of SLM in the previous posts. We now turn to a list of issues with traditional Service Level Management.\n\nQuantity instead of Quality\nIn general, it is easier to define a metric or KPI that is based on a quantitative parameter: the number of incidents, number of escalations, number of errors, minutes downtime, etc. That is the reason that this type of parameters are often found in SLAs. In a lot of cases, quantity is not related to quality.\n‘The number of incidents resolved’, for instance, is not a good quality parameter. It may well be that some of the incidents took years to acknowledge. ‘The number of incidents resolved in an agreed time frame’ is better, but the resolution may be only a temporary workaround? Or, in order to attain the required time frame, staffing is doubled and costs skyrocket.\nWe can go on giving examples like this. Sometimes, quantitative information is good, or can be rephrased to be meaningful. Sometimes quantitative parameters can be used as informational. But be avoid drawing up an SLA on the basis of purely quantitative parameters only.\nThere is another way to handle quantitative KPIs, but we leave this for a later post."
  },
  {
    "objectID": "insights/blog/2009-06-19-how-to-cope-with-change-an-alternative-approach/index.html",
    "href": "insights/blog/2009-06-19-how-to-cope-with-change-an-alternative-approach/index.html",
    "title": "How to cope with change: An alternative approach",
    "section": "",
    "text": "CHANGE…\nA lot of people are involved in change: Every project is a change, and every change should be a project. In project management, one is used to document risks of the project and think about how to mitigate those risks. Sometimes, mitigating the risks can be costly. But on the other hand, having the risks and the possibility that something goes wrong may cost more.\nA risk that is often forgotten in projects (and thus also in processes of change) is the one of ‘changing people’s mind’. Letting a human being start working (or even thinking) differently is a great challenge.\nIn other words, there is a technological barrier and a mental barrier to any project or change process. Both generate their own set of risks to be taken into account. In many cases the mental barrier is forgotten or at least underestimated.\nA technique that is widely adopted in situations like this is ‘chunking’ the technology: go step-by-step. Start small, but gradually extend the scope of the change. Doing this on a technical level usually also impacts the mental barrier.\nRecently, I was talking to a project manager for a technology company involved in the ‘people change’ process in relation to nanotechnology and its applications in biomedicine. He told me that the mental aspect of the whole project, the fact that little things enter our body, or even remain there for years, is to a large extend not yet common ground in our society. This is a considerable risk for the development of new applications of nanotechnology.\nIn order to mitigate this risk, the project manager told me they sometimes developed a completely different product first (and freezed the other project) because this new product was easier ‘to sell’. So basically, you develop something in parallel in order for people to feel comfortable with this change because it will make future changes easier.\n“But this costs a lot of money!” Yes, obviously, but so does developing a product that has a high risk of not being adopted because the mental leap is too big.\nDid you encounter situations where the parallel development approach might have been successful? Please share it with us in the comments."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html",
    "title": "Bullet Proof Data Science in Scala",
    "section": "",
    "text": "In this post, we go over some typical aspects and challenges that occur in typical data science projects in order to extract some requirements for data analysis in the broad sense of the word. We then illustrate how we tackle these requirements in typical data science projects using Scala."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#joining-and-annotations",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#joining-and-annotations",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Joining and Annotations",
    "text": "Joining and Annotations\nThe reality is that often times several data sources need to be aggregated. Aggregation can be in two ways/directions:\n\nVertical: New data, similar to what we already have, and\nHorizontal: Additional information about data we already obtained."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#parsing-libraries",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#parsing-libraries",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Parsing Libraries",
    "text": "Parsing Libraries\nLast week I was parsing a tab-separated file with information about genomic variants (aka a VCF file). The parsing was done using a custom library created by a colleague of mine. I had uses the library before, but now suddenly it did not work anymore. All I got was a confusing error about some exception:\n\nAnd then, the fun starts. How to find out where things go wrong and how to make sure you don’t have to rewrite (part of) the parser? It turned out additional fields can be added to a VCF format file, which the parser does not take into account."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Missing Values",
    "text": "Missing Values\nA similar issue occurs when dealing with missing values, or missing columns in the data. It’s very easy to end up with exceptions in the Scala/Java world or equivalent in other languages.\nThe challenge of missing data becomes even more concrete when additional data is aggregated. Suppose we have additional annotations about a subset of the data. There needs to be a way to cover situations like this."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#solving-the-challenge-in-scala",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#solving-the-challenge-in-scala",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Solving the Challenge in Scala",
    "text": "Solving the Challenge in Scala\nIn other words, you don’t have control over the input in most cases. In what follows, we describe an approach to data analysis in Scala that takes into account the above challenges. The approach is heavily based on principles of Functional Programming while capturing the data model in an object model."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values-1",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#missing-values-1",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Missing Values",
    "text": "Missing Values\nOn important aspect of the above challenges is missing values. Say you’re a service organization that keeps records of potential customers. Furthermore, say you want to analyze people’s hobbies. You would like to allow for a distinction between 3 situations:\n\nThere is no information about the customer’s hobbies\nThe customer does not have any hobbies\nThe customer has 1 or more hobbies\n\nSay you encode the hobbies as a List of String (free form), then (2) corresponds to an empty list and (3) corresponds to a list of n hobbies. But what does (1) correspond to? In R, one usually gets NA. In Java, one would often sees the occurrence of null, but using null is not a good habit.\nInstead, we use the Option type. It encapsulates whatever other data structure you want. The above 3 situations then correspond to:\n\nNone for no hobbies known about this person\nSome(List()) for this person does not have hobbies\nSome(List(hobby1, hobby2, ...)) for the hobbies for this person\n\nFor the FP (Functional Programming) people among us, the option is a Monad. But let’s not go there yet in order not to scare off the others…"
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#start-with-the-end-in-mind",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#start-with-the-end-in-mind",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Start with the End in Mind",
    "text": "Start with the End in Mind\nIn our experience, it makes sense to define a good model for the data after aggregation and processing and capture this in an object model. This model need to be the one that can be used as input for a machine learning library as such, but it should capture the logic of the application domain.\nAnd Scala’s case classes come in very handy. We will not cover the specifics or benefits of case classes here, but remember this: always put case in front of your class definition. Oh, and while you’re at it, add val before every class parameter as well.\nFor instance, and to be in line with the discussion above, we could define a Name class definition as follows:\ncase class Name(val firstName:Option[String], val lastName:Option[String])\nOne can add safe getters and setters if necessary:\ncase class Name(val firstName:Option[String], \n                val lastName:Option[String]) {\n    def getSafeFirstName = firstName.getOrElse(\"First name not known\")\n    def getSafeLastName  = lastName.getOrElse(\"Last name not known\")\n}\nPlease note that the safe getters return a String, even if the value is not available."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#safe-transformation",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#safe-transformation",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Safe transformation",
    "text": "Safe transformation\nOnce we encapsulated the data in an Option, we can safely process this data as well. There are multiple way to do this, but all come down to the same principle:\n\nOnly process values that should be processed, so don’t process missing entries, and\nMake sure the processing itself is safe by catching exceptions where necessary.\n\nComing back to our example: In practice one seldom gets a dataset with first name and last name clearly split. We could store input as well, and define a companion object in order for people to easily use the API:\ncase class Name(val unparsed:Option[String], \n                val firstName:Option[String]=None,\n                val lastName:Option[String]=None)\n\nobject Name {\n    def apply(unparsedStr: String) = new Name(Some(unparsedStr))\n    def apply(unparsedStr: String, fnStr:String, lnStr:String) = {\n        new Name(Some(unparsedStr), Some(fnStr), Some(lnStr))\n    }\n}\nWe used default values for the first and last name. Imagine now what you can do with an object model like this:\nval name1BeforeParsing = Name(\"John Doo\")\nval name2BeforeParsing = Name(\"Bar Foo\")\nval name3BeforeParsing = Name(\"Franz Octupus\")\n\n// A very crude parsing function\ndef parseName(in:Name):Name = {\n    val names = in.unparsed.map(_.split(\" \"))\n    in.copy(firstName = names.map(_(0)), lastName = names.map(_(1)))\n}\n\nval name1 = parseName(name1BeforeParsing)\nval name2 = parseName(name2BeforeParsing)\nval listOfPersons = List(name1, name2, name3BeforeParsing)\nNow, obviously there are lots of problems with this approach to parsing the name. One could improve this in many ways. For instance, we did not take into account a middle name. One approach to improve the parsing itself could be to use a database of common first names and last names. But that’s not the aim of this post. So let us continue with the important stuff here. Let us convert all first names to initials in a safe way:\ndef getInitials(s: String):String = s.toCharArray.head.toString.toUpperCase + \".\"\n\ndef firstNameToInitials(name: Name):Name = name.firstName match {\n    case Some(nameString) => name.copy(firstName = Some(getInitials(nameString))\n        )\n    case None => name\n}\nThe result is as follows:\nlistOfPersons.map(firstNameToInitials).foreach{println}\nName(Some(John Doo),Some(J.),Some(Doo))\nName(Some(Bar Foo),Some(B.),Some(Foo))\nName(Some(Franz Octupus),None,None)\nSee how the function gracefully managed to pass over nonexistent values! The initials of nothing is still nothing. It does not end here, though, because we managed to cope with missing values, but we did not yet make the transformation fully bullet proof! See what happens in the following case:\nfirstNameToInitials(parseName(Name(\"Jacobus\")))\njava.util.NoSuchElementException: next on empty iterator\n  at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n  at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n  at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n  at scala.collection.IterableLike$class.head(IterableLike.scala:107)\n  at scala.collection.mutable.ArrayOps$ofChar.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:222)\n  at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n  at scala.collection.mutable.ArrayOps$ofChar.head(ArrayOps.scala:222)\n  at .firstNameToInitials(<console>:10)\n  ... 33 elided\nAn easy way to solve this, and one that is compatible with our approach thus far is by using Try in Scala. You have two choices now, which depend on how you want the API to work:\n\nWhen an exception occurs during the transformation, let the result be None. So in other words, let it correspond to a missing value.\nWhen an exception occurs, insert a default value\n\nBoth scenarios are shown below:\nimport scala.util.Try\n\nval DEFAULT:String = \"\"\n\ndef getInitialsDefault(s: String):Option[String] = {\n    Some(Try(s.toCharArray.head.toString.toUpperCase + \".\").toOption.getOrElse(DEFAULT))\n}\n\ndef getInitialsOption(s: String):Option[String] = {\n    Try(s.toCharArray.head.toString.toUpperCase + \".\").toOption\n}\n\n\ndef firstNameToInitials(name: Name, \n                        getInitialF:String=>Option[String]):Name = {\n    name.firstName match {\n        case Some(nameString) => name.copy(firstName = getInitialF(nameString))\n        case None => name\n    }\n}\nAnd use it as follows:\nfirstNameToInitials(Name(\"def\", \"\", \"def\"), getInitialsDefault _)\nres: Name = Name(Some(def),Some(),Some(def))\n\nfirstNameToInitials(Name(\"def\", \"\", \"def\"), getInitialsOption _)\nres: Name = Name(Some(def),None,Some(def))\nBy now, the FP adepts give us 1 point for keeping our data structures immutable: the above functions do not mutate the name object, but rather instantiate a new one.\nBut we receive a negative score on omitting to acknowledge that Option is a Monad and that a much better way of writing the above exists:\ndef getInitialsDefault2(name: Option[String]):Option[String] = \n    name.map( s => Try(s.toCharArray.head.toString.toUpperCase + \".\").getOrElse(DEFAULT))\n\ndef getInitialsOption2(name: Option[String]):Option[String] = \n    name.flatMap( s => Try(s.toCharArray.head.toString.toUpperCase + \".\").toOption)\n\ndef firstNameToInitials2(name: Name, \n                         getInitialF:Option[String]=>Option[String]):Name = {\n    name.copy(firstName = getInitialF(name.firstName))\n}\nThese functions do not exactly do the same as the earlier functions, but they illustrate a very important concept: map or flatMap on None results in None. So, there is no need to explicitly use pattern matching here. In order to parse the list of names, we simply map over it (indentation added for ease of reading):\nlistOfPersons.map(firstNameToInitials2(_, getInitialsDefault2))\nres: List[Name] = List(\n                    Name(Some(John Doo),Some(J.),Some(Doo)), \n                    Name(Some(Bar Foo),Some(B.),Some(Foo)), \n                    Name(Some(Franz Octupus),None,None))\nFor a nice and graphical introduction to Monads and related concepts, I recommend the following blog post. Transformations like the above can be composed in a bullet-proof way, once a missing value or exception occurs, we either continue with a default value or with None.\nWe can go a bit further still for the FP fanatics among us. The function firstNameToInitials in fact is the setter part of what is called a Lens. We will come back to this later in this post."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#immutability-and-lenses",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#immutability-and-lenses",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Immutability and Lenses",
    "text": "Immutability and Lenses\nPlease note the examples above do not mutate any objects. We use the copy method to create an updated version of an object. We will not discuss the benefits of this kind of programming, but just mention the use of a Lens in order to update an immutable data structure.\nSeveral libraries exist for working with Lenses, some based on Scala macros others more high-level. For the sake of the argument, we lift the Lens definition out the Scalaz project:\ncase class Lens[A,B](get: A => B, set: (A,B) => A) extends Function1[A,B] with Immutable {\n  def apply(whole: A): B   = get(whole)\n  def updated(whole: A, part: B): A = set(whole, part) // like on immutable maps\n  def mod(a: A, f: B => B) = set(a, f(this(a)))\n  def compose[C](that: Lens[C,A]) = Lens[C,B](\n    c => this(that(c)),\n    (c, b) => that.mod(c, set(_, b))\n  )\n  def andThen[C](that: Lens[B,C]) = that compose this\n}\nA lens for first name can then be defined as such:\nval aLens = Lens[Name,Option[String]](\n    _.firstName, \n    (o, value) => o.copy(firstName = value))\nSo, you provide two functions: a getter and a setter.\naLens.get(listOfPersons(1))\nres: Option[String] = Some(Bar)\n\naLens.set(listOfPersons(1), Some(\"Barby\"))\nres: Name = Name(Some(Bar Foo),Some(Barby),Some(Foo))\nIn this case the usage of a lens is almost trivial because using the copy method on a class is an easy thing to do. But when you start to nest classes to create a more complicated model, multiple copy calls are required. Lenses are a good alternative in that case."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-reading-of-data-sources",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-reading-of-data-sources",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Bullet Proof Reading of Data Sources",
    "text": "Bullet Proof Reading of Data Sources\nThe foundation of the bullet proof approach thus far clearly is the Option type, aka the Maybe Monad in Haskell. We already used Try to catch exceptions in a user-friendly and safe way.\nUsing Try while reading a file allows us to cope with missing values, columns or when transforming numbers encoded in strings to integers or floating point numbers. In every case, we have the option to cast the possible exceptions to None (missing value) or a default value."
  },
  {
    "objectID": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-aggregation-of-data-sources",
    "href": "insights/blog/2016-06-25-bullet-proof-data-analysis-in-scala/index.html#bullet-proof-aggregation-of-data-sources",
    "title": "Bullet Proof Data Science in Scala",
    "section": "Bullet Proof Aggregation of Data Sources",
    "text": "Bullet Proof Aggregation of Data Sources\nAs mentioned already it is seldom the case that all data is available from one input file/database. Sometimes additional annotations need to be added (horizontal aggregation), coming from a different source. Sometimes, more data should be added which lacks certain features that the already parsed data contains (vertical aggregation).\nThere are several approaches to this. One is to go from one class-representation to another. But in this case, classes should be closely matched to the source of the data. We use a different approach, also making use of the (… tada! …) Option type.\nSince a) the data ready for analytics should be in denormalized form, and b) we already have a model for that data that is able to cope with missing values, it is not hard to start from the data source that contains the most information about the denormalized form of the data. All fields/features that are not available in this first data source remain None (aka the default).\nAdding additional features later can easily be done by updating the features from None to Some(...)1. Adding additional data vertically can be done in the same way. It’s perfectly fine to end up with a data structure where most of the rows have None for a certain feature but some contain more information. And since our transformations are bullet proof, all runs safe."
  },
  {
    "objectID": "insights/blog/2008-04-03-vmware-vscsistats-the-paper/index.html",
    "href": "insights/blog/2008-04-03-vmware-vscsistats-the-paper/index.html",
    "title": "VMware vscsiStats: The paper",
    "section": "",
    "text": "I wrote about vscsiStats before, but it seems I was amongst the first to do so. Luckily, one of the creators has put some more info on his blog. In this post, he refers to his paper about the technology."
  },
  {
    "objectID": "insights/blog/2012-11-20-slm-introduction-part-3/index.html",
    "href": "insights/blog/2012-11-20-slm-introduction-part-3/index.html",
    "title": "SLM: Introduction (part 3)",
    "section": "",
    "text": "As mentioned before, lots of data points are gathered and averages are often used to reduce this amount to smaller proportions. The ultimate goal is often to reduce different KPIs to one single number: the service level.\n\nService Level\nTraditionally, a service level is either defined as the result of one KPI or as the (weighted) average of a small set of KPIs, most often expressed as a percentage. This way, it is at best an average of a limited number of KPIs.\nIn practice, one often measures the service level and takes a baseline which is then used as benchmark."
  },
  {
    "objectID": "insights/blog/2007-10-26-vmware-capacity-planner-taking-the-data-offline-part-2-cygwin-wget/index.html",
    "href": "insights/blog/2007-10-26-vmware-capacity-planner-taking-the-data-offline-part-2-cygwin-wget/index.html",
    "title": "VMware Capacity Planner: taking the data offline (Part 2: Cygwin & wget)",
    "section": "",
    "text": "I started thinking about avoiding the manual exports from the VMCP website and remembered I used WGET in the past for this kind of stuff. The difference with before was that this time, I needed to login to the website before being able to get data from it.\nLogin in to the VMCP website is done using a POST method and luckily wget supports that. The next thing is to understand what the post data needs to be. This can be fetched from the source of the logon page:\n\nThe relevant POST data is derived to be\nfuseaction=Security   Page=users_login.cfm   username=???   password=???\nSince I run Windows XP on my laptop, I use Cygwin to run wget. Cookies are used to store a session ID for you logon session, so wget has to be told to store those cookies in a file. This is the resulting command line:\nwget --keep-session-cookies --save-cookies cookies.txt 'https://optimize.vmware.com/index.cfm' --post-data 'fuseaction=Security&Page;=users_login.cfm&username;=???&password;=???' --no-check-certificate -O output.html\nParsing the output.html file, you should be able to see whether logon was succesful. The success depends on many factors (local proxy, network settings, username/password, etc.). You can get more info by adding suitable options to wget.\nThis concludes part 2 of this series of articles. In this part, we used wget to logon to the VMware capacity planner website."
  },
  {
    "objectID": "insights/blog/2007-09-17-file-versioning/index.html",
    "href": "insights/blog/2007-09-17-file-versioning/index.html",
    "title": "File Versioning",
    "section": "",
    "text": "File versioning is something I have been interested in since I started out doing programming for my simulations back in university. Later, I was lucky enough to be able to use (La)TeX to type my PhD thesis, which enabled me to use CVS to track changes to my text.\nYears later, I’m less lucky in that I do not use LaTeX anymore (neither do I program in C ;-), but I do still want my documents and scripts to be versioned, especially when actively working on them. I used to create copies for major versions of Word or PowerPoint documents. At the end, I deleted most of the intermediate copies because they were no longer relevant.\nBut now, everything has changed dramatically… due to File hamster \nFileHamster is a versioning tool, that automatically tracks changes to files and directories and allows you to keep those changes, revert to earlier versions, backup and by means of some handy plugins add notes and remarks to versions. And … it is free !\nBehind the scenes, the tool creates a copy of every version you save, so you do use a lot of space for big files, but you cannot blame a tool for the fact that most of the files we work with nowadays are composed of binary data!"
  },
  {
    "objectID": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html",
    "href": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html",
    "title": "Viz and Math in Apache Zeppelin",
    "section": "",
    "text": "For some years, I have been going back an forth between Spark-Notebook and Apache Zeppelin for different use-cases. Already 2 years ago, I made a little comparison of the two technologies.\nI used Spark-Notebook in order to do develop the code necessary to write the post about model error. It came in handy that Spark-Notebook supports math notation in Markdown out-of-the-box. Zeppelin does not do that.\nOn the other hand, I have never been fond of the graphics support in Spark-Notebook. Zeppelin has some interesting functionality by default, including the ability to pivot data tables. But it still is too limited for customized charts and visualizations.\nNow, luckily, both the visualization aspect and math support can easily be solved."
  },
  {
    "objectID": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#math-support",
    "href": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#math-support",
    "title": "Viz and Math in Apache Zeppelin",
    "section": "Math support",
    "text": "Math support\nSupport math notation is as easy as inserting a cell with the following content:\n%angular\n<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'>\n</script>\nA markdown formatted cell looks like this:\n# Introduction\n\nI don't want to discuss the biology of a virus or its spreading, even though virus spreading is the topic of the simulation we're about to do. I invite you to look [elsewhere](http://arxiv.org/abs/1411.1472) for that.\n\nWe mainly want to get a feel for the main message Taleb's argument in his analysis linked above: The dangers of basing decisions on simple models for the spreading of virusses and the lack discussing the risks involved in analysing these models.\n\nWe do this by modelling the spreading of a virus by means of a simple [geometric Brownian motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion) process. It's the same process that is used in the (in)famous [Black-Scholes model](https://en.wikipedia.org/wiki/Black%E2%80%93Scholes_model).\n\nThe differential form of the process is as follows:\n\n$$\nd S(t) = S(t) \\mu dt + S(t) \\sigma d W(t)\n$$\nThis is an excerpt from the post on model error. Please note that the $$ symbols are on a line of their own. This is to avoid that Zeppelin renders math in the input field itself.\nThe following screenshot shows this in action in Zeppelin:\n\n\n\nMathjax in Zeppelin\n\n\nOh, yes, you might need to refresh the notebook in order for the rendering to take effect."
  },
  {
    "objectID": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#vis",
    "href": "insights/blog/2016-10-27-viz-and-math-in-apache-zeppelin/index.html#vis",
    "title": "Viz and Math in Apache Zeppelin",
    "section": "Vis",
    "text": "Vis\nZeppelin has an interesting approach to visualization already. It offers basic types of plots. Unfortunately, these plots are often not sufficient. Especially when one wants to provision dashboards based on notebook content.\nOf course, one can use R and Python in Zeppelin, and even interchange data between contexts so that plotting could be done using ggplot2 or matplotlib. But it remains a hassle to use, and not all of these interpreter are always installed in every instance of Zeppelin.\nThere are a number of Scala plotting libraries, but none that convinced me up till now. Until Vegas came around. It is based on vega-lite and offers an API for Scala/Spark. I have done custom visualizations in Zeppelin and Spark-Notebook before, but those involved including additional javascript libraries. Vegas does not require a custom build. It can be used like this:\nFetch the binary package:\n%dep\nz.load(\"org.vegas-viz:vegas_2.11:0.3.6\")\nImport the necessary classes:\nimport vegas._\nimport vegas.render.HTMLRenderer._\nimplicit val displayer: String => Unit = { s => print(\"%html \" + s) }\nAnd plot the data:\nval plot = Vegas(\"Country Pop\")\n                .withData(\n                    Seq(\n                        Map(\"country\" -> \"USA\", \"population\" -> 314),\n                        Map(\"country\" -> \"UK\", \"population\" -> 64),\n                        Map(\"country\" -> \"DK\", \"population\" -> 80)\n                    )\n                 )\n                .encodeX(\"country\", Nom)\n                .encodeY(\"population\", Quant)\n                .mark(Bar)\n                .show\nPlease not that withData takes a Seq of Map, contrary to what is stated on the github page.\nThe following screenshot shows this running:\n\n\n\nVegas in Zeppelin"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "",
    "text": "It’s been a while since I first wrote about tackling model error using a simple model. It’s about time to come back to it.\nWhat triggered the current post is the opportunity I was given to give a masterclass in the Evidence and policy summer school. Since my masterclass is about uncertainty in decision making, it seemed like a nice opportunity to look back at the simple model."
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-1-no-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-1-no-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 1: No Randomness",
    "text": "Step 1: No Randomness\nBy clicking on the parameters of the simulation, or below in the footer on the words can be changed, the simulation settings can be adjusted.\nSet the variables such that there is no randomness applied to the scenario. Only one scenario is sufficient in this case:\n\nAnd see what the result looks like:"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-2-a-bit-of-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-2-a-bit-of-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 2: A bit of Randomness",
    "text": "Step 2: A bit of Randomness\nIn step 2, we add a bit of randomness. For this we use the default settings, although you could increase the number of scenarios if you wanted to:\n\nThe result should look similar to the following. Please note that randomness has been added, so your result should not look exactly like the one presented here!"
  },
  {
    "objectID": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-3-a-bit-more-randomness",
    "href": "insights/blog/2018-09-06-a-practical-approach-to-model-error-part-2/index.html#step-3-a-bit-more-randomness",
    "title": "A Practical Approach to Model Error - Part 2",
    "section": "Step 3: A bit more Randomness",
    "text": "Step 3: A bit more Randomness\nIn step 3, we increase the amount of randomness added to the scenarios. For instance:\n\nNow, given more randomness in the scenarios, yours might be completely different from the one below:"
  },
  {
    "objectID": "insights/blog/2016-10-26-data-intuitive-at-spark-summit-2016/index.html",
    "href": "insights/blog/2016-10-26-data-intuitive-at-spark-summit-2016/index.html",
    "title": "Data Intuitive at Spark Summit 2016",
    "section": "",
    "text": "Short version\nNo, I’m not at the spark summit in Brussels. Time is not on my side, too much things to do.\nBut, being there is not necessary when other people make sure they do the publicity for you.\nThank you Miel Hostens!\n\n\nLonger version\nAbout two years ago, I gave a presentation at the first Spark Meetup in Belgium. It’s when I met Miel. I think it is fair to say that the architecture and application I was describing back then (and which is still under active development) made Miel realize that what he wanted to achieve was possible.\nThe talk used to be available as a video, but the link is no longer active. The slides, however, can be fetched here.\nMiel and I have stayed in contact. I’m happy he can get his message across. And I’m happy of course I have played a role in this."
  },
  {
    "objectID": "insights/blog/2012-11-19-slm-introduction-part-2/index.html",
    "href": "insights/blog/2012-11-19-slm-introduction-part-2/index.html",
    "title": "SLM: Introduction (part 2)",
    "section": "",
    "text": "In the previous post about SLM, we gave some examples of metrics that are often used in the practice of SLM. We now turn to some characteristics of most service level implementations.\n\nAverages and Sums\nOne often deals with lots of data points, for instance when considering incidents or service calls to a helpdesk.\nUsually, a (small) number of important metrics are selected and calculated on a regular basis (monthly, quarterly). The metrics are defined as averages or sums, such that one number is characteristic for the set of incidents, calls, problems, … that it covers.\nAn important metric is often called a Key Performance Indicator."
  },
  {
    "objectID": "insights/blog/2013-11-25-running-docker-on-macosx/index.html",
    "href": "insights/blog/2013-11-25-running-docker-on-macosx/index.html",
    "title": "Running Docker on MacOSX",
    "section": "",
    "text": "I found out about docker this morning via the blog of the people behind AMPlab, creators of Spark and such. In short (because it’s actually much more than this) it lets you run a Spark/Shark cluster (pre-built!) on your PC.\nI want to use it to experiment with a (virtual) Spark/Shark cluster on my laptop. Isn’t that cool?!\nUnfortunately, docker is built for Ubuntu. Fortunately, there are instructions on how to set it up on other systems.\nTwo things needed to be done before I got it to work:\n\nUpgrade the amount of memory available to Vagrant to 2GB. See here: http://docs-v1.vagrantup.com/v1/docs/config/vm/customize.html.\nEdit the nameserver script in order for the nameserver test to complete with success.\n\nFor the second, I changed\ndig nameserver @${NAMESERVER_IP} | grep ANSWER -A1 | grep 127.0.0.1 > /dev/null;\ninto\ndig nameserver @${NAMESERVER_IP} | grep ANSWER -A1 | grep ${NAMESERVER_IP} > /dev/null;"
  },
  {
    "objectID": "insights/blog/2010-03-11-using-a-hammer-to-paint-the-wall-part-1/index.html",
    "href": "insights/blog/2010-03-11-using-a-hammer-to-paint-the-wall-part-1/index.html",
    "title": "Using a hammer to paint the wall (part 1)",
    "section": "",
    "text": "It doesn’t make sense, does it? Using a hammer to paint a wall? No, it doesn’t, because we know what a hammer looks like and we all know what it takes to paint a wall.\nSimilarly, we don’t run out buying a PC to know what 2 times 2 is, we don’t spend twice our yearly income for a software application that would fill out our tax papers for us, we don’t buy a mechanical crane in order to drill a small hole in the wall. And so on, and so on. You get the point…\nThis is the first part of a series on using tools to solve problems in real-life, especially in organizations.\nThe point is that human beings seem to have an obsession for tools, especially ICT tools (hardware, software). And somehow, during the development or implementation of the tool, we seem to lose sight of the reason why it is there. The development or implementation becomes the new goal, rather than the underlying reasons for selecting the tool in the first place.\nMore about this topic later…"
  },
  {
    "objectID": "insights/blog/2012-01-17-infografiek/index.html",
    "href": "insights/blog/2012-01-17-infografiek/index.html",
    "title": "Infografiek",
    "section": "",
    "text": "There’s a new word in the Dutch language: infografiek. It’s clearly derived from the English infographic. If this is a trend, it’s only the start. FlowingData has a nice overview of different names that are all related to this new type of journalism and reporting which blends data with visuals.\nThe power of this approach to reporting is that it can yield better insight and understanding. The dangers are twofold:\nFirstly there’s the risk of overshooting, of spending more time and effort in using a visual approach but ending up with something that doesn’t improve understanding nor insight.\nSecondly (and more profoundly), there’s the risk of oversimplication or drawing false conclusions. That is part of what EagerEyes refers to when citing Jason Moore’s version of the Hippocratic oath for infographics.\nLet’s hope the Dutch speaking community of artists and journalists who deal with ‘infografieken’ do it in a scientifically correct way."
  },
  {
    "objectID": "insights/blog/2007-12-14-capacity-planning-what-to-monitor-and-how-to-interpret/index.html",
    "href": "insights/blog/2007-12-14-capacity-planning-what-to-monitor-and-how-to-interpret/index.html",
    "title": "Capacity Planning: What to monitor and how to interpret",
    "section": "",
    "text": "Capacity planning starts with capacity (of performance) monitoring.\nEverybody who is involved in the monitoring of systems will acknowledge that the most difficult aspects in monitoring a server (or set of servers) are:\n\nFinding the proper indicators for the performance of the system (CPU usage, CPU cycles, memory usage, paging, etc.)\nMaking sure they are queried regularly, but not too much in order to avoid impacting the performance of the system by monitoring it.\nStoring the resulting data\nSummarize, create views, average, etc. (this also depends on what you want to know about the system)\nAnalyze, interpret, etc.\n\nDid I say the most difficult aspects? Are there any other aspects? Well, not really… capacity monitoring (and planning as a further step) is not an easy task:\n\nAre you aware of the utilization of your systems? Even of your workstations?\nWould you have any idea how many of your servers could be placed on a virtualization platform with a specific set of hardware characteristics?\nWould you know when your mail server had the hardest time managing mail boxes the last couple of weeks?\n\nProbably the answer is ‘no’. Maybe the answer is ‘I don’t care’?\nMost companies do care, because of several reasons: cost, manageability, flexibility, scalability, environment, space, etc.\nThere are already some players on the market: VMware Capacity Planner (see earlier posts), PlateSpin PowerRecon, Veeam Monitor, etc. I’m mostly used to VMware Capacity Planner (VMCP) but recently, I have also evaluated both PowerRecon and Veeam Monitor. More about this later."
  },
  {
    "objectID": "insights/blog/2012-11-15-service-level-management-the-series/index.html",
    "href": "insights/blog/2012-11-15-service-level-management-the-series/index.html",
    "title": "Service Level Management: The Series",
    "section": "",
    "text": "Service Level Management (SLM) is the practice of managing the service level and making sure its compliant with the level agreed with customers[1]. As indicated by the word level, one often seeks to make sure that service quality and performance are measurable.\nIn a series of blog articles, we will first give examples of the way in which service levels are traditionally measured, followed by an explanation of how service levels are measured and calculated in most cases. This enables us to explain what is wrong with the current way of working. To conclude, we will formulate ways to improve service level management.\nAs we will see, many of the arguments are not restricted to SLM alone, but can be applied to bonuses and targets in organizations, for instance.\n[1] see ITIL terminology"
  },
  {
    "objectID": "insights/blog/2007-06-15-to-use-the-mouse-or-not/index.html",
    "href": "insights/blog/2007-06-15-to-use-the-mouse-or-not/index.html",
    "title": "To use the mouse or not?",
    "section": "",
    "text": "Some time ago, I installed Enso (http://www.humanized.com) and fell in love with the easy and fast way of working. Tools like this can really give your productivity a boost. There is one disadvantage for poor consultants like me: Enso is not free (little less than 20 dollars). Nevertheless, this company is one to keep an eye on (check their blog as well, when you’re at it) as other great ideas are under way.\nThis morning, I noticed that a plugin for Launchy (open source) has been created that enables to you to open already open windows. Launchy is free and the plugin as well, so this means that there is a free alternative to Enso. Well, not completely but I’ll leave it up to you to take a look at both products and find the differences."
  },
  {
    "objectID": "insights/blog/2008-09-12-definition-of-productivity/index.html",
    "href": "insights/blog/2008-09-12-definition-of-productivity/index.html",
    "title": "Definition of Productivity",
    "section": "",
    "text": "I’ve always liked looking for structure and relations between things that surround me. I guess I’m not the only one? One of the things I would like to find structure in today is the term ‘productivity’ and what is usually associated with it. This includes so-called productivity systems, lifehacks, workflow tools, etc. In this article, I want to argument that 3 levels can be distinguished.\nEfficiency is a term that is often used together with productivity. The Wikipedia page tells us:\n\n… While productivity is the amount of output produced relative to the amount of resources (time and money) that go into the production, efficiency is the value of output relative to the cost of inputs used. …\n\nMost definitions of productivity are based on the production or manufacturing of (physical) goods. In sharp contrast with this, our Western civilization has evolved into a service-oriented society. Most of us no-longer produce anything physical (except for documents perhaps). This is often referred to as knowledge-work. By definition, knowledge work productivity is much harder to measure, as it involves creativity, thinking, finding solutions to problems, etc.\nIn my opinion, productivity-related information can be divided in 3 groups, I call them levels:\n\nLevel 1: Tips and Tricks\nThis level deals with the question: How can I optimally perform the task at hand. This task could be: process email, have a meeting, brainstorm, etc. Note that this level does not deal with the which task is done first or why this task is important.\nThe various popular life hack blogs and sites are usually concerned with this level of productivity and give plenty of tips on how to collaborate online, clean your house, etc. in a productive way.\n\n\nLevel 2: The Process\nAt this level, we ask ourselves: ‘What’, ‘When’ and ‘in which order’? In other words: how do we approach things?\nThis level is about the tools and techniques that let you plan your life and work: todo lists, Getting Things Done, Do It Tommorrow, etc.\n\n\nLevel 3: Purpose\nIn this level, we ask ourselves ‘Why’. In other words: what drives us, what is our vision and mission, what is our purpose?\nIn many cases, this level is forgotten about. Think about the successful manager who at the age of 60 regrets not having spent more time with his kids. Or think about people trying to do 1001 things on a day without standing still to see whether these things are really valuable.\nEach of these levels can be further split in parts and obviously some things will be on the boundary or cross these levels. Generally speaking, though, every level influences the level below: Understanding your purpose (level 3) tells you which activities are valuable (level 2) and enables you to find tricks (level 1) to do them more quickly and productively.\nThe question remains which questions should be asked first. This is for later.\nThis article is the first in a series of almost literal translations from dutch blog posts by myself on choose2live.blogspot.com."
  },
  {
    "objectID": "insights/blog/2010-02-18-life-expectancy-the-difference-between-male-and-female/index.html",
    "href": "insights/blog/2010-02-18-life-expectancy-the-difference-between-male-and-female/index.html",
    "title": "Life Expectancy: The difference between male and female",
    "section": "",
    "text": "In this post on FlowingData, some interesting statistics are shown about life expectancy in the US.\nEver wondered why insurance companies have higher rates for men than women? The main reason is that on average, women live longer than men. In other words, the risk of dying for a man of, say 50, is higher than for a woman of that age.\nWhy is this? I have some ideas, but no means to prove them. What can be studied from the data is the evolution of life expectancy and the consequences of this evolution. When looking at US data for the last 20 years, simple (linear) extrapolation tells us that in 2047, men and women will have the same average age. Looking at the data for Belgium, the year is 2074.\nI will probably be long dead by then, but according to this linear extrapolation, my grandchildren will have children that have life expectancies of 93 years!\nI agree, linear extrapolation is an approximation. But the tendency is there: men’s life expectancy is getter higher at a slightly faster pace than women’s.\nMore about this topic (including some cool graphs) later."
  },
  {
    "objectID": "insights/blog/2013-01-18-slm-issues-with-traditional-slm-part-4/index.html",
    "href": "insights/blog/2013-01-18-slm-issues-with-traditional-slm-part-4/index.html",
    "title": "SLM: Issues with traditional SLM (Part 4)",
    "section": "",
    "text": "Consider the following (fake) recommendation for a hotel room:\n\nThe guaranteed average room temperature is 20 degrees Celcius.\n\nEven with a money back guarantee, I would not rent the room. Suppose its 40 degrees during the day and 0 degrees during the night? The average temperature may still be 20 degrees…\n\nAverages\nWhen dealing with customer interactions (calls), IT incidents and problems, etc., one usually deals with a large number of instances that are observed. Hundreds or even thousands of calls may be logged in a call center per day. In order to aggregate the information on these calls, one usually takes averages over a given period of time. This is reflected in the examples given before.\nAverages, however, are only a first order representation of a set of instances. One bad instance can easily be compensated by a good instance resulting in a good average. In other words, if the average on-hold time for a callcenter is 2 minutes, it may well be that I have to wait 10 minutes. If only at another time 5 calls are answered within a second, the average is back to 2 minutes."
  },
  {
    "objectID": "insights/blog/2012-11-16-slm-introduction-part-1/index.html",
    "href": "insights/blog/2012-11-16-slm-introduction-part-1/index.html",
    "title": "SLM: Introduction (part 1)",
    "section": "",
    "text": "We start this introduction with some examples of traditional metrics for measuring performance.\n\nExamples\nIn what follows, we give some examples of metrics that are traditionally used in SLM in a number of contexts. First, some callcenter metrics:\n\nAverage time elapsed for calls on hold\nAverage number of interactions with customer before problem solved\nNumber of escalations in a given period\n\nThe second set of examples are related to IT helpdesk services:\n\nAverage time taken to acknowledge a problem ticket\nAverage resolution time for an incident (with priority 1)\nPercentage of incidents with resolution time above threshold\n\nThe third set of examples has to do with IT operations:\n\nPercentage systems uptime\nUnexpected systems downtime\nMean time between system failures\n\nWe will refer to these examples in later posts. We now turn to some characteristics of the way most service levels are implemented."
  },
  {
    "objectID": "insights/blog/2010-10-26-johan-bollen-twitter-mood-predicts-the-stock-market/index.html",
    "href": "insights/blog/2010-10-26-johan-bollen-twitter-mood-predicts-the-stock-market/index.html",
    "title": "Comments on Johan Bollen’s stock market predictions",
    "section": "",
    "text": "You could not escape the news, even if you wanted to. The paper is published on Arxiv.org. It’s a typical example of what our book will be about.\nOn the _optimistic _side, the future of the stock market is somewhere in our collective conscious, so capturing all the data about our lives should, in theory, enable one to make ‘predictions’. It is the basis of so-called crowd-sourcing.\nOn the _pessimistic _side, some considerations are in place:\n\nPrediction does not necessarily mean ‘understanding’, or in other words, correlation is not the same as causation.\nFinding an algorithm that does a prediction based on a huge amount of data is cumbersome, to say the least. The team of researcher found out about this particular correction accidentally, they were not explicitly looking for it.\nAs mentioned in comments on other sites (Technology Review, ReadWriteWeb, etc.), if everyone would use this algorithm for its prediction it would become useless.\nAn accuracy of  87,6% seems high, but bare in mind that a monkey (or a randomized algorithm for that matter) should get you already to 50%.\n\nThis is not the first twitter analysis that shows predictive or analytical value. It is, to our knowledge, the first that is well-founded and statistically relevant."
  },
  {
    "objectID": "insights/blog/2010-10-19-wordpress-it-will-be/index.html",
    "href": "insights/blog/2010-10-19-wordpress-it-will-be/index.html",
    "title": "WordPress it will be…",
    "section": "",
    "text": "I’ve tried Drupal. It failed me, or I failed it. More about the reasoning behind the step later, right now I have tweaking, configuration and theming to do…"
  },
  {
    "objectID": "insights/blog/2013-06-24-writing-workflow-markdown-pandoc-latex-and-the-likes/index.html",
    "href": "insights/blog/2013-06-24-writing-workflow-markdown-pandoc-latex-and-the-likes/index.html",
    "title": "Writing workflow: Markdown, Pandoc, LaTeX and the likes",
    "section": "",
    "text": "You wouldn’t tell from the updates on this website, but I’m actively writing again. Offline, that is, the online part is for later. For now, I want to share my experience improving my writing workflow.\nIn the past, I used LaTeX for scientific texts and MS Word for everything else. LaTeX gives me the professional and typographically correct texts that I want, but I spent too much time fiddling around with packages, remembering markup, etc. MS Word, on the other hand, quickly made me get things done, albeit without the professional look or scientific powers.\nI’m now in a situation that any writing (technical, scientific and even prose) can be done in the same way, delivering results in PDF, html or even MS Word:\n\nIt usually starts in iA Writer (on the Mac or the iPad), but any word processor able to handle ASCII text can be used. I choose iA Writer because of its distraction free writing.\nMarkdown is used as markup specification (including figures, footnotes, emphasis, etc.). Markdown is very basic, but it lets you focus on the content, rather than the form.\nProgramming code (R for instance), formulas, etc. can all be included in the Markdown format by means of the proper notation and possibly some extensions to the parser (see step 4).\nBy means of Pandoc, the text is converted into the appropriate format (html, pdf, LaTeX, ePub, DocBook, …)\nReady!\n\nOk, I hear you thinking, but you just lost all possible configuration of look and feel, layout, etc… That’s correct, there are some Markdown writing tools that allow you to create PDFs that look awful.\nThe nice thing about Pandoc though is that during the conversion step (4), you can specify the templates (CSS, LaTeX header code, MS Word template) that should be used.\nIt takes some fiddling in order to get the correct options to Pandoc and get proper templates in place. A Google should get you going.\nAn example. From a slight adaptation of this file, we generate a Markdown file and this is converted in the PDF linked here. A similar process is used to create the reports for the different analysis steps for the dataMineR project.\nLeave a comment if you would like to see more examples."
  }
]